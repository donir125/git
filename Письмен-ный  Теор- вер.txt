Дмитрий Письменный
Конспект лекций
по теории	
вероятностей,
математической
статистике	
и случайным
процессам
Высшее образование
3-е издание
МОСКВА
АЙРИС ПРЕСС
2008

УДК 519.2(075.8)
ББК 22.17я73-2
П35
Все права защищены.
Никакая часть данной книги не может переиздаваться
или распространяться в любой форме и любыми средствами,
электронными или механическими, включая фотокопирование,
звукозапись, любые запоминающие устройства
и системы поиска информации,
без письменного разрешения правообладателя.
Серийное оформление А М. Драгового
Письменный, Д. Т.
П35 Конспект лекций по теории вероятностей, математической ста¬
тистике и случайным процессам / Дмитрий Письменный. — 3-е
изд. — М.: Айрис-пресс, 2008. — 288 с. — (Высшее образование).
ISBN 978-5-8112-2966-6
Настоящая книга представляет собой курс лекций по теории вероятностей,
случайным процессам и математической статистике.
Первая часть книги содержит основные понятия и теоремы теории вероятно¬
стей, такие как случайные события, вероятность, случайные функции, корреля¬
ция, условная вероятность, закон больших чисел и предельные теоремы. В от¬
дельной главе приведены основные понятия'теории случайных процессов (ста¬
ционарный процесс, марковский процесс, теорема Винера-Хинчина).
Вторая часть книги посвящена математической статистике, в ней излагают*
ся основы выборочного метода, теории оценок и проверки гипотез. Изложение
теоретического материала сопровождается рассмотрением большого количества
примеров и задач, ведется на доступном, по возможности строгом языке.
Предназначена для студентов экономических и технических вузов.
ББК 22.17я73-2
УДК 519.2(075.8)
ISBN 978-5-8112-2966-6
© ООО «Издательство
« АЙРИС-пресс », 2004

Содержание
Введение		6
Раздел первый
Элементарная теория вероятностей и случайных процессов
Глава 1. Случайные события
1.1.	Предмет теории вероятностей		8
1.2.	Случайные события, их классификация 		9
1.3.	Действия над событиями		1L
1.4.	Случайные события. Алгебра событий. (Теоретико-множественная
трактовка) 		13
1.5.	Свойство статистической устойчивости относительной частоты
события		16
1.6.	Статистическое определение вероятности		17
1.7.	Классическое определение вероятности		18
1.8.	Элементы комбинаторики 		20
1.9.	Примеры вычисления вероятностей 		28
1.10.	Геометрическое определение вероятности	 31
1.11.	Аксиомати ческое определен ие вероятности		34
1.12.	Свойства вероятностей		35
1.13.	Конечное вероятностное пространство		36
1.14.	Условные вероятности		37
1.15.	Вероятность произведения событий. Независимость событий		38
1.16.	Вероятность суммы событий				42
1.17.	Формула полной вероятности		44
1.18.	Формула Байеса ( гсюрема гипотез)		45
1.19.	Независимые испытания. Схема Бернулли		47
1.20.	Формула Бернулли		48
1.21.	Продельные теоремы в схеме Бернулли		51
Глава 2. Случайные величины
2.1.	Понятие случайной величины. Закон распределения случайной
величины 		60
2.2.	Закон распределения декретной случайной величины. Многоугольник
распределения		61
2.3.	Функция распределения и ее свойства. Функция распределения
дискретной случайной величины		64
2.4.	Плотность распределения и ее свойства		69
2.5.	Числовые характеристики случайных величин		73
2.6.	Производящая функция		84
2.7.	Основные законы распределения случайных величин		85

4 ■ Содержание
Глава 3. Системы случайных величин
3.1.	Понятие о системе случайных величии и законе ее распределения	104
3.2.	Функция распределения двумерной случайной величины и ее свойства .. 107
3.3.	Плотность распределения ве!>оятностей двумерной случайной величины
и ее свойства	110
3.4.	Зависимость и независимость двух случайных величин	116
3.5.	Условные законы распределения	118
3.6.	Числовые характеристики двумерной случайной величины.
Математическое ожидание и дисперсия	122
3.7.	Корреляционный момент, коэффициент корреляции	124
3.8.	Двумерное нормальное распределение	131
3.9.	Регрессия. Теорема о нормальной корреляции	135
3.10.	Многомерная (n-мерная) случайная величина (общие сведения)	139
3.11.	Характеристическая функция и ее свойства	140
3.12.	Характеристическая функция нормальной случайной величины	143
Глава 4. Функции случайных величин
4.1.	Функция одного случайного аргумента			145
4.2.	Функции двух случайных аргументов	 150
4.3.	Распределение функций нормальных случайных величин	 L58
Глава 5. Предельные теоремы теории вероятностей
5.1.	Неравенство Чебышева	162
5.2.	Теорема Чебышева	165
5.3.	Теорема Бернулли	168
5.4.	Центральная предельная теорема	170
5.5.	Интегральная теорема Муавра-Лапласа	172
Глава 6. Основы теории случайных процессов
6.1.	Понятие случайной функции (процесса)	176
6.2.	Классификация случайных процессов	178
6.3.	Основные характеристики случайного процесса 	179
6.4.	Стационарный случайный процесс в узком и широком смысле	187
6.5.	Линейные и нелинейные преобразования случайных процессов		190
6.6.	Дифференцирование и интегрирование случайных процессов	191
6.7.	Спектральное разложение стационарного случайного процесса	194
6.8.	Спектральная плотность случайного процесса. Теорема
Винера Хинчина	197
6.9.	Стационарный белый шум		201
6.10.	Понятие марковского случайного процесса	203
6.11.	Дискретный марковский процесс. Цепь Маркова	205
6.12.	Понятие о непрерывном марковском процессе. Уравнения
Колмогорова			207

Содержание ■ 5
Раздел второй
Основы математической статистики
Глава 7. Выборки и их характеристики
7.1.	Предмет математической статистики	212
7.2.	Генеральная и выборочная совокупности 	213
7.3.	Статистическое распределение выборки. Эмпирическая функция
распределения	215
7.4.	Графическое изображение статистического	распределения	219
7.5.	Числовые характеристики статистического	распределения	221
Глава 8. Элементы теории оценок и проверки гипотез
8.1.	Оценка неизвестных параметров	225
8.2.	Методы нахождения точечных оценок	231
8.3.	Понятие интервального оценивания параметров	236
8.4.	Доверительные интервалы для параметров нормального
распределения	238
8.5.	Проверка статистических гипотез			244
8.6.	Проверка гипотез о законе распределения		248
Ответы к упражнениям	255
Приложения	284

Введение
Теория вероятностей, как и другие науки, возникла из потребностей прак¬
тики. Ее элементы были «знакомы» еще первобытным людям: шансы убить
зверя у двух охотников, конечно, больше, чем у одного.
Возникновение «математики случайного» относится к середине XVII века
и связано с попыткой создания теории азартных игр, особенно в кости.
Пример одной из ситуаций: два игрока договорились играть в кости до мо¬
мента, когда одному из них удастся выиграть три партии; игра была прервана,
когда первый игрок выиграл две партии, а второй — одну; как справедливо
разделить ставку? 3:1 — как показали французские математики Б. Паскаль
(1623-1662) и П. Ферма (1601 1665). И в настоящее время примеры из области
азартных игр широко применяются в теории вероятностей, так как для них
легко строить математические модели.
Первую книгу по теории вероятностей «О расчетах в азартной игре» опу¬
бликовал голландский математик X. Гюйгенс (162& 1695).
Становление теории вероятностей как математической науки связано с
именем Я. Бернулли (1654 -1705), который ввел классическое определение со¬
бытия и доказал простейший случай закона больших чисел.
В XYIII XIX веках центральное место в развитии теории вероятностей
занимали предельные теоремы. К этому периоду относятся работы А. Муавра
(1667-1754), П. Лапласа (1749-1827), К. Гаусса (1777 1855), С. Пуассона (1781-
1840).
В конце XIX — начале XX века благодаря усилиям П. JI. Чебышева (1821 -
1894), А. А. Маркова (1856 L922), А. М. Ляпунова (1857-1918) созданы методы
доказательства предельных теорем для сумм независимых произвольно рас¬
пределенных случайных величин.
Дальнейшее развитие теории вероятностей связано с именами русских
математиков Е. Е. Слуцкого (1880- 1948), А. Я. Хинчина (1894 1959), А. Н. Кол¬
могорова (1903-1987), Б. В. Гнеденко (1912 1995), а также зарубежных уче¬
ных Н. Винера (1894 1964), Э. Бореля (1871-1956), В.Феллера (1906 1970),
Р. Фишера (1890 1962) и др. Теория вероятностей получила строгое формаль¬
но-логическое основание на базе теории множеств. Следует особо отметить
академика А. Н. Колмогорова, установившего аксиоматику теории вероятно¬
стей. Огромное развитие получили «отпочковавшиеся» от теории вероятно¬
стей такие отрасли науки, как математическая статистика, теория случайных
процессов, теория массового обслуживания, теория информации и др.
Современная теория вероятностей — строго обоснованная математиче¬
ская наука. Она широко использует достижения других математических наук
(по этому поводу современный вероятностик Дж. Дуб в шутку как-то сказал:
«Всем специалистам по теории вероятностей хорошо известно, что математи¬
ка представляет собой часть теории вероятностей»); имеет, в свою очередь,
многочисленные приложения в естественных и гуманитарных науках.

Элементарная
теория	
вероятностей
и случайных
процессов	
Раздел первый

Глава 1
Случайные события
1.1.	Предмет теории вероятностей
Любая точная наука изучает не сами явления, протекающие в при¬
роде, в обществе, а их математические модели, т. е. описание явлений
при помощи набора строго определенных символов и операций над ни¬
ми. При этом для построения математической модели реального явле¬
ния во многих случаях достаточно учитывать только основные фак¬
торы, закономерности, которые позволяют предвидеть результат опы¬
та (наблюдения, эксперимента) по его заданным начальным условиям.
Этим и занимаются большинство математических (и других) дисцип¬
лин. Обнаруженные закономерности явления называются детермини¬
стическими (определенными). Так, например, формула
позволяет найти путь, пройденный свободно падающим телом за t се¬
кунд от начала движения.
Однако есть множество задач, для решения которых приходится
(надо!) учитывать и случайные факторы, придающие исходу опыта
элемент неопределенности. Например, в вопросах стрельбы по цели не¬
возможно без учета случайных факторов ответить на вопрос: сколько
ракет нужно потратить для поражения цели? Невозможно предсказать,
какая сторона выпадет при бросании монеты. Сколько лет проживет
родившийся сегодня ребенок? Сколько времени проработает куплен¬
ный нами телевизор? Сколько студентов опоздают на лекцию по тео¬
рии вероятностей? И т.д. Такие задачи, исход которых нельзя пред¬
сказать с полной уверенностью, требуют изучения не только основ¬
ных, главных закономерностей, определяющих явление в общих чер¬
тах, но и случайных, второстепенных факторов. Выявленные в таких
задачах (опытах) закономерности называются статистическими (или

Глава 1. Случайные события ■ 9
вероятностными). Статистические закономерности исследуются мето¬
дами специальных математических дисциплин —- теории вероятностей
и математической статистики.
Теория вероятностей — математическая наука, изучающая зако¬
номерности, присущие массовым случайным явлениям. При этом из¬
учаемые явления рассматриваются в абстрактной форме, независимо
от их конкретной природы. То есть теория вероятностей рассматрива¬
ет не сами реальные явления, а их упрощенные схемы — математиче¬
ские модели. Предметом теории вероятностей являются математи¬
ческие модели случайных явлений. При этом под случайным явлением
понимают явление, предсказать исход которого невозможно (при не¬
однократном воспроизведении одного и того же опыта оно протекает
каждый раз несколько по-иному). Примеры случайных явлений: вы¬
падение герба при подбрасывании монеты, выигрыш по купленному
лотерейному билету, результат измерения какой-либо величины, дли¬
тельность работы телевизора и т. п.
Цель теории вероятностей — осуществление прогноза в области
случайных явлений, влияние на ход этих явлений, контроль их, огра¬
ничение сферы действия случайности. В настоящее время нет практи¬
чески ни одной области науки, в которой в той или иной степени не
применялись бы вероятностные методы.
1.2.	Случайные события, их классификация
Сначала определим понятие «случайное событие» исходя из его ин¬
туитивного, наглядного понимания. Пусть проводится некоторый опыт
(эксперимент, наблюдение, испытание), исход которого предсказать за¬
ранее нельзя. Такие эксперименты в теории вероятностей называют
случайными. При этом рассматриваются только такие эксперименты,
которые можно повторять, хотя бы теоретически, при неизменном ком¬
плексе условий произвольное число раз.
Случайным событием (или просто: событием) называется любой
исход опыта, который может произойти или не произойти.
События обозначаются, как правило, заглавными буквами латин¬
ского алфавита: А, В, С, ... .
Пример 1.1. Опыт: бросание игральной кости; событие А — выпа¬
дение 5 очков, событие В — выпадение четного числа очков, событие
С — выпадение 7 очков, событие D — выпадение целого числа очков,
событие Е — выпадение не менее 3-х очков,	
О

10 я Раздел первый. Элементарная теория вероятностей и случайных процессов
Непосредственные исходы опыта называются элементарными со¬
бытиями и обозначаются через го. Элементарные события (их назы¬
вают также «элементами», «точками», «случаями») рассматриваются
как неразложимые и взаимоисключающие исходы W2, 'w.\ •. .этого
опыта.
Множество всех элементарных событий называется пространст¬
вом элементарных событий или пространством исходов, обозначает¬
ся через il.
Рассмотрим пример 1.1. Здесь 6 элементарных событий W],	w%,
ЭД4, г^5, we- Событие иц означает, что в результате бросания кости вы¬
пало г очков, г = 1,2.3,4,5.6. Пространство элементарных событий
таково: Q = {wi,W2,W3,W4iW5,wg} или S2 = {1,2,3,4,5,6}.
Событие называется достоверным, если они обязательно наступит
в результате данного опыта, обозначается через fi.
Событие называется невозможным. если оно заведомо не произой¬
дет в результате проведения опыта, обозначается через 0.
В примере 1.1 события А и В — случайные, событие С — невоз¬
можное, событие D — достоверное.
Два события называются несовместными. если появление одного
из них исключает появление другого события в одном и том же опыте,
т. е. не смогут произойти вместе в одном опыте. В иротивнохМ случае
события называются совместными.
Так, в примере 1.1 события А и В — несовместные, А и Е — со¬
вместные.
События Ai, А2, ..., Ап называются попарно-несовместными, если
любые два из них несовместны.
Несколько событий образуют полную группу, tic л и они попарно не¬
совместны и в результате каждого опыта происходит одно и только
одно из них.
В примере 1.1 события w\ образуют полную группу, w\ w§ —
нет.
Несколько событий в данном опыте называются равновозможны¬
ми, если ни одно из них не является объективно более возможным, чем
другие, т. е. все события имеют равные «шансы».
В примере 1.1 элементарные события ш\, ^2, гиз, w4, w$, wq равно¬
возможны. Вынадение герба (А) или решки (В) при бросании монеты —
равновозможные события, если, конечно, монета имеет симметричную
форму, не погнута, ....

Глава 1. Случайные события ■ 11
1.3.	Действия над событиями
Введем основные операции над событиями; они полностью соответ¬
ствуют основным операциям над множествами.
Суммой событий Аи В называется событие С = А + В, состоящее
в наступлении хотя бы одного из них (т. е. или Л, или £?, или А и В
вместе).
Произведением событий А и В называется событие С = А * В,
состоящее в совместном наступлении этих событий (т. е. и А и В одно¬
временно).
Разностью событий А и В называется событие С = А — В, про¬
исходящее тогда и только тогда, когда происходит событие Л, но не
происходит событие В.
Противоположным событ,ию А называется событие Л, которое
происходит тогда и только тогда, когда не происходит событие А (т. е.
А означает, что событие А не наступило).
Событие А влечет событие В (или А является частным случаем
В), если из того, что происходит событие Л, следует, что происходи!
событие В; записывают Л С В.
Если Л С В и В С Л, то события Л и В называются равными;
записывают Л = В.
Так, в примере 1.1 (п. 1.2) В = {2,4,6}, Е = {3,4,5.6}, Л = {5},
D = {1,2,3,4,5,6}. Тогда: В 4- Е = {2,3,4,5,6}, В-Е = {4,6},
В-Е = {2}, Л = {1,2,3,4,6}, ВСД/) = Й = {1,2,3,4,5.6}.
События и действия над ними можно наглядно иллюстрировать с
помощью диаграмм Эйлера-Венна: достоверное событие ft изобража¬
ется прямоугольником; элементарные случайные события — точками
прямоугольника; случайное событие — областью внутри него.
Действия над событиями можно изобразить так, как показано на
рис. 1-5.
Операции над событиями обладают следующими свойствами:
А -\- В — В А. А - В = В	А (переместительное);
(Л + В) • С = Л • С + В • С\	Л - В + С	=	(Л +	С)	-	(В + С) (распреде¬
лительное);
{А + В) + С = А + (В + С), (Л • В) • С = А • (В • С) (сочетательное);
Л + Л = Л, Л-Л-Л:
А + £} = £}, А • ft = А;
А + А = П,А-А = 0
0 = ft, ft = 0, Л = Л
Л - В = Л - В;
А-\-В = А'ВиА'В = А + В — законы де Моргана.

12 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Рис. 1
Рис. 2
Рис. 3
Рис. ^
Рис. 5
В их справедливости можно убедиться с помощью диаграмм Эйле-
ра-Венна.
Пример 1.2. Доказать формулу А + В = А + АВ.
О Используя некоторые из выше приведенных правил, получаем:
А+В = (А+В)-П - А-П+B-Sl = А-0 + Б-(А+А) = А-0 + (А + Л)-В =
= A-SI +А-В+ А-В = (П + В)- А + ~А- В = А + А- В = А + А- В.
Таким образом, сумму любых deyx событий можно представить в
виде суммы двух несовместных событий.
Геометрическое доказательство представлено на рис. 6.	•
	
А + В
А + АВ
Рис. 6

Глава 1. Случайные события ■ 13
Упражнения
1-	Доказать формулы: 1) В = А-В-\~А-В\ 2) (А+С)-(В+С) = А-В+С;
Ъ)АТВ = АВ.
2.	Пусть Л, В и С — три произвольных события. Выразить через них
следующие события: а) произошли все три события; б) произошло
только С\ в) произошло хотя бы одно из событий; г) ни одного
события не произошло; д) произошли Л и В, но С не произошло;
е) произошло одно из этих событий; ж) произошло не более двух
событий.
3- Релейная схема (рис. 7) состоит из 6 элементов. Пусть события А\
(i = 1,6) состоят в том, что соответствующие элементы работают
безотказно в течение времени Т. Выразить через Ai событие, состо¬
ящее в том, что схема за время Т работает безотказно.
Рис. 7
1.4.	Случайные события. Алгебра событий.
(Теоретико-множественная трактовка)
Определим теперь основные понятия теории вероятностей, следуя
теоретико-множественному подходу, разработанному академиком Кол¬
могоровым А. Н. в 1933 году.
Пусть производится некоторый опыт со случайным исходом.
рч|	Множество Л = {и;} всех возможных взаимоисключающих исхо¬
дов данного опыта называется пространством элементарных событий
(коротко: ПЭС), а сами исходы и — элементарными событиями (или
«элементами», «точками»).

14 1 Раздел первый. Элементарная теория вероятностей и случайных процессов
Случайным событием, А (или просто событием, А) называется лю¬
бое подмножество множества fi, если £1 конечно или счетно (т. е. эле¬
менты этого множества можно пронумеровать с помощью множества
натуральных чисел): А С Q.
Элементарные события, входящие в подмножество А прос транства
называются благоприятствуюгцими событию А.
Множество fi называется достоверным событием. Ему благопри¬
ятствует любое элементарное событие: в результате опыта оно обяза¬
тельно произойдет.
Пустое множество 0 называется невозможным, событием; в резуль¬
тате опыта оно произойти не может.
Пример 1.3, Опыт: один раз бросают игральную кость. В этом случае
ПЭС таково: fi = {1,2,3,4, 5,6} или 12 = {w\ .w2,	}, где *ац — эле¬
ментарное событие, состоящее в выпадении грани с Точками (г = 1,6).
В данном случае fI конечно. Примером события А является, например,
выпадение нечетного числа очков; очевидно, что А =	со¬
бытию А благоприятствуют элементарные события Ш], t/73, Ш5. Однако
если нас интересует только факт выпадения четного числа очков, то
ПЭС можно построить иначе: О = {а; 1,^2}? где ио\ — выпадение четного
числа очков, ио2 — нечетного.
Пример 1.4. Опыт: стрельба по цели до первого попадания. Тогда
^	где	П	означает	попадание	в	цель,
Н — непопадание. Исходов у этого опыта бесконечно (теоретически);
Q счетно.
Пример 1.5. Опыт: наблюдение за временем безотказной работы не¬
которого агрегата. В этом случае в качестве результата может появить¬
ся любое число t ^ 0; время t меняется непрерывно: ПЭС таково:
П = {/,0 ^ t < ос}. Исходов у этого опыта бесконечно, Q несчетно
(континуально).
Над событиями можно проводить все операции, выполнимые для
множеств.
Сумм,а, (или объединение) двух событий А £ ft и В £ ft (обознача¬
ется А + В или A U В) — это множество, которое содержит элементы,
принадлежащие хотя бы одному из событий А и В.
Произведение двух событий А £ ft и В £ И (обозначается АВ или
АП В) — эти множество, которое содержит элементы, общие для собы¬
тий А и В.

№ G8
Глава 1. Случайные события *15
Разность событий A Е ft и В Е Л (обозначается А — В или Л\£?) —
это множество, которое содержит элементы события Л, не принадле¬
жащие событию В.
Противоположное событию Л Е ft событие Л = Л\Л. (Л называют
также дополнением множества Л.)
Событие А влечет событие В (обозначается Л С В), если каждый
элемент события Л содержится в В.
По определению: 0 С Л для любого А.
События Л и В называются несовместными, если их произведение
есть невозможное событие, т. е. Л • В = 0.
Несколько событий Лх, Лг,-. -, Ап образуют полную группу несов¬
местных событий, если их сумма представляет все ПЭС, а сами события
п
несовместны, т. е. ^ Л?; = ft и Л? * Л?- = 0 (г 7^ j).
*=1
Полную группу образуют, например, события Л и Л (Л + Л =
Л • А = 0).
В случае несчетного пространства Л в качестве событий рассмат¬
риваются не все подмножества ft. а лишь некоторые классы этих под¬
множеств, называемые алгебрами и сг-алхебрами множеств.
Класс S подмножеств пространства $1 называется алгеброй мно¬
жеств (событий), если:
1.	0 Е S, ft Е 5:
2.	из Л Е S' вытекает, что Л Е 5;
3.	из Л Е S', В Е S вытекает, что Л 4- В Е S, Л • В Е S'.
Заметим, что в условии 3 достаточно требовать либо Л	+	В	Е	S,
либо Л£? Е S, так как Л + £? = Л • Б, Л • .В = Л + I?.
Алгебру событий образует, например, система подмножеств	S	=
= {0.$!}. Действительно, в результате применения любой из вышепри¬
веденных операций к любым двум элементам класса S снова получа¬
ется элемент данного класса: 0 ft = ft, 0 • ft = 0, 0 = ft, ft = 0.
При расширении операций сложения и умножения на случай счет¬
ного множества алгебра множеств S называется гг-алгеброй,
ос	оо
если из Ап £ S, п = 1,2,3,..., следует	£	S,	П	А„. £ 5 (доота-
72— 1	П=1
ОО	оо
точно требовать либо £ Лп Е S, либо fj Ап Е S).
ГС—1	71 — 1
Множество всех подмножеств множества ft, если оно конечно или
счетно, образует алгебру.

16 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
1.5.	Свойство статистической устойчивости
относительной частоты события
Пусть в п повторяющихся опытах некоторое событие А наступило
тьд раз.
Число па называется частотой события А, а отношение
Ц£ = р*{А)	(1.1)
называется относительной частотой (или частостью) события А в
рассматриваемой серии опытов.
Относительная частота события обладает следующими свойствами:
1.	Частость любого события заключена между нулем и единицей, т. е.
О < Р*(А) ^ 1.
2.	Частость невозможного события равна нулю, т. е.
Р*(0) = 0.
3.	Частость достоверного события равна 1, т. е.
Р*(П) = 1.
4.	Частость суммы двух несовместных событий равна сумме часто¬
стей этих событий, т. е. если АВ = 0, то
Р*(А + В) = Р*(А) + Р*(В).
□	Свойства очевидны, так как 0 ^	^ п для любого события А; для
невозможного события пд = 0; для достоверного события пд = п; если
события А и В несовместны (АВ = 0), то пд+в = па + пв, следова-
телыю, Р*(А + В) =	= ПАп ПВ = Ж + Ж = Р*^ + р*(в)- ■
Частость обладает еще одним фундаментальным свойством, назы¬
ваемым свойством статистической устойчивости: с увеличением чи¬
сла опытов (т. е. п) она принимает значения, близкие к некоторому по¬
стоянному числу (говорят: «частость стабилизируется, приближаясь
к некоторому числу», «частость колеблется около некоторого числа»,
или «ее значения группируются около некоторого числа»).
Так, например, в опыте — бросание монеты (однородной, симмет¬
ричной, ...) — относительная частота появления герба при 4040 броса-
2048
ниях (Ж. Бюффон) оказалась равной 0,5069 = ^q^q, а в опыте с 12000

Глава 1. Случайные события «17
и 24000 бросаниями (К. Пирсон) она оказалась равной соответствен¬
но 0,5015 =	И	=	2^000* Т'е’ частность приближается к
числу ^ = 0,500	А частость рождения мальчика, как показывают
наблюдения, колеблется около числа 0,515.
Отметим, что теория вероятностей изучает только те массовые слу¬
чайные явления с неопределенным исходом, для которых предполага¬
ется наличие устойчивости относительной частоты.
1.6.	Статистическое определение вероятности
Для математического изучения случайного события необходимо
ввести какую-либо количественную оценку события. Понятно, что одни
события имеют больше шансов («более вероятны») наступить, чем дру¬
гие. Такой оценкой является вероятность события, т. е. число, выра¬
жающее степень возможности его появления в рассматриваемом опыте.
Математических определений вероятности существует несколько, все
они дополняют и обобщают друг друга.
Рассмотрим опыт, который можно повторять любое число раз (го¬
ворят: «проводятся повторные испытания»), в котором наблюдается
некоторое событие А.
Рч|	Статистической	вероятностью	события А называется число,
около которого колеблется относительная частота события А при до¬
статочно большом числе испытаний (опытов).
Вероятность события А обозначается символом Р(А). Согласно
данному определению
Р(Л)«Р*(Л) = ^.	(1.2)
Математическим обоснованием близости относительной частоты Р*(А)
и вероятности Р(А) некоторого события А служит теорема Я. Бернулли
(см. п. 5.3).
Вероятности Р(А) приписываются свойства 1—4 относительной ча¬
стоты:
1.	Статистическая вероятность любого события заключена между ну¬
лем и единицей, т. е.
0 < Р(А) < 1.
2.	Статистическая вероятность невозможного события равна нулю,
т. е.
Р(0) = 0.

18 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
3.	Статистическая вероятность достоверного события равна единице,
т. е.
Р(П) = 1.
4.	Статистическая вероятность суммы несовместных событий равна
сумме вероятностей этих событий, т. е. если А • В = 0, то
Р(А + В) = Р(А)+Р(В).
Статистический способ определения вероятности, опирающийся на
реальный опыт, достаточно полно выявляет содержание этого поня¬
тия. Некоторые ученые (Р. Мизес и другие) считают, что эмпирическое
определение вероятности (т. е. р = lim Ц~) следует считать основным
п—>оо 11
определением вероятности.
Недостатком статистического определения является неоднознач¬
ность статистической вероятности; так в примере с бросанием монеты
(п. 1.5) в качестве вероятности можно принять не только число 0,5, но
и 0,49 или 0,51 и т.д. Для надежного определения вероятности нуж¬
но проделать большое число испытаний (опытов), что не всегда просто
(или дешево).
1.7.	Классическое определение вероятности
Существует простой способ определения вероятности события,
основанный на равновозможности любого из конечного числа исходов
опыта. Пусть проводится опыт с п исходами, которые можно предста¬
вить в виде полной группы несовместных равновозможных событий.
Такие исходы называются случаями, шансами, элементарными собы¬
тиями, опыт — классическим. Про такой опыт говорят, что он сводится
к схеме случаев или схеме урн (ибо вероятностную задачу для такого
опыта можно заменить эквивалентной ей задачей с урнами, содержа¬
щими шары разных цветов).
Рч|	Случай о;, который приводит к наступлению события А, называет¬
ся благоприятным (или — благоприятствующим) ему, т. е. случай w
влечет событие A: w С А.
Рч|	Вероятностью события А называется отношение числа т случаев,
благоприятствующих этому событию, к общему числу п случаев, т. е.
Р(А) = Щ,	(1.3)
Наряду с обозначением Р{А) для вероятности события А используется
обозначение р, т. е. р = Р{А).

Глава 1. Случайные события "19
Из классического определения вероятности (1.3) вытекают следу¬
ющие свойства:
1.	Вероятность любого события заключена между нулем и единицей,
т. е.
4.	Вероятность суммы несовместных событий равна сумме вероятно¬
стей этих событий, т. е. если А ■ В = 0, то
Они проверяются так же, как и для относительной частоты (п. 1.5).
В настоящее время свойства вероятности определяются в виде аксиом
(см. п. 1.11).
Пример 1.6. В урне (емкости) находятся 12 белых и Ь черных шаров.
Какова вероятность того, что наудачу вынутый шар будет белым?
О	Пусть А — событие, состоящее в том, что вынут белый шар. Яс¬
но, что п = 12 4- 8 = 20 — число всех равновозможных случаев (исхо¬
дов опыта). Число случаев, благоприятствующих событию Л, равно 12,
12
т. е. 771 = 12. Следовательно, по формуле (1.3) имеем: Р(А) = ^q, т. е.
1	- Найти вероятность тою, что в наудачу написанном двузначном чиг-
сле цифры разные.
2.	Набирая номер телефона, абонент забыл 2 последние цифры и на¬
брал их наугад. Найти вероятность того, что набраны нужные ци¬
фры.
0 < Р(А) ^ 1.
2.	Вероятность невозможного события равна нулю. т. е.
Р{0) = П.
3.	Вероятность достоверного события равна единице, т. е.
Р(П) = 1.
Р{А + В) = Р(А) + Р(В).
Р(А) =0,6.
Упражнения

20 " Раздел первый. Элементарная теория вероятностей и случайных процессов
Согласно классическому определению подсчет вероятности собы¬
тия А сводится к подсчету числа благоприятствующих ему исходов.
Делают это обычно комбииаторными методами.
Комбинаторика — раздел математики, в котором изучаются за¬
дачи выбора элементов из заданного множества и расположения их в
группы по заданным правилам, в частности задачи о подсчете числа
комбинаций (выборок), получаемых из элементов заданного конечно¬
го множества. В каждой из них требуется подсчитать число возмож¬
ных вариантов осуществления некоторого действия, ответить на вопрос
«сколькими способами?».
Многие комбинаторные задачи могут быть решены с помощью сле¬
дующих двух важных правил, называемых соответственно правилами
умножения и сложения.
Правило умножения (основной принцип): если из некоторого ко¬
нечного множества первый объект (элемент х) можно выбрать щ спо¬
собами и после каждого такого выбора второй объект (элемент у) мож¬
но выбрать П2 способами, то оба объекта (х и у) в указанном порядке
можно выбрать п\ • способами.
Этот принцип, очевидно, распространяется на случай трех и более
объектов.
Пример 1.7. Сколько трехзначных чисел можно составить из цифр 1.
2,	3. 4, 5, если: а) цифры не повторяются? б) цифры могут повторяться?
О Имеется 5 различных способов выбора цифры для первого места
(слева в трехзначном числе). После того как первое место занято, на¬
пример, цифрой 2, осталось четыре цифры для заполнения второго
места. Для заполнения третьего места остается выбор из трех цифр.
Следовательно, согласно правилу умножения имеется 5 • 4 • 3 = 60 спо¬
собов расстановки цифр, т. е. искомое число трехзначных чисел есть
60. (Вот некоторые из этих чисел: 243, 541, 514, 132, ....) Понятно, что
если цифры могут повторяться, то трехзначных чисел 5-5*5 = 125.
(Вот некоторые из них: 255, 333, 414, 111, ...)	•
Правило суммы. Если некоторый объект х можно выбрать п\ спосо¬
бами, а объект у можно выбрать П2 способами, причем первые и вторые
способы не пересекаются, то любой из указанных объектов (х или у),
можно выбрать п\ 4- пч способами.
Это правило распространяется на любое конечное число объектов.
1.8.	Элементы комбинаторики

Глава 1. Случайные события "21
Пример 1.8. В студенческой группе 14 девушек и 6 юношей. Сколь¬
кими способами можно выбрать, для выполнения различных заданий,
двух студентов одного пола?
о По правилу умножения двух девушек можно выбрать 14 ■ 13 = 182
способами, а двух юношей — 6 • 5 = 30 способами. Следует выбрать
двух студентов одного пола: двух студенток или двух юношей. Соглас¬
но правилу сложения таких способов выбора будет 182 + 30 = 212.	#
Решение вероятностных (и не только их) задач часто облегчается,
если использовать комбинаторные формулы. Каждая из них опреде¬
ляет число всевозможных исходов в некотором опыте (эксперименте),
состоящем в выборе наудачу га элементов из п различных элементов
рассматриваемого множества.
Существуют две схемы выбора га элементов (0 < га ^ п) из исход¬
ного множества: без возвращения (без повторений) и с возвращением (с
повторением). В первом случае выбранные элементы не возвращаются
обратно; можно отобрать сразу все га элементов или последовательно
отбирать их по одному. Во второй схеме выбор осуществляется поэле¬
ментно с обязательным возвращением отобранного элемента на каждом
шаге.
Схема выбора без возвращений
Пусть дано множество, состоящее из п различных элементов.
Рч|	Размещением	из	п	элементов	по	га	элементов	(0	<	га	^	п)	назы¬
вается любое упорядоченное подмножество данного множества, содер¬
жащее га элементов.
Из определения вытекает, что размещения — это выборки (комби¬
нации), состоящие из т элементов, которые отличаются друг от друга
либо составом элементов, либо порядком их расположения.
Число размещений из п элементов по га элементов обозначается
символом А™ («А из эн по эм») и вычисляется по формуле
А™ — п(п — 1)(п — 2) • ... • (п — га + 1)	(1.4)
или
АП = 7—~Ц|>	где п\ = 1 • 2 - 3 •... • n, 1! = 1,	0!	=	1.	(1.5)
(п — ту.

22 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
□	Для составления размещения А™ надо выбрать m элементов из
множества с п элементами и упорядочить их, т. е. заполнить га мест
элементами множества. Первый элемент можно выбрать п способами,
т. е. на первое место можно поместить любой из п элементов. После
этого второй элемент можно выбрать из оставшихся п — 1 элементов
п — 1 способами. Для выбора третьего элемента имеется п — 2 способа,
четвертого — п — 3 способа, и, наконец, для последнего га-го элемен¬
та — (и — (га — 1)) способов. Таким образом, по правилу умножения,
существует п(п — 1)(п — 2)... (п — (га — 1)) способов выбора га элементов
из данных п элементов, т. е. А™ = п(п — 1)(п — 2)... (п — га + 1).	■
Пример 1.9. Составить различные размещения по 2 из элементов мно¬
жества D = {а,Ь, с}; подсчитать их число.
О Из трех элементов можно образовать следующие размещения по
два элемента: (а, 6), (Ь,а), (а, с), (с, а), (Ь,с), (с, Ь). Согласно форму¬
ле (1.4) их число: А$ = 3 • 2 = 6.	•
Перестановкой из п элементов называется размещение из п эле¬
ментов по п элементов.
Из определения вытекает, что перестановки — это выборки (ком¬
бинации), состоящие из тг элементов и отличающиеся друг от друга
только порядком следования элементов. Число перестановок из п эле¬
ментов обозначается символом Рп («пэ из эн») и вычисляется по фор¬
муле
Рп = nl	(1.6)
Формула (1.6) следует из определения перестановки:
р — лп —		-.гй |
^ лп - (п _ п)!	0!	п"
Пример 1.10. Составить различные перестановки из элементов мно¬
жества Е = {2,7,8}; подсчитать их число.
о Из элементов данного множества можно составить следующие пе¬
рестановки: (2,7,8); (2,8,7); (7,2,8); (7,8,2); (8,2,7); (8,7,2). По фор¬
муле (1.6) имеем: Рз = 3! = 1 • 2 • 3 = 6.	•
Пример 1.11. Сколькими способами можно расставить на полке 5
различных книг?

Глава 1. Случайные события ■ 23
о Искомое число способов равно числу перестановок из 5 элементов
(книг), т. е. Р$ — 5! = 1 • 2 • 3 • 4 • 5 = 120.	•
ф	Сочетанием из п элементов по га (0 < га ^ п) элементов назы¬
вается любое подмножество, которое содержит т элементов данного
множества.
Из определения вытекает, что сочетания — это выборки (комбина¬
ции), каждая из которых состоит из т элементов, взятых из данных
п элементов, и которые отличаются друг от друга хотя бы одним эле¬
ментом, т. е. отличаются только составом элементов.
Число сочетаний из п элементов по т элементов обозначается сим¬
волом С™ («цэ из эн по эм») и вычисляется по формуле
/тш _ «(« - 1)(« - 2)... (п - т + 1)	м	^
1-2-3...т	(	>
или
(1'8)
Q Число А™ размещений из п элементов по га элементов можно най¬
ти следующим образом: выбрать га элементов из множества, содержа¬
щего п элементов (это можно сделать С™ способами); затем в каж¬
дом из полученных сочетаний (подмножеств) сделать все перестанов¬
ки для упорядочения подмножеств (это можно сделать Рш способа¬
ми). Следовательно, согласно правилу умножения, можно записать:
AS = С? • Отсюда С? = g =	или
sjm _ 	тЛ	
71	га! (го — га)!
Можно показать, что имеют место формулы:
С? = (%-т, (т < го),	(1.9)
с£ + С£ + С* + ... + С£ = 2",	(1.10)
с™ = CZ-x + с^\	(1 < т < п).	(1.11)
Формулу (1.9) удобно использовать при вычислении сочетаний, когда
га > Так, СЦ = Cf5 =	^ — Ю5. Формула (1.10) выражает число
всех подмножеств множества из п элементов (оно равно 2П). Числа
CyJ, (7^, ..., С™ являются коэффициентами в разложении бинома
Ньютона: (а + Ь)п = C%anb° + C^an~lb + ... + C%a°bn.

24 а Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 1.12. Составить различные сочетания по 2 из элементов мно¬
жества D = {а,Ь, с}; подсчитать их число.
О Из трех элементов можно образовать следующие сочетания по два
Q * ^
элемента: (а, 6); (а, с); (Ь, с). Их число: (7| =	=	3	(формула	(1-7)).
•
Пример 1.13. Сколькими способами можно выбрать 3 цветка из вазы,
в которой стоят 10 красных и 4 розовых гвоздики? А если выбрать 1
красную гвоздику и 2 розовых?
О Так как порядок выбора цветов не имеет значения, то выбрать 3
цветка из вазы, в которой стоят 14 гвоздик, можно C‘f4 способами. По
формуле (1.7) находим: Cf4 = ^ ^ ^ = 7 • 13 - 4 = 364. Далее: крас¬
ную гвоздику можно выбрать С\0 = 10 способами. Выбрать две розовые
с 4*3
гвоздики из имеющихся четырех можно С\ —	~	®	способами.	По¬
этому букет из одной красной и двух розовых гвоздик можно составить,
по правилу умножения, С{$ • С\ — 10 • 6 = 60 способами.	•
Схема выбора с возвращением
Если при выборке т элементов из п элементы возвращаются обрат¬
но и упорядочиваются, то говорят, что это размещения с повторе¬
ниями.
Размещения с повторениями могут отличаться друг' от друга эле¬
ментами, их порядком и количеством повторений элементов. Число
всех размещений из п элементов по т с повторениями обозначается
символом А™ и вычисляется по формуле
Л™ = nm.	(1.12)
Пример 1.14. Из 3 элементов a, fe, с составить все размещения по два
элемента с повторениями.
о По формуле (1.12) число размещений по два с повторениями равно
-Д§ = З2 = 9. Это:'(о, а), (а, 6), (а, с), (6,6), (6, а), (6, с), (с, с), (с. а), (с. 6).
•

Глава 1. Случайные события ■ 25
Пример 1.15. Сколько пятизначных чисел можно составить, исполь¬
зуя цифры: а) 2, 5, 7, 8; б) 0, 1, 9?
О а) Все пятизначные числа, составленные из цифр 2, 5, 7, 8, отли¬
чаются друг от друга либо порядком их следования (например, 25558
и 52855), либо самими цифрами (например, 52788 и 78888). Следова¬
тельно, они являются размещениями из 4 элементов по 5 с повторени¬
ями, т. е. А\. Таким образом, искомое число пятизначных чисел равно
= 4° = 1024. Этот же результат можно получить, используя правило
умножения: первую цифру слева в пятизначном числе можно выбрать
четырьмя способами, вторую — тоже четырьмя способами, третью —
четырьмя, четвертую — четырьмя, пятую — четырьмя. Всего получа¬
ется 4 ■ 4 • 4 • 4 ■ 4 = 1024 пятизначных чисел.
б)	Если пятизначные числа состоят из цифр 0, 1,9, то первую ци¬
фру слева можно выбрать двумяспособами (0 не может занимать пер¬
вую позицию), каждую из оставшихся четырех цифр можно выбрать
тремя способами. Согласно правилу умножения, таких чисел будет
2 • 3 ■ 3 ■ 3 ■ 3 = 162. (Иначе: А\-А\ = 243 - 81 = 162.)	•
Если при выборке т элементов из п элементы возвращаются обрат¬
но без последующего упорядочивания, то говорят, что это сочетания с
повторениями.
Число всех сочетаний из п элементов поте повторениями обозна¬
чается символом (7™ и вычисляется по формуле
сг = с?+т .1-	H is)
Пример 1.16. Из трех элементов а, 6, с составить все сочетания по
два элемента с повторениями.
о По формуле (1.13) число сочетаний по два с повторениями равно
~е	4 3
С* = С3+2- i = ^4 ~ у~2 ” Составляем эти сочетания с повторени¬
ями: (а, а), (а, 6), (а, с), (6,6), (6, с), (с, с).	•
Пример 1.17. Сколькими способами можно составить букет из 5 цве¬
тов, если в наличии есть цветы трех сортов?
О Рассматриваемое множество состоит из трех различных элементов,
а выборки имеют объем, равный 5. Поскольку порядок расположения
цветов в букете не играет роли, то искомое число букетов равно чи¬
слу сочетаний с повторениями из трех элементов по 5 в каждом. По
формуле (1.13) имеем С| =	= С?~5 = Су =	= 21.	•

26 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пусть в множестве с п элементами есть к различных элементов,
при этом 1-й элемент повторяется щ раз, 2-й элемент — П2 раз .... к-й
элемент — щ раз, причем щ + 77-2 + • •. Н- = п.
Перестановки из п элементов данного множества называют пере¬
становками с повторениями из п элементов.
Число перестановок с повторениями из п элементов обозначается
символом Рп{щ,П21 • ■ ■ ■>пк) и вычисляется по формуле
Рп{П1,П2,...,Пк) = —j—f	f.	(1.14)
Tl\!fl2i . . .
Пример 1.18. Сколько различных пятизначных чисел можно соста¬
вить из цифр 3, 3, 5, 5, 8?
О Применим формулу (1.14). Здесь п = 5, щ = 2,	—	2,	пз	=	1. Чи¬
сло различных пятизначных чисел, содержащих цифры 3. 5 и 8, равно
Р5(2,2.1) = 2! • 2! • 1! = 3°‘	#
Упражнения
1.	Сколько различных «слов», состоящих из трех букв, можно обра¬
зовать из букв слова БУРАН? А если «слова» содержат не менее
трех букв?
2- Сколькими способами можно выбрать один цветок из корзины, в
которой имеется 12 гвоздик, 15 роз и 7 хризантем?
3.	Группа студентов изучает 10 различных дисциплин. Сколькими
способами можно составить расписание занятий в понедельник,
если в этот день должно быть 4 разных занятия?
4.	Из 10 мальчиков и 10 девочек спортивного класса для участия в
эстафете надо составить три команды, каждая из которых состоит
из мальчика и девочки. Сколькими способами это можно сделать?
5-	Сколько можно составить четырехзначных чисел так, чтобы любые
две соседние цифры были различны?

Глава 1. Случайные события ■ 27
6.	В электричке 12 вагонов. Сколько существует способов размещения
4 пассажиров, если в одном вагоне должно быть не более одного
пассажира?
7.	Сколькими способами 3 награды могут быть распределены между
10	участниками соревнования?
8.	Из 4 первокурсников, 5 второкурсников и 6 третьекурсников надо
выбрать 3 студента на конференцию. Сколькими способами мож¬
но осуществить этот выбор, если среди выбранных должны быть
студенты разных курсов?
9.	Сколькими способами можно расставить на полке 7 различных
книг, чтобы определенные три книги стояли рядом? Не рядом?
10- Сколькими способами можно рассадить 5 человек за круглым сто¬
лом? (Рассматривается только расположение сидящих относитель¬
но друг друга.)
11.10 студентов, среди которых С. Федин и А. Шилов, случайным обра¬
зом занимают очередь в библиотеку. Сколько имеется вариантов
расстановки студентов, когда между Фединым и Шиловым окажут¬
ся 6 студентов?
12.У	одного школьника имеется 7 различных книг для обмена, а у
другого — 16. Сколькими способами они могут осуществить обмен:
книга на книгу? Две книги на две книги?
13.	В урне 12 белых и 8 черных шаров. Сколькими способами можно
выбрать 5 шаров, чтобы среди них было: а) 5 черных; б) 3 белых и
2 черных?
14.	Сколькими способами можно распределить 15 выпускников по трем
районам, если в одном из них имеется 8, в другом - 5 и в третьем —
2	вакантных места?
15- Известно, что 7 студентов сдали экзамен по теории вероятностей на
хорошо и отлично. Сколькими способами могли быть поставлены
им оценки?
16.	Игральная кость (на ее 6 гранях нанесены цифры от 1 до 6) бро¬
сается 3 раза. Сколько существует вариантов выпадения очков в
данном опыте? Напишите некоторые из них.

28 " Раздел первый. Элементарная теория вероятностей и случайных процессов
17.	Сколькими способами можно распределить 6 различных подарков
между четырьмя ребятишками?
18.	Сколькими способами можно составить набор из 6 пирожных, если
имеется 4 сорта пирожных?
19.	Группа учащихся из 8 человек отправляется в путешествие по Кры¬
му. Сколькими способами можно составить группу из учащихся 5- 7
классов?
20.	Сколькими способами можно распределить 4 книги на трех пол¬
ках книжного шкафа? Найти число способов расстановки книг на
полках, если порядок их расположения на полке имеет значение.
21.	Сколько «слов» можно получить, переставляя буквы в слове:
а)	ГОРА; б) ИНСТИТУТ?
22.	Сколько существует способов размещения 9 человек в двухмест¬
ный, трехместный и четырехместный номера гостиницы?
23.	Сколькими способами можно распределить 16 видов товаров по
трем магазинам, если в 1-й магазин надо доставить 9, во 2-й —
4,	а в третий — 3 вида товаров?
1.9.	Примеры вычисления вероятностей
Пример 1.19. В урне находятся 12 белых и 8 черных шаров. Найти
вероятность того, что среди наугад вынутых 5 шаров 3 будут черными?
О	Выбрать 5 шаров из 20 можно (7|0 различными способами (все вы¬
борки — неупорядоченные подмножества, состоящие из 5 элементов),
т. е. п = С|0. Определим число случаев, благоприятствующих событию
В — «среди 5 вынутых шаров 3 будут черными». Число способов вы¬
брать 3 черных шара из 8, находящихся в урне, равно (7|. Каждому
такому выбору соответствует С(2 способов выбора 2-х белых шаров из
12 белых в урне. Следовательно, по основному правилу комбинаторики
(правилу умножения), имеем: т = (7| * Cf2- По формуле (1.3) находим,
что Р{В) =	8	5	12	?г	0,24.	•
^20

Глава 1. Случайные события ■ 29
Пример 1.20. В коробке 5 синих, 4 красных и 3 зеленых карандаша.
Наудачу вынимают 3 карандаша. Какова вероятность того, что: а) все
они одного цвета; б) все они разных цветов; в) среди них 2 синих и 1
зеленый карандаш.
0	Сначала заметим, что число способов выбрать 3 карандаша из 12
имеющихся в наличии равно п = Cf2 = 220.
а)	Выбрать 3 синих карандаша из 5 можно Cf способами; 3 крас¬
ных из имеющихся 4 можно выбрать Cf способами; 3 зеленых из 3
зеленых — Cf способами.
Пи правилу сложения общее число т случаев, благоприятствую¬
щих событию А = {три карандаша, вынутых из коробки, одного цвета},
равно т = Cf + С$ + Cf = 15. Отсюда Р(А) = ~	^
б)	Пусть событие В = {три вынутых карандаша разных цветов}.
Число т исходов, благоприятствующих наступлению события В, по
правилу умножения равно т = С\ * С\ •	— 5 • 4 • 3 = 60. Поэтому
Р( д\ —	HL	—	60 _ А
у }	п	220	11*
в)	Пусть событие С = {из трех выбранных карандашей 2 синих и
1	зеленый}. Выбрать 2 синих карандаша из имеющихся 5 синих можно
С\ способами, а 1 зеленый из имеющихся 3 зеленых — С\ способами.
Отсюда по правилу умножения имеем: т — Cf • С\ — 30. Поэтому
р(сл\ —	TUl—	ЗС) 	_3_	а
{)	п	220	22*
Пример 1.21. Дано шесть карточек с буквами Н, М, И, Я, JI, О. Найти
вероятность того, что: а) получится слово ЛОМ, если наугад одна за
другой выбираются три карточки; б) получится слово МОЛНИЯ, если
наугад одна за другой выбираются шесть карточек и располагаются в
ряд в порядке появления.
О	а) Из шести данных букв можно составить п —	= 120 трехбук¬
венных «слов» (НИЛ, ОЛЯ, ОНИ, ЛЯМ, МИЛ и т.д.). Слово ЛОМ
при этом появится лишь один раз, т. е. m = 1. Поэтому вероятность
появления слова ЛОМ (событие А) равна Р{А) = Щ- =
б)	Шестибуквенные «слова» отличаются друг от друга лишь по¬
рядком расположения букв (НОЛМИЯ, ЯНОЛИМ, ОЛНИЯМ и т.д.).
Их число равно числу перестановок из 6 букв, т. е. п =	= 6!. Очевид¬
но, что т = 1. Тогда вероятность появления слова МОЛНИЯ (событие
В) равна F(B) = ^ = i =	.

30 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 1.22. В почтовом отделении имеются открытки 6 видов. Ка¬
кова вероятность того, что среди 1 проданных открыток все открытки:
а) одинаковы, б) различны?
О Выбрать 4 открытки 6 видов можно = 126 способами, т. е.
п = 126.
а)	Пусть событие А = {продано 4 одинаковые открытки}. Число
т исходов, благоприятствующих наступлению события А, равно числу
видов открыток, т. е. т = 6. Поэтому
= 1§6 = 2Т'
б)	Пусть событие В = {проданы 4 различные открытки}. Выбрать
4 открытки из 6 можно С$ = 15 способами, т. е. гп = 15. Следовательно,
Р(в) =	= —	•
1	}	126	42'
Упражнения
1.	В лифт 9-этажного дома вошли 4 человека. Каждый из них неза¬
висимо друг от друга может выйти на любом (начиная со второго)
этаже. Какова вероятность того, что все вышли: а) на разных эта¬
жах; б) на одном этаже: в) на 5 этаже?
2-	Из колоды карт (их 36) вытаскивают наудачу 5 карт. Какова веро¬
ятность того, что будут вытащены 2 туза и 3 шестерки?
3.	Семь человек рассаживаются наудачу на скамейке. Какова веро¬
ятность того, что два определенных человека будут сидеть рядом?
4.	На 5 карточках разрезной азбуки изображены буквы Е, Е, Л, П, П.
Ребенок случайным образом выкладывает их в ряд. Какова веро¬
ятность того, что у него получится слово ПЕПЕЛ?
5.	Из 60 вопросов, входящих в экзаменационные билеты, студент зна¬
ет 50. Найти вероятность того, что среди 3-х наугад выбранных
вопросов студент знает: а) все вопросы; б) два вопроса.
6-	В барабане револьвера 7 гнезд, из них в 5 заложены патроны. Бара¬
бан приводится во вращение, потом нажимается спусковой курок.
Какова вероятность того, что, повторив такой опыт 2 раза подряд:
а)	оба раза не выстрелит; б) оба раза револьвер выстрелит?

Глава 1. Случайные события "31
7-	Для проведения соревнования 10 команд, среди которых 3 лиде¬
ра, путем жеребьевки распределяются на 2 группы по 5 команд
в каждой. Какова вероятность того, что 2 лидера попадут в одну
группу, 1 лидер — в другую?
8.	Из колоды карт (их 36) наугад вынимают 2 карты. Найти вероят¬
ность, что среди них окажется хотя бы одна «дама».
1.10.	Геометрическое определение вероятности
Геометрическое определение вероятности применяется в случае,
когда исходы опыта равновозможны, а ПЭС (или ft) есть бесконечное
несчетное множество. Рассмотрим на плоскости некоторую область ft,
имеющую площадь Sn, и внутри области ft область D с площадью Sd
(см. рис. 8).
В области ft случайно выбирается точка X. Этот выбор можно ин¬
терпретировать как бросание точки X в область fi. При этом попа¬
дание точки в область ft — достоверное событие, ъ D — случайное.
Предполагается, что все точки области ft равноправны (все элементар¬
ные события равновозможны), т. е. что брошенная точка может попасть
в любую точку области ft и вероятность попасть в область D пропор¬
циональна площади этой области и не зависит от ее расположения и
формы. Пусть событие А = {X £ D}, т. е. брошенная точка попадет в
область D.
Геометрической вероятностью события А называется отношение
площади области D к площади области ft, т. е.
(1.15)

32 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Геометрическое определение вероятности события применимо и в
случае, когда области ft и D обе линейные или объемные. В первом
случае
Р(Л) = ^,	(1.16)
во втором —
р{Л) = Щ,	(1.17)
где I — длина, а	V	—	объем соответствующей	области.
Все три формулы	((1.15)), ((1.16)), ((1.17))	можно	записать в виде
Р(А) = g,	(U8)
где через mes обозначена мера (S', /, V) области.
Геометрическая вероятность обладает всеми свойствами, присущи¬
ми классическому (и другим) определению:
1.	Геометрическая вероятность любого события заключена между ну¬
лем и единицей, т. е.
0 <$ Р(А) ^ 1.
2.	Геометрическая вероятность невозможного события равна нулю,
т. е.
Р(0) = 0.
3.	Геометрическая вероятность достоверного события равна единице,
т. е.
Р{П) = 1.
4.	Геометрическая вероятность суммы несовместимых событий равна
сумме вероятностей этих событий, т. е. если А * В = 0, то
Р{А + В) = Р(А) + Р(В).
Проверим, например, свойство 4: пусть А = {х Е jDi}, В =
= {х Е где D\ • D<i = 0, т. е. D\ и D<i непересекающиеся области.
Тогда Р(А + В) =	= ^L + ^1 = Р(А) + Р(В).
Ьп	Ьп	bn
Пример 1.23. (Задача о встрече.) Два человека договорились о встре¬
че между 9 и 10 часами утра. Пришедший первым ждет второго в те¬
чение 15 мин, после чего уходит (если не встретились). Найти веро¬
ятность того, что встреча состоится, если каждый наудачу выбирает
момент своего прихода.

Г лава 1. Случайные события ■ 33
О	Пусть х — время прихода первого, а у — второго. Возможные зна¬
чения х и у: 0 ^ х ^ 60, 0 ^ у ^ 60 (в качестве единиц масштаба
возьмем минуты), которые на плоскости Оху определяют квадрат со
стороной, равной 60. Точки этого квадрата изображают время встреча¬
ющихся (см. рис. 9).
Рис. 9
Тогда Л = {(х,у) : 0 ^ х ^ 60,0 ^ у ^ 60}; все исходы Q, равновоз¬
можны, так как лица приходят наудачу. Событие А — лица встретят¬
ся — произойдет, если разность между моментами их прихода будет не
более 15 мин (по модулю), т. е. А = {(#, у) : \у — х\ ^ 15}. Неравенство
\у — х\ ^ 15, т. е. х — 15 ^ у ^ х + 15 определяет область, заштри¬
хованную на рис. 9, т. е. точки полосы есть исходы, благоприятству¬
ющие встрече. Искомая вероятность определяется по формуле (1.15):
602 - 2 • \ • 45 • 45	7
ПА) =	W	= в«°.44-	•
Упражнения
1	- В круг радиуса R вписан правильный треугольник. Найти вероят¬
ность того, что точка, брошенная в этот круг, попадет в данный
треугольник.
2.	На отрезке [0,5] случайно выбирается точка. Найти вероятность то¬
го, что расстояние от нее до правого конца отрезка не превосходит
1,6 единиц.
3.	Стержень длины / разломан в двух наугад выбранных точках. Най¬
ти вероятность того, что из полученных отрезков можно составить
треугольник.

34 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
1.11.	Аксиоматическое определение вероятности
Аксиоматическое построение теории вероятностей создано в начале
30-х годов академиком А. Н. Колмогоровым. Аксиомы теории вероят¬
ностей вводятся таким образом, чтобы вероятность события обладала
основными свойствами статистической вероятности, характеризующей
ее практический смысл. В этом случае теория хорошо согласуется с
практикой.
Пусть Л — множество всех возможных исходов некоторого опы¬
та (эксперимента), S — алгебра событий. Напомним (см. п. 1.4), что
совокупность S подмножеств множества ft называется алгеброй (сг-
алгеброй), если выполнены следующие условия:
1.	S содержит невозможное и достоверное события.
2.	Если события А\, А2, A3, ... (конечное или счетное множество)
принадлежат S', то S принадлежит сумма, произведение и дополнение
(т. е. противоположное для Ai) этих событий.
ф	Вероятностью	называется	функция	Р(А), определенная на алгеб-
—	ре событий S', принимающая действительные значения и удовлетворя¬
ющая следующим аксиомам:
А1. Аксиома неотрицательности: вероятность любого события А Е S
неотрицательна, т. е.
Р(А) > 0.
А2. Аксиома нормированности: вероятность достоверного события
равна единице, т. е.
P(ft) = 1.
АЗ. Аксиома аддитивности: вероятность суммы несовместных собы¬
тий равна сумме вероятностей этих событий, т. е. если Ai • Aj = 0
(г ф j), то
р( = ЕР<А*)-
к ' к
ф	Совокупность объектов (ft, S, Р), где ft — пространство элементар-
—	ных событий, S — алгебра событий, Р — числовая функция, удовлетво¬
ряющая аксиомам А1 АЗ, называется вероятностным пространством
случайного эксперимента.
Вероятностное пространство служит математической моделью лю¬
бого случайного явления; заданием этого пространства завершается ак¬
сиоматика теории вероятностей.

Глава 1. Случайные события ■ 35
1.12.	Свойства вероятностей
Приведем ряд свойств вероятности, являющихся следствием акси¬
ом Колмогорова.
С1. Вероятность невозможного события равна нулю, т. е.
Р{0) = 0.
С2. Сумма вероятностей противоположных событий равна единице,
т. е.
Р{А) + Р(А) = 1.
СЗ. Вероятность любого события не превосходит единицы, т. е.
Р{А) ^ 1.
С4. Если А С £?, т. е. событие А влечет за собой событие В, то
Р(А) ^ Р(В).
С5. Если события А\, ^2,..., Ап образуют полную группу несовмест-
п
ных событий, т. е. А{ = И и Ai * Aj = 0, то
i= 1
£р(А) = 1.
г=1
□	Cl. Так как У1 + 0 = ^иЛ-0 = 0, то согласно аксиоме АЗ имеем
Р(А) + Р(0) = Р(А), следовательно, Р(0) = 0.
С2. Поскольку А + А = Л, то Р{А + А) — Р{&), а так как А • А = 0,
то в силу аксиом А2 и АЗ получаем Р(А) + Р{А) = 1.
СЗ. Из свойства С2 вытекает, что Р(А) = 1 — Р(А). С учетом ак¬
сиомы А1 получаем Р(А) ^ 1.
С4. Так как В — {В — А) А при А С В и (5 — А) • А = 0, то
согласно аксиоме АЗ получаем Р(В) = P(j3—Л)+Р(А). Но Р(£?—Л) ^ 0
(аксиома А1), поэтому Р(В) > Р(Л).
С5. Так как Л] + ^2 +... + Ап = Q, то, согласно аксиомам А2 и АЗ,
имеем /^(Лх -Ь Ач + ... 4- Ап) = Р(А\) -f- Р(Ач) + ... + Р{Ап) = 1. I
Заметим, что из Р{А) — 0 не следует А = 0.

36 а Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 1.24. Из колоды, содержащей 36 карт, наудачу вынимают три
карты. Найти вероятность того, что среди них окажется хотя бы одна
«дама».
О Пусть А — интересующее нас событие, А\ — появление одной «да¬
мы», А2 — двух «дам», A3 — трех «дам». Тогда А = А\ 4- А^ 4- Аз,
причем события Ai, А2, A3 несовместные. Поэтому Р(А) — Р{А\) +
4 Р{Ач) 4 Р(Аз). Число всевозможных случаев выбора трех карт из 36
равно число случаев, благоприятных событиям Ai, А2, A3, соот¬
ветственно равно mi = 64 * С32, m2 = С\ • С32, m3 = С4 * Таким
1	/п>2	I	| ^3	/^0
образом, Р(А) =			0,31.
^36
Задача решается проще, если воспользоваться свойством С2. Нахо¬
дим Р(А), где А — среди вынутых карт нет ни одной «дамы»! Р(А) =
С3
= —|р- « 0,69. Значит, Р(А) = 1 — 0,69 = 0,31.	•
^36
1.13.	Конечное вероятностное пространство
Пусть производится некоторый опыт (эксперимент), который имеет
конечное число возможных исходов w\. и>2<>	..., wn. В этом случае
Q, — {г^1, г^2,..., г/;п} (или коротко fi = {го}) — конечное пространство,
S' — алгебра событий, состоящая из всех (их 2П) подмножеств множе¬
ства Л.
Каждому элементарному событию Wi Е fi, i = 1,2,..., п поставим в
соответствие числоp(w{), которое назовем «вероятностью элементарно¬
го события», т. е. зададим на ft числовую функцию, удовлетворяющую
двум условиям:
1)	условие неотрицательности: p{wi) ^ 0 для любого Е fi;
п
2)	условие нормированное™ ^ P(wi) — 1-
i=1
Вероятность Р(А) для любого подмножества A Е Q определим как
сумму
Р(А) = J2 рЫ,	(1-!9)
Wi£A
т. е. вероятностью Р(А) события А назовем сумму вероятностей элемен¬
тарных событий, составляющих событие А. Введенная таким образом

Глава 1. Случайные события ■ 37
вероятность удовлетворяет аксиомам Колмогорова (А1-АЗ):
п
Р(А) > О, Р(П) = рЫ =
u)i£Q	i— 1
р(а+в)= ^2 р(т) = ^2 р(щ)+ ^2	=	р(л)+р(в)'
Wi^A	Wt(zB
если АВ = 0, т. е. А и В — два несовместных события. Так опре¬
деленная тройка {ft, S', Р} есть конечное вероятностное пространство,
называемое «дискретным вероятностным пространством».
Частным случаем определения вероятности (1.19) является клас¬
сическое определение вероятности, когда все исходы опыта равновоз¬
можны: p(wi) = р(и)2) = ... = p{wn) = ^ (следует из условия нормиро-
п
ванности:	р{щ)	—	1)* Формула (1.19) приобретает вид:
г=1
Р{А) = P(wi) = fi + b + '-- + k = 1W’
wzeA	4	v	'
m
т. e. P{A) = где m — число элементарных событий, образующих
событие А (т. е. т — число случаев, благоприятствующих появлению
события А).
1.14.	Условные вероятности
Пусть А и В — два события, рассматриваемые в данном опыте. На¬
ступление одного события (скажем, А) может влиять на возможность
наступления другого (В). Для характеристики зависимости одних со¬
бытий от других вводится понятие условной вероятности.
К|	Условной	вероятностью	события	В	при	условии,	что	произошло
событие А называется отношение вероятности произведения этих со¬
бытий к вероятности события Л, причем Р(А) ф 0, обозначается сим¬
волом Р(В\А).
Таким образом, по определению
Р(В\А) =	,	P(A)ft	0.	(1.20)
Вероятность Р(В), в отличие от условной, называется безусловной ве¬
роятностью. .

38 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Аналогично определяется условная вероятность события* А при
условии В, т. е. Р(А\В):
р(А\В) = Р{р{в^ . Р(В) ф 0.	(1.21)
Отметим, что условная вероятность, скажем Р(В\А), удовлетворя¬
ет аксиомам Колмогорова (п. 1.11): Р(В\А) ^ 0. очевидно: Р(ЩА) =
= -Р(ЛГ = Щ = 1: Р((В + С)\А) = Р(В\А) + Р(С\А), если В С =
= 0. Поэтому для условной вероятности справедливы все следствия
(свойства) из аксиом, полученные в п. 1.12. Формула (1.20) принимает¬
ся по определению при аксиоматическом определении вероятности; в
случае классического (геометрического, статистического) определения
она может быть доказана.
Пример 1.25. В урне 2 белых и 7 черных шаров. Из нее последователь¬
но вынимают два шара. Какова вероятность того, что 2-й шар окажется
белым при условии, что 1-й шар был черным?
О Решим задачу двумя способами.
1.	Пусть А — 1-й шар черный, В — 2-й шар белый. Так как событие
А произошло, то в урне осталось 8 шаров, из которых 2 белых. Поэтому
ИЩА) = I = f
2.	Найдем Р(В\А) по формуле (1.20). Очевидно, что /'(/1) = ^ .
Находим Р(АВ): ?г = 9 • 8	72 — общее число исходов (появление двух
шаров). Событию АВ благоприятствуют тп = О.] • С) = 14 исходов.
Поэтому Р(АВ) = ^	^ . Следовательно, Р(В\А) = ^ ^	^.	•
1.15.	Вероятность произведения событий.
Независимость событий
Из определения условной вероятности (п. 1.14) следует, что
Р(А • В) = Р(А) • Р(В\А) = Р{В) • Р{А\В),	(1.22)
т. е. вероятность произведения двух событий равна произведению ве¬
роятности одного из них на условную вероятность другого при условии,
что первое событие произошло.

Глава 1. Случайные события ■ 39
Равенство (1.22) называют правилом или теоремой (для схемы слу¬
чаев оно доказывается) умножения вероятностей. Это правило обоб¬
щается на случай п событий:
Р(А1-А2-...-Ап) =
= Р{А\) • Р(А2\А1) ■ Р(Аз|Л! • А2) •... • Р{Ап\А\ • А2 ■ ■ . • ■ К-О-
(1.23)
Так для 3-х событий А\, А2, А% получаем
Р(Аг • А2 . А3) = Р({А\ . Л2) • М) = Р(Аг • А2) • Р(А3Иг • Л2) -
= P{Al)-P{A2\A1).P{Az\Al.A2).
Пример 1.26. В коробке находится 4 белых. 3 синих и 2 черных шара.
Наудачу последовательно вынимают 3 шара. Какова вероятность того,
что 1-й шар будет белым, 2-й — синим, 3-й — черным?
О Введем следующие события: А\ — первым вытащили белый шар,
А2 — вторым — синий, А% — третьим — черный. Тогда интересую¬
щее нас событие А представится в виде А = А\ • Л2 • А%. По прави¬
лу умножения вероятностей Р(А) = ^(^i) • Р{А2\А\) • Р(А$\А\ • А2).
Но Р{А\) =	Р(А2\А\) — §, так как шаров осталось 8, а число
2
благоприятных случаев для события А2 равно 3; P(A^\Ai • А2) = у ,
так как уже два шара (белый и синий) вытащены. Следовательно,
р<'4> = §'1'1 = ггк0-05-	•
Правило умножения вероятностей имеет особо простой вид, если
события, образующие произведение, независимы.
Событие А называется независимым от события В, если его услов¬
ная вероятность равна безусловной, т. е. если выполняется равенство
Р(А\В) = Р(А).	(1.24)
Лемма 1.1 (о взаимной независимости событий). Если событие А
не зависит от события В, то и событие В не зависит от события А.
Q Из равенства (1.22), с учетом равенства (1.24), следует Р(В\А) =
Р(А\В) • Р(В) Р(А) • Р(В) _
Р(А)	Р{А)	У
Р(В\А) = Р(В),
(1.25)

40 в Раздел первый. Элементарная теория вероятностей и случайных процессов
а это означает, что событие В не зависит от события А.	■
Можно дать следующее (новое) определение независимости собы¬
тий.
Два события называются независимыми, если появление одного из
них не меняет вероятность появления другого.
Для независимых событий правило умножения вероятностей (1.22)
принимает вид:
Р(А-В) = Р(А)-Р(В),	(1.26)
т. е. вероятность произведения двух независимых событий равна про¬
изведению вероятностей этих событий.
Равенство (1.26) часто используют в качестве определения (еще од¬
ного!) независимости событий: события А и В называются независимы¬
ми, если Р(А * В) = Р(А) • Р(В).
Можно показать, что если события А и В независимы, то незави¬
симы события А и В, А и В, А и В.
На практике о независимости тех или иных событий часто судят
исходя из интуитивных соображений и анализа условий опыта, считая
независимыми события, «между которыми нет причинно-следственных
связей».
Понятие независимости может быть распространено на случай п
событий.
События А\,А2,..., Ап называются независимыми (или независи¬
мыми в совокупности), если каждое из них не зависит от произведения
любого числа остальных событий и от каждого в отдельности. В про¬
тивном случае события Ах, Аг, ■. -, А71 называются зависимыми.
Для независимых событий их условные вероятности равны без¬
условным, и формула (1.23) упрощается
Р(А! -А2-...-Ап) = Р(А,) - Р(А2) • ... • Р(Ап). (1.27)
Из попарной независимости событий А\, А2, ■.., Ап (любые два из
них независимы) не следует их независимость в совокупности (обратное
верно).
Убедимся в этом, рассмотрев следующий пример.
Пример 1.27. Производится выбор (наудачу) флага из 4-х, имеющих¬
ся в наличии: красного, голубого, белого и трехцветного (красно-бело¬
голубого). Исследовать на независимость события: К — выбранный
флаг имеет красный цвет; Г — имеет голубой цвет; Б — имеет белый
цвет.

Глава 1. Случайные события ■ 41
О Возможных исходов выбора 4; событию К благоприятствуют 2 исхо-
2	1
да (красный цвет имеется у двух флагов). Поэтому Р(К) = - = - . Ана-
1
логично находим, что -Р(Г) = Р(Б) = - . Событию К-Г — выбран флаг,
имеющий 2 цвета (красный и голубой), — благоприятствует один исход.
Поэтому, Р(К • Г) = ± . И так как Р(К ■ Г) = |	|	• А = Р(К) • Р(Г), то
события К и Г независимы. Аналогично убеждаемся в независимости
событий К и Б, Б и Г. Стало быть, события К, Б, Г попарно независи-
мы. А так как Р(К • Г• Б) = \ ф Р(К) • Р(Г) • Р(Б) = J,to события К,
4	о
Г и Б не являются независимыми в совокупности.	•
Упражнения
1.	Бросается игральная кость. Пусть событие А — появление четного
числа очков, событие В — появление более трех очков. Зависимы
или нет события А и В1
2-	Из букв разрезной азбуки составлено слово СТАТИСТИКА. Како¬
ва вероятность того, что, перемешав буквы и укладывая их в ряд по
одной (наудачу), получим слово : а) ТИСКИ; б) КИСКА; в) КИТ:
г) СТАТИСТИКА?
3-	Найти вероятность отказа схемы (рис. 10), предполагая, что отказы
отдельных элементов независимы, а вероятность отказа элемента
с номером г равна 0,2.
Рис. 10

42 а Раздел первый. Элементарная теория вероятностей и случайных процессов
1.16.	Вероятность суммы событий
Как известно (п. 1.11), вероятность суммы двух несовместных со¬
бытий определяется аксиомой АЗ: Р(А + В) — Р(А) И- Р(В). А - В = 0.
Выведем формулу вероятности суммы двух совместных событий.
Теорема 1.1. Вероятность суммы двух совместных событий равна сумме
их вероятностей без вероятности их произведения,
Р(А + В) = Р(А) + Р(В) - Р(А * В).	(1.28)
Q Представим события А + В и В в виде суммы двух несовместных
событий: А+В = А+Р-А, В = АВ+ВА (см. п. 1.3, пример 1.2 и упраж¬
нение 1). В справедливости этих формул можно наглядно убедиться на
рис. 11.
Рис, 11
Тогда, согласно аксиоме АЗ, имеем Р(А + В) = Р(А) + Р(В • А) и
Р(В) = Р(А • В) + Р(В • А). Отсюда следует Р(А + В) — Р(А) + Р(В) —
- Р(А - В).	■
Формула (1.28) справедлива для любых событий А и В.
Можно получить формулу вероятности суммы трех и большего чи¬
сла совместных событий; для трех событий она имеет вид
Р(А + В + С) = Р(А) + Р(В) + Р{С)-
- Р(А • В) - Р(А • С) - Р(В * С) + Р{А -В-С). (1.29)
Справедливость равенства поясняет рис. 12.
Проще, однако, найти вероятность суммы нескольких совместных
событий P(S) = Р(А\ +А2+...+Ап), используя равенство Р(5)+Р(5) =
= 1, где S = А\ • А2 • ... • Ап — противоположно событию S. Тогда
P(S) = 1 — Р(5). Мы уже использовали этот прием в п. 1.12.

Глава 1. Случайные события ■ 43
А В
-ABC
П
Рис. 12
Пример 1.28. Бросаются две игральные кости. Какова вероятность
появления хотя бы одной шестерки?
о Введем события: А — появление шестерки на первой кости, В —
на второй кости. Тогда А + В — появление хотя бы одной шестерки при
бросании костей. События А и В совместные. По формуле (1.28) нахо-
димР(А+В) = i +	=	ii.	(Иначе: P(S) = P(A-B) = §•§ = Щ.
ОК 1 1
1.	В урне 2 белых и 7 черных шаров. Из нее наудачу вынимают (без
возврата) 2 шара. Какова вероятность того, что они оба будут раз¬
ных цветов?
2.	Три орудия стреляют в цель независимо друг от друга. Вероятность
попадания в цель каждого равна 0,7. Найти вероятность попадания
в цель: а) только одного из орудий; б) хотя бы одного.
3.	Надежность (т. е. вероятность безотказной работы) прибора равна
0,7. Для повышения надежности данного прибора он дублируется
п — 1 другими такими же приборами (рис. 13). Сколько приборов
надо взять, чтобы повысить его надежность до 0,95?
Упражнения

44 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Рис. 13
1.17.	Формула полной вероятности
Одним из следствий совместного применения теорем сложения и
умножения вероятностей являются формулы полной вероятности и
Байеса. Напомним, что события А\, Л2, ..., Ап образуют полную груп-
п
пу, если Ai-Aj = 0, г / j и Ai — ft. Систему таких событий называют
г=]
также разбиением.
Теорема 1.2. Пусть события Н\, #2, - • ■, Нп образуют полную группу.
Тогда для любого, наблюдаемого в опыте, события А имеет место фор¬
мула полной вероятности или средней вероятности.
Р(А) = ^Р(Щ)-Р(А\Нг),
(1.30)
i—V
□	Так как Н\ + Н2 + •. • 4- Нп = ft, то в силу свойств операций
над событиями (п. 1.3), А = А • ft = А • (Н\ + Н2 + • • • + Нп) =
= А • Н\ + А * i?2 + -.. + А • i/n. Из того, что Hi • Hj — 0, следу¬
ет, что (Л • Яг) * (А • Я^) = 0, г ф j, т. е. события А • Щ и A Hj
также несовместны. Тогда по теореме сложения вероятностей Р(А) =
= Р(А - Нг) + Р(А- Н2) + ... + Р(А- Нп) т.е. Р(А) = £Р{А-Щ). По
г=1
теореме умножения вероятностей Р(А • Hi) = Р{Щ) • Р(А\Щ), откуда и
следует формула (1.30).	■
Отметим, что в формуле (1.30) события Н\, Я2, * • •, Нп обычно на¬
зывают гипотезами; они исчерпывают все возможные предположения
(гипотезы) относительно исходов как бы первого этапа опыта, событие
А — один из возможных исходов второго этапа.

Глава 1. Случайные события ■ 45
Пример 1.29. В сборочный цех завода поступает 40% деталей из I цеха
и 60% — из II цеха. В I цехе производится 90% стандартных деталей,
а во II — 95%. Найти вероятность того, что наудачу взятая сборщиком
деталь окажется стандартной.
О Взятие детали можно разбить на два этапа. Первый — это выбор
цеха. Имеется две гипотезы: Нj — деталь изготовлена I цехом, Н2 —
II	цехом. Второй этап — взятие детали. Событие А - взятая наудачу
деталь стандартна. Очевидно, события Н\ и Н2 образуют полную груп¬
пу, P(Hi) = 0,4, Р(Н2) = 0,6. Числа 0,90 и 0.95 являются условными
вероятностями события А при условии гипотез Н\ и Н2 соответствен¬
но, т.е. Р(А\Н\) = 0,90 и Р(А\Н2) = 0.95. По формуле (1.30) находим
2
р(А) = р{Нг) ■ P(A\Hi) = 0,4 • 0,90 + 0,6 ■ 0,95 = 0,93.	•
i= 1
1.18.	Формула Байеса (теорема гипотез)
Следствием формулы (1.30) является формула Байеса или теоре¬
ма гипотез. Она позволяет переоценить вероятности гипотез Hj, приня¬
тых до опыта и называемых априорными («а priori», доопытные, лат.)
по результатам уже проведенного опыта, т. е. найти условные веро¬
ятности Р(Щ\А)У которые называют апостериорными («а posteriori»,
послеопытные).
Теорема 1.3. Пусть события Н2,.. -, Нп образуют полную группу
событий. Тогда условная вероятность события Нд. (к = 1, п) при условии,
что событие А произошло, задается формулой
т = Щ1тА,	„31)
где Р{А) = P{Hi) • Р(А\Н\) + ... + Р(Нп) • Р(А\Нп) — формула полной
вероятности. Формула (1.31) называется формулой Байеса1.
21702 1761, английский священник, математик

46 ■ Раздел первый- Элементарная теория вероятностей и случайных процессов
Q| Применив формулы условной вероятности (п. 1.14) и умножения
вероятностей (п. 1.15), имеем
Р (Hk-A)	P(Ht) ■ Р(Л\Н„)
Л = ■ ТСаГ =	ПА)	’
где Р(А) — формула полной вероятности (п. 1.17)	■
Пример 1.30. В примере 1.29 (п. 1.17) найти вероятность того, что эта
стандартная деталь изготовлена II цехом.
О	Определим вероятность гипотезы H<i при условии, что событие А
(взятая деталь стандартна) уже произошло, т. е. Р(Н2\Л):
Р(Я\\\ р{нг) ■ Р(А\нг) 0,6-0,95	19
РША) =	= п « 0.6U.	•
Упражнения
1	- Прибор содержит две микросхемы. Вероятность выхода из строя в
течение 10 лет первой микросхемы равна 0,07, а второй — 0,10. Из¬
вестно, что из строя вышла одна микросхема. Какова вероятность
того, что вышла из строя первая микросхема?
2.	Из 40 экзаменационных билетов студент П выучил только 30. Ка¬
ким выгоднее ему зайти на экзамен, первым или вторым?
3.	Известно, что 90% изделий, выпускаемых данным предприятием,
отвечает стандарту. Упрощенная схема проверки качества продук¬
ции признает пригодной стандартную деталь с вероятностью 0,96 и
нестандартную с вероятностью 0.06. Определить вероятность того,
что:
а)	взятое наудачу изделие пройдет конт{юль;
б)	изделие, прошедшее контроль качества, отвечает стандарту.

Глава 1. Случайные события ■ 47
1.19.	Независимые испытания. Схема Бернулли
С понятием «независимых событий» связано понятие «независи¬
мых испытаний (опытов)».
Несколько опытов называются независимыми, если их исходы
представляют собой независимые события (независимые в совокупно¬
сти).
Другими словами, если проводится несколько испытаний, т. е. опыт
выполняется при данном комплексе условий многократно (такое явле¬
ние называется «последовательностью испытаний»), причем вероят¬
ность наступления некоторого события А в каждом испытании не за¬
висит от исходов других испытаний, то такие испытания называются
независимыми.
Примерами независимых испытаний могут служить: несколько (п
раз) подбрасываний монеты; стрельба (гг раз) по мишени без поправок
на ранее допущенную ошибку при новом выстреле; несколько (п раз)
выниманий из урны одинаковых на ощупь занумерованных шаров, если
шары каждый раз (после просмотра) возвращаются назад в урну, и т. д.
При практическом применении теории вероятностей часто исполь¬
зуется стандартная схема, называемая схемой Бернулли или схемой
независимых испытаний.
Последовательность п независимых испытаний, в каждом из кото¬
рых может произойти некоторое событие А (его называют успехом) с
вероятностью Р(А) — р или противоположное ему событие А (его на¬
зывают неудачей) с вероятностью Р{А) = q = 1 — р, называется схемой
Бернулли.
Например, при стрельбе по мишени: событие А — попадание
(успех), событие А — промах (неудача); при обследовании п изделий
на предмет годности: событие А — деталь годная (успех), событие А —
деталь бракованная (неудача) и т. д.
В каждом таком опыте ПЭС состоит только из двух элементар¬
ных событий, т. е. Q = {wo,wi}, где г^о — неудача, w\ — успех, при
этом А = {гиг}, А = {г^о}- Вероятности этих событий обозначают че¬
рез р и q соответственно (р + q — 1). Множество элементарных исхо¬
дов для п опытов состоит из 2п элементов. Например, при п = 3, т. е.
0 t(A,A,A) (А, А, А) (А, А, А) (А, А, А)
опыт повторяется 3 раза, П = < —щ	; ——	; —щ	; ——	;
(А, А, А) (А, А, А) (А, А, А) (А, А, А)
W4 ,	г„5	, щ.	W7	Вероятность	каждого	элемен-
тарного события определяется однозначно. По теореме умножения ве-

48 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
роятностъ события, скажем we — (Л, А,А), равна q-q-p = pg2, события
W7 — р • р • р = р3д° = р3 и т. д.
Часто успеху сопоставляют число 1, неудаче — число 0. Элемен¬
тарным событием для го опытов будет последовательность из го нулей
и единиц. Тройка чисел (0,0,0) означает, что во всех трех опытах со¬
бытие А не наступило; тройка чисел (0,1,0) означает, что событие А
наступило во 2-м опыте, а в 1-м и 3-м — не наступило.
1.20.	Формула Бернулли
Простейшая задача, относящаяся к схеме Бернулли, состоит в
определении вероятности того, что в го независимых испытаниях собы¬
тие А наступит га раз (0 ^ га ^ го). Обозначается искомая вероятность
так: Рп(т) или Рп,т или P{f^n — ^)> гДе f^n — число появления события
А в серии из го опытов.
Например, при бросании игральной кости 3 раза -Рз(2) означает
вероятность того, что в 3-х опытах событие А — выпадение цифры
4 — произойдет 2 раза. Очевидно,
рз(2) = Р2Я + Р2Я + Р2Я =
= [{(А А. Л); (А, Л, А); (4, А, Л)}] = 3р2<? = 3- (±)" • § = ^ = 0,069.
Теорема 1.4. Если производится го независимых испытаний, в каждом
из которых вероятность появления события А равна р, а вероятность
его непоявления равна q — 1 — р, то вероятность того, что событие А
произойдет га раз определяется формулой Бернулли
Рп{т) = С™ • рт • qn~m, m = 0,1,2,..., го.	(1.32)
Q Вероятность одного сложного события, состоящего в том, что со¬
бытие А в го независимых опытах появится га раз в первых т опы¬
тах и не появится (го — га) раз в остальных опытах (это событие
А - А • А •... ■ Д • А ■ А • ... • А.) по теореме' умножения вероятностей рав-
Раз	(п—гп)	раз
нарт<7п_т. Вероятность появления события А снова га раз, но в другом

Глава 1. Случайные события ■ 49
порядке (например, А-А - А... • АА-А-...-А или АААА •... • АА и т. д.)
ч,-„
га раз
будет той же самой, т. е. pmqn~m.
Число таких сложных событий — в гг опытах m раз встречается со¬
бытие А в различном порядке — равно числу сочетаний из п по га, т. е.
С™. Так как все эти сложные события несовместны, то по теореме сло¬
жения вероятностей искомая вероятность равна сумме вероятностей
всех возможных сложных событий, т. е.
pn(m) = ртдп-™ + ... + ртдп~™ = C™prnqn-m, га = 0,1	гг. ■
С™ с л агаем ы х
Можно заметить, что вероятности Pn(m), m = 0.1	п являются
коэффициентами при хш в разложении (q + рх)п по формуле бинома
Ньютона:
{q+рх)п =qn + С^~1рх + Clqn~2p2x2 +... + C"lqn-mpmx,n +... +рпхп.
Поэтому совокупность вероятностей Рп(т) называют биномиальным
за,коном распределения вероятностей (см. п. 2.7), а функцию ^>{х) =
—	(Я + Рх)п — производящей функцией для последовательности неза¬
висимых опытов.
Если в каждом из независимых испытаний вероятности наступле¬
ния события А разные, то вероятность того, что событие А наступит
га раз в п опытах, равна коэффициенту при гп-й степени многочлена
<Pn(z) = (</i + Piz){Q‘2 + P‘iz) - .. - * [qn + Pn*)i где <Pn(z) — производящая
функция.
Если в серии из п независимых опытов, в каждом из которых может
произойти одно и только одно из к событий А\, А2,	Аь с соответ¬
ствующими вероятностями р\% Р2* ..., Рк, то вероятность того, что в
этих опытах событие А\ появится чщ раз, событие А2 — m2 раз,
событие А^ — rrtk раз, равна
Pn(mi,m2,• -• • тк) = mi!m27!...mfc!(1-33)
где т\ И- m2 + ... + т^ = п. Вероятности (1.33) называются полиноми¬
альным распределением.
Пример 1.31. Производится 3 независимых выстрела по цели. Веро¬
ятности попадания при разных выстрелах одинаковы и равны р = 0,9.
Какова вероятность: а) промаха; б) одного попадания; в) двух попа¬
даний; г) трех попаданий? Решить задачу в случае, если вероятности
попадания при разных выстрелах различны: р\ = 0,7, Р2 = 0,8, рз = 0,9.

50 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
о	В данном случае п = 3, р = 0,9, q = 0,1. Пользуясь формулой
Бернулли (1.32), находим:
а)	Рз(0) — С§ • 0,9° - ОД3 = 0,001 — вероятность трех промахов;
б)	Р3(1) = С\ * 0,9х • ОД2 = 3 • 0,9 - 0,01 = 0,027 — вероятность одного
попадания;
в)	Рз(2) = Сд • 0,92 • ОД1 = 3 • 0,81 • ОД = 0,243 — вероятность двух
попаданий;
г)	Рз(3) = Cf • 0,93 -0Д° = 0,93 = 0,729 — вероятность трех попада¬
ний.	•
Эти результаты можно изобразить графически, отложив на оси Ох
значения ш, на оси Оу — значения Рп(га) (рис. 14).
Ломаная, соединяющая точки (0; 0,001),	(1;	0,027),	(2;	0,243),
(3; 0,729), называется многоугольником распределения вероятностей.
Если вероятности при разных выстрелах различны, то производя¬
щая функция имеет вид <рз{г) = (0,3 + 0,7z)(0,2 + 0,8г)(0Д + 0,9z) =
= 0,504г3 4- 0,398z2 + 0,092;г + 0,006. Откуда находим вероятность трех,
двух, одного попаданий, промаха соответственно: Рз(3) = 0,504, Рз(2) =
= 0,398, Р3(1) = 0,092, Р3(0) = 0,006. (Контроль: 0,504 + 0,398 + 0,092 +
+ 0,006 = 1.)

Глава 1. Случайные события ■ 51
Упражнения
1	- Монету подбрасывают 10 раз. Какова вероятность того, что герб
выпадет (появится): а) 4 раза; б) ни разу; в) хотя бы один раз.
2-	Что вероятнее выиграть у равносильного противника-шахма.тиста:
две партии из четырех или три из шести? Ничьи во внимание не
принимаются.
3.	В семье трое детей. Какова вероятность того, что: а) все они маль¬
чики; б) один мальчик и две девочки. Считать вероятность рожде¬
ния мальчика 0,51, а девочки — 0,49.
4.	В каждом из карманов (их 2) лежит по коробку спичек (по 10 спи¬
чек в коробке). При каждом закуривании карман выбирается нау¬
дачу. При очередном закуривании коробок оказался пустым. Найти
вероятность того, что во втором коробке 6 спичек.
1.21.	Предельные теоремы в схеме Бернулли
Использование формулы Бернулли (1.32) при больших значениях п
и 7Т) вызывает большие трудности, так как это связано с громоздкими
вычислениями. Так, при п = 200, m = 116, р = 0,72 формула Бер¬
нулли принимает вид Р200(И6) = C^qq * (0,72)116 • (0,28)84. Подсчитать
результат практически невозможно. Вычисление Рп(тп) вызывает за¬
труднения также при малых значениях р (q). Возникает необходимость
в отыскании приближенных формул для вычисления Pn(m), обеспечи¬
вающих необходимую точность. Такие формулы дают нам предельные
теоремы; они содержат так называемые асимптотические формулы, ко¬
торые при больших значениях испытаний дают сколь угодно малую
относительную погрешность. Рассмотрим три предельные теоремы, со¬
держащие асимптотические формулы для вычисления биномиальной
вероятности Рп(т) при п —> оо.

52 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Теорема Пуассона
Теорема 1.5. Если число испытаний неограничено увеличивается
(п —> оо) и ве{юятнисть р наступления события А в каждом испыта¬
нии неограничено уменьшается (р -> 0), но так, чао их произведение пр
является постоянной величиной (пр = а = const), то вероятность Рд(га)
удовлетворяет предельному равенству
liin Рп(гп) = а--, .	(1.34)
п->оо nv ’ m!	v
Выражение (134) называется асимптотической формулой Пуас¬
сона.
Q Преобразуем формулу Бернулли (1.32) с учетом того, что р = ^ :
n(n-l)(n-2)...(n-(m-l)) а'п ,	,	а\-ш
га!	пт '	п' '	п'
—	я™. п 71 ~ 1 п ~~ 2	n	—	(m	— 1)	/	_	fty/i.	/	_	^\_т _
~ ml п п * п ***** п	\	п)	V	п/	~
= S'1' (‘ “ н) • (! -1) ••• О -	•	о	-	я)" • U - 1)“т-
ате~а
Переходя к пределу при п —> ос, получим liin Рп(т) —	\—
TL—^ос	гп\
( lim (l — ~) = е~° согласно второму замечательному пределу). ■
71—У СО V	П/
Из предельного равенства (1.34) при больших п и малых р вытекает
приближенная формула Пуассона
Рп(гп) »	j— , а = пр, т = 0,1,2,....	(1.35)
Формулу (1.35) применяют, когда вероятность р = const успеха крайне
мала, т. е. сам по себе успех (появление события А) является редким
событием (например, выигрыш автомобиля по лотерейному билету),
но количество испытаний п велико, среднее число успехов пр — а, не¬
значительно. Приближенную формулу (1.35) обычно используют, когда
п ^ 50, а пр ^ 10.
Формула Пуассона находит применение в теории массового обслу¬
живать.

Глава 1. Случайные события ■ 53
Пример 1.32. Завод «Золотая балка» (Крым) отправил в Москву 1500
бутылок вина «Каберне». Вероятность того, что в пути бутылка может
разбиться, равна 0,002. Найти вероятность того, что в пути будет раз¬
бито не более 4-х бутылок (событие А).
о Искомая вероятность равна
^1500(0) + Pl500(l) -I- Pl500(2) + Pi500(3) 4 Pl500(4).
Так как п = 1500, р = 0,002, то а — [пр] = 3. Вероятность события А
найдем, используя формулу Пуассона (1.35):
о0 ^-3 о1 „-3	*>2	^-3	оЗ 0-3	о4 „-3
Р{А) = 2-^- +	*	0,815.	•
Формулу Пуассона можно считать математической моделью про¬
стейшего потока событий.
Потоком событий называют последовательность событий, насту¬
пающих в случайные моменты времени (например, поток посетителей в
парикмахерской, поток вызовов на телефонной станции, поток отказов
элементов, поток обслуженных абонентов и т.п.).
Поток событий, обладающий свойствами стационарности, ординар¬
ности и отсутствия последствия называется простейшим (пуассонов-
ским) потоком.
Свойство стационарности означает, что вероятность появления к
событий на участке времени длины т зависит только от его длины
(т. е. не зависит от начала его отсчета). Следовательно, среднее число
событий, появляющихся в единицу времени, так называемая интен¬
сивность А потока, есть величина постоянная: A(t) = А.
Свойство ординарности означает, что событие появляется не груп¬
пами, а поодиночке. Другими словами, вероятность появления более
одного события на малый участок времени At пренебрежительно мала
по сравнению с вероятностью появления только одного события (на¬
пример, поток катеров, подходящих к причалу, ординарен).
Свойство отсутствия последствия означает, что вероятность по¬
явления к событий на любом участке времени длины т не зависит от
того, сколько событий появилось на любом другом не пересекающимся
с ним участком (говорят: «будущее» потока не зависит от «прошлого»,
например, поток людей, входящих в супермаркет).
Можно доказать, что вероятность появления т событий простей¬
шего потока за время продолжительностью t определяется формулой

54 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пуассона
„ , .	(At)m ■ e~xt
Pt(m) = Pm =
777!
Пример 1.33. Телефонная станция обслуживает 2000 абонентов. Веро¬
ятность позвонить любому абоненту в течение часа равна 0,003. Какова
вероятность того, что в течение часа позвонят 5 абонентов?
О	Среднее число позвонивших в течение часа абонентов равно
2000 • 0,003 = 6 (а = пр = At). Стало быть, р$ =	— « 0,13.	•
5!
Локальная и интегральная теоремы Муавра-Лапласа
В тех случаях, когда число испытаний п велико, а вероятность
р не близка к нулю (р ф 0, р ф 1), для вычисления биномиальных
вероятностей используют теоремы Муавра-Лапласа. Приведем только
их формулировки в силу сложности доказательства.
Теорема 1.6 (Локальная теорема Муавра-Лапласа). Если вероят¬
ность р наступления события А в каждом испытании постоянна и отлична
от нуля и единицы, а число независимых испытаний достаточно велико,
то вероятность Рп(т) может быть вычислена по приближенной формуле
Рп{т) « i— • ~=	где х = т .	(1.36)
у/ЩЩ у/2ж	s/m
Равенство (1.36) тем точнее, чем больше п.
Выражение
1
1	,е~Т = <р(х)	(1.37)
л/2тг
называется функцией Гаусса, а ее график — кривой вероятностей (см.
рис. 15).
Равенство (1.36) можно переписать в виде
Рп{т) » i—где х = т	.	(1.38)
yjrvpq	y/npq

Глава 1. Случайные события ■ 55
¥>(*)|о>4
-1 О
12	3**
Рис. 15
Для функции (р(х) составлены таблицы значений (они находятся, как
правило, в так называемых «Приложениях» книг по теории вероятно¬
стей см. приложение 1 на с. 284). Пользуясь таблицей, следует учиты¬
вать, что:
а)	функция <р{х) четная, т. е. <р(—х) = <р(х);
б)	при х ^ 4 можно считать, что ср(х) = 0.
Функция Гаусса (1.37) будет подробнее рассмотрена в п. 2.7.
Пример 1.34. Вероятность попадания в мишень при одном выстреле
для данного стрелка равна 0,7. Найти вероятность того, что при 200
выстрелах мишень будет поражена 160 раз.
О	Здесь п — 200, р — 0,7, q = 0,3, т = 160. Применим форму¬
лу (1.38). Имеем: 1Jnpq = v/200 • 0,7 • 0,3 = \/42 « 6,48, следователь-
В тех случаях, когда требуется вычислить вероятность того, что
в п независимых испытаниях событие А появится не менее к\ раз, но
не более раз, т. е. Рп{к>\ ^ m ^ А^) или ^пЙА^г)? используют инте¬
гральную теорему Муавра-Лапласа (является частным случаем более
общей теоремы — центральной предельной теоремы).
но, х =
« 3,09. Учитывая, что <£>(3,09) « 0,0034,
получаем Р20о(160) «	*	0?0034	«	0,0005.
о,4о

56 " Раздел первый. Элементарная теория вероятностей и случайных процессов
Теорема 1.7 (Интегральная теорема Муавра— Лапласа). Если ве¬
роятность р наступления события А в каждом испытании постоянна и
отлична от нуля и единицы, то вероятность Рп{кi ^ т ^ /^2) может быть
найдена по приближенной формуле
Z2
Рп(к 1 < т ^ /с2) « -~= [
v 27г J
Х2 2
,	fcj-np	к2~пр
е 2 ах, где х\ — —-— - , х^ =
s/ъг J	vm	л/fm
(1.39)
Равенство (1.39) тем точнее, чем больше п.
Используя функцию Гаусса (1.37), равенство (1.39) можно записать
в виде
Z2
Рп(&1,/С2)^ J (p(x)d,x.
XI
Однако для упрощения вычислений, при использовании форму¬
лы (1.39), вводят специальную функцию
г ^
Фо(ж) = -у= /V Т dt,
V27T J
(1.40)
называемую нормированной функцией Лапласа.
1 ~х
Функция (1.40) нечетна (Фо(—х) = — - / в 2 d,t = [t = —2:] =
\/27г о
1	^
=	—- J е 2 dz — — Ф0(х)); при ^ 5 можно считать, что Фо(гс) —
у2тг о
= 0,5; график функции Фо(х) приведен на рис. 16.
Рис. 16

Глава 1. Случайные события ■ 57
Выразим правую часть равенства (1.39) через функцию Лапла¬
са (1.40):
}=[e-TdX = -±=[e-Tdl =
у/2т\ J	л/2к	J
XI	XI
о
/t‘2 р ^2
dt +	/	е“Т dt = Фо(г2) - Фо(®1)-
\Z2nJ
0	Х‘2
XI
Равенство (1.39) принимает вид
Pn(h ^ m ^ к2) = Ф0(ж2) - Ф0(жх),
к\ — пр	ко	—	пр	А	+	\
где Х\ =	1	' , ж2 = V- •	!-И
Эту формулу обычно используют на практике.
Наряду с нормированной функцией Лапласа (1.40) используют
функцию
X	2
Ф(х) = -}= [ e~l2dt,	(1.42)
v27t J
—оо
называемую также функцией Лапласа. Для нее справедливо равенство
Ф(—ж) + Ф(х) = 1; она связана с функцией Фо(^) формулой
ф(х) = 0,5 + Ф0(а;).	(1.43)
Имеются таблицы приближенных значений функций Фо(^) и Ф(ж) (ин¬
теграл не берется в элементарных функциях), которые приводятся в
большинстве учебников по теории вероятностей (см. также приложе¬
ние 2 на с. 285).
Приближенную формулу для вычисления вероятности Рп(к\ ^
^ m ^ &2) (1.39) можно записать в виде
Pn{ki ^ т. О2) = Ф(ж2) - Ф(х-х) = Ф0(х2) - Фо(а-‘1),
к\ — пр	к2-пр	.	.
где xi =		 , х2 =	.	 -	(1.41)
yjfvpq	yjnpq
Пример 1.35. Проверкой установлено, что цех в среднем выпускает
96% продукции высшего сорта. На базе приемщик проверяет 200 изде¬
лий этого цеха. Если среди них окажется более 10 изделий не высшего
сорта, то вся партия изделий бракуется, т. е. возвращается в цех. Ка¬
кова вероятность того, что партия будет принята?

58 в Раздел первый. Элементарная теория вероятностей и случайных процессов
О Здесь п = 200, р ~ 0,04 (вероятность негодного изделия), q =
= 0,96. Вероятность принятия всей партии, т. е. Р2сю(0 ^ т ^ 10),
можно найти по формуле (1.44); здесь к\ = 0. к2 = 10. Находим,
0 - 200 ■ 0,04	0	ОЛ	10	-	200 - 0,04
ЧТО Х\ =
-2,89, х2 =
0,72,
v/200 * 0,04 • 0,96	у/200	•	0,04	•	0,96
^200(0 < т < 10) = Ф0(0,72) - Ф0(-2,89) = 0,26424 + 0,49807 =
= 0,7623. Заметим, что Ф(0,72) - Ф(—2,89) = 0,7642 - (1 - Ф(2,89)) -
= 0,7642 - (1 - 0,998074) = 0,7623.	•
Замечание. С помощью функции Лапласа можно найти вероят¬
ность отклонения относительной частоты от вероятности р в п не¬
зависимых испытаниях. Имеет место формула
р{|^-Н«£}=2Ф"(е\/й)'
где е > 0 — некоторое число.
□ Из
71А
^ е следует: —е ^ —— р ^ е, пр — пе ^ ид ^пр + пе.
По формуле (1.39) получаем:
с-,/25-
РЯ
Рп{пр — пе ^ п 4 ^ пр + пе} ~	1		/
V 27г J
л
е 2dt =
T-e-P{|lf-р|^е} = 2Фо {e'\/W)'
Пример 1.36. Вероятность попадания в цель при одном выстреле рав¬
на 0,6. Найти вероятность того, что при п = 1200 независимых выстре¬
лах отклонение «частости» от вероятности по модулю не превышает
е = 0,05.
О ^1200 {|^г - 0,б| ^ 0,05} = 2Ф0 (о,05 •	=	2Ф°<3’54)	=
= 0,9996.	’	•

Глава 1. Случайные события ■ 59
Упражнения
1.	На лекции по теории вероятностей присутствуют 84 студента. Ка¬
кова вероятность того, что среди них есть 2 студента, у которых
сегодня день рождения?
2.	Вероятность брака при изготовлении некоторого изделия равна
0,02. Найти вероятность того, что среди 200 произведенных изде¬
лий не более одного бракованного.
3.	Найти вероятность того, что при подбрасывании монеты 100 раз
событие А — появление герба — наступит ровно 60 раз.
4-	Найти такое число га, чтобы с вероятностью 0,95 можно было бы
утверждать, что среди 800 новорожденных более т девочек. Счи¬
тать, что вероятность рождения девочки равна 0,485.

Глава 2
Случайные величины
2.1.	Понятие случайной величины. Закон
распределения случайной величины
Одним из важнейших понятий теории вероятностей (наряду со слу¬
чайным событием и вероятностью) является понятие случайной вели¬
чины.
Под случайной величиной понимают величину, которая в результа¬
те опыта принимает то или иное значение, причем неизвестно заранее,
какое именно.
Случайные величины (сокращенно: с. в.) обозначаются прописны¬
ми латинскими буквами X. У, Zy... (или строчными греческими буква¬
ми £ (кси), г) (эта), в (тэта), гр (пси) и т. д.), а принимаемые ими значения
соответственно малыми буквами Х2, • • •, g/i, 2/2, Уз,	
Примерами с. в. могут служить: 1) X число очков, появляющих¬
ся при бросании игральной кости; 2) У — число выстрелов до первого
попадания в цель; 3) Z — время безотказной работы прибора и т. п.
(рост человека, курс доллара, количество бракованных деталей в пар¬
тии, температура воздуха, выигрыш игрока, координата точки при слу¬
чайном выборе ее на [0; 1], прибыль фирмы, ...).
Случайная величина, принимающая конечное или счетное множе¬
ство значений, называется дискретной (сокращенно: д. с. в.).
Если же множество возможных значений с. в. несчетно, то такая
величина называется непрерывной (сокращенно: н.с. в.).
То есть д. с. в. принимает отдельные изолированные друг от дру¬
га значения, а н. с. в. может принимать любые значения из некоторого
промежутка (например, значения на отрезке, на всей числовой прямой
и т.д.). Случайные величины X и У (примеры 1) и 2)) являются дис¬
кретными. С. в. Z (пример 3)) является непрерывной: ее возможные
значения принадлежат промежутку [0, £), где t ^ 0, правая граница
не определена (теоретически t = +00). Отметим, что рассматриваются
также с. в. смешанного типа.
Дадим теперь строгое определение с. в., исходя из теоретико-мно¬
жественной трактовки основных понятий теории вероятностей.

Глава 2. Случайные величины ■ 61
Случайной величиной X называется числовая функция, опреде¬
ленная на пространстве элементарных событий Л, которая каждому
элементарному событию w ставит в соответствие число X(w), т. е.
X = X(w), w Е ii (или X = f(w)).
Пример. Опыт состоит в бросании монеты 2 раза. На ПЭС Л =
—	{^ъ'и;2,гиз, т*}, где w\ = ГГ. W2 = ГР, г^з = РГ, ги4 = РР, можно
рассмотреть с. в. X — число появлений герба. С. в. X является функ¬
цией от элементарного события wf. X(w\) = 2, X(w2) = 1, -ЛГ(^з) = Ь
X(w4) = 0; X — д. с. в. со значениями х\ — 0,	= 1, #з = 2.
Отметим, что если множество Q койечно или счетно, то случай¬
ной величиной является любая функция, определенная на Q. В общем
случае функция X(w) должна быть такова, чтобы для любых х £ М
событие А = {w : X(w) < х} принадлежало сг-алгебре множеств S
и,	значит, для любого такого события была определена вероятность
Р{А) = Р{Х <х).
Для полного описания с. в. недостаточно лишь знания ее возмож¬
ных значений; необходимо еще знать вероятности этих значений.
Любое правило (таблица, функция, график), позволяющее нахо¬
дить вероятности произвольных событий Л С S' (S' — гх-апгебра собы¬
тий пространства Л), в частности, указывающее вероятности отдель¬
ных значений случайной величины или множества этих значений, на¬
зывается законом распределения случайной величины (или просто: рас¬
пределением). Про с. в. говорят, что «она подчиняется данному закону
распределения»:
2.2.	Закон распределения дискретной случайной
величины. Многоугольник оаспределения
Пусть X — д. с. в., которая принимает значения х\, #2,	, хп,...
(множество этих значений конечно или счетно) с некоторой вероят¬
ностью pi, где * = 1.2.3	п	 Закон распределения д. с. в. удобно
задавать с помощью формулы pi = Р{Х — Xi\, г = 1,2,3,...,™,...,
определяющей вероятность того, что в результате опыта с. в. X примет
значение Х{. Для д. с. в. X закон распределения может быть задан в
виде таблицы распределения,:
X
XI
Х2
Р
Р\
Р2
Рп

62 " Раздел первый. Элементарная теория вероятностей и случайных процессов
где первая строка содержит все возможные значения (обычно в порядке
возрастания) с. в., а вторая — их вероятности. Такую таблицу называ¬
ют рядом распределения.
Так как события {X = х\), {X = #2} • - • несовместны и образуют
полную группу, то сумма их вероятностей равна единице (см. п. 1.12),
Ер* =1
г
Закон распределения д. с. в. можно задать графически, если на оси
абсцисс отложить возможные значения с. в., а на оси ординат — вероят¬
ности этих значений. Ломаную, соединяющую последовательно точки
(#i?Pi)> (#2^2)5. называют многоугольником (или полигоном) рас¬
пределения (см. рис. 17).
Рис. 17
Теперь можно дать более точное определение д. с. в.
Случайная величина X дискретна, если существует конечное или
счетное множество чисел х\, #2, ...таких, что Р{Х = Xi} = Pi > О
(г = 1,2,...) и pi + р2 + рз + • - • = 1.
Определим математические операции над дискретными с. в.
Суммой (разностью, произведением) д. с. в. X, принимающей зна¬
чения Xi с вероятностями р\ — Р{X = a^}, i = 1,2,..., п и д. с. в. У. при¬
нимающей значения у3 с вероятностями pj = P{Y = yj}, j = 1,2,..., га,
называется д. с. в. Z = X + У (Z = X — У, Z = X • У), принимающая
значения Zij =	+ yj (^ = Xi — ?/j, ^	• уj) с вероятностями
Pij = Р{Х — х*г,У = yj} для всех указанных значений г и j. В случае
совпадения некоторых сумм хi + yj (разностей Xi — yj, произведений
%iVj) соответствующие вероятности складываются.
Произведение д. с. в. на число с называется д. с. в. сХ, принимающая
значения сщ с вероятностями pi = Р{Х = xi}.
Две д. с. в. X и У называются независимыми, если события
{X =	=	Лг	и {У = yj} = независимы для любых г = 1,2,... .п;

Глава 2. Случайные величины ■ 63
j = 1,2,... ,m, т.е.
Р{Х = xi; Y =	= F{X	= ж*}	■ P{Y = у,}*
В противном случае с. в. называются зависимыми. Несколько с. в. на¬
зываются взаимно независимыми, если закон распределения любой из
них не зависит от того, какие возможные значения приняли остальные
величины.
Пример 2.1. В урне 8 шаров, из которых 5 белых, остальные — чер¬
ные. Из нее вынимают наудачу 3 шара. Найти закон распределения
числа белых шаров в выборке.
О	Возможные значения с. в. X — числа белых шаров в выборке есть
х\ = О, Х2 = 1, #з = 2, #4 = 3. Вероятности их соответственно будут
-	PfX = ГН = ^5 '	_	_L	_	prv	_	.1 _	•	Cg	15
Pi	Р\Х	£,з	5g’	Р2	Р\Х	1)	£,3 gg,
8	8
Рз = gg, р4 = ^ . Закон распределения запишем в виде таблицы.
X
0
1
2
3
р
1
56
15
56
30
56
10
56
(Контроль: ЕРг 56 + 56 + 56 + 56	*
Упражнения
1 - Монета бросается 4 раза. Построить многоугольник распределения
с. в. X — числа выпадений герба.
2- Вероятность сдачи экзамена первым студентом равна 0,6, а вто¬
рым — 0,9. Составить ряд распределения с. в. X — числа студен¬
тов, успешно сдавших экзамен в случае, когда: а) экзамены пере¬
сдавать нельзя; б) экзамен можно один раз пересдать.

64 " Раздел первый. Элементарная теория вероятностей и случайных процессов
2.3.	Функция распределения и ее свойства. Функция
распределения дискретной случайной величины
Очевидно, ряд распределения с. в. может быть построен только для
д. с. в.; для н. в. нельзя даже перечислить все ее возможные значения.
Кроме того, как увидим позже (п. 2.3, 2.4), вероятность каждого отдель¬
но взятого значения н. с. в. равна нулю! Представим себе вероятность
того, что рост мужчины — н. с. в. — точно равен v^3 = 1,7320508 ... ме¬
тров; купленная нами лампа проработает — н. с. в. — ровно 900 часов;
.... Удивительно интересный факт: событие возможное, но имеет ну-
левую вероятность.
Для характеристики поведения н. с. в. целесообразно использовать
вероятность события {А" < х} (а не {X — х}), где х — некоторое дей¬
ствительное число. С точки зрения практики нас мало интересует собы¬
тие, состоящее, например, в том, что лампочка проработает ровно 900
часов, т. е. X = 900. Более важным является событие вида {X < 900}
(или {X > 900}). Такое событие имеет ненулевую вероятность; при из¬
менении х вероятность события {X < х} в общем случае будет менять¬
ся. Следовательно, вероятность Р{Х < х} является функцией от х.
Универсальным способом задания закона распределения вероятно¬
стей, пригодным как для дискретных, так и для непрерывных случай¬
ных величин, является ее функция распределения, обозначаемая Fx{x)
(или просто F(x), без индекса, если ясно, о какой с. в. идет речь).
Рч|	Функцией распределения с. в. X называется функция F(x), которая
для любого числа х G R равна вероятности события {X < х}.
Таким образом, по определению
F(x) = Р{Х < х} т. е. F{x) = P{w : X{w) < х}.	(2.1)
Функцию F(x) называют также интегральной функцией распределения.
Геометрически равенство (2.1) можно истолковать так: F(x) есть
вероятность того, что с. в. X примет значение, которое изображается
на числовой оси точкой, лежащей левее точки т. е. случайная точка
X попадет в интервал (—оо,х), см. рис. 18.
	Х<х
X х	я
Рис. 18
Функция распределения обладает следующими свойствами:

Глава 2. Случайные величины ■ 65
1.	F(x) ограничена, т. е.
О	^ F(x) ^ 1.
2.	F(x) — неубывающая функция на i?, т. е. если х2 > х\, то
F(x2) > F(x\).
3.	F(x) обращает в ноль на минус бесконечности и равна единице в
плюс бесконечности, т. е.
F(—00) = 0, F(+oo) = 1.
4.	Вероятность попадания с. в. X в промежуток [а, Ь) равна прираще¬
нию ее функции распределения на этом промежутке, т. е.
Р{а^Х <Ь}= F{b) - F{a).	(2.2)
5.	F(x) непрерывна слева, т. е.
lim F(x) = F(x 0).
х—Ухо—О
Q 1. Первое свойство следует из определения (2.1) и свойств вероят¬
ности (п. 1.11, 1.12).
2.	Пусть А = {X < x*i}, В — {X < х2}. Если х\ < х2, то собы¬
тие А влечет событие В (п. 1.4), т. е. А С В. Но тогда согласно свой¬
ству 4 (п. 1.12), имеем Р(А) ^ Р(В), т. е. Р{Х < х\} ^ Р{Х < х2} или
F(x 1) ^ F(x2).
Геометрически свойство 2 очевидно: при перемещении точки х
вправо по числовой оси вероятность попадания случайной точки X в
интервал (—оо, х) не может уменьшаться.
3.	Третье свойство вытекает непосредственно из того, что
{X < —оо} = 0, а {X < +00} = ft; согласно свойствам вероятности
(п. 1.11, 1.12), имеем: F(—00) = Р{Х < —оо} = Р{0} = О, F(+00) —
= Р{Х < +00} = P{ft} = 1.
4.	Так как а < Ь, то очевидно, что {X < b} = {X < а) + {а ^ X < Ь}
(это хорошо видно на рис. 19).
Так как слагаемые в правой части — несовместные события, то
по теореме сложения вероятностей (п. 1.11) получаем Р{Х < Ь} =
= Р{Х < а} + Р{а ^ X < Ь}. Отсюда следует Р{а ^ X < 6} =
= Р{Х < Ь} - Р{Х < а} = F(b) - F(a).
5.	Свойство 5 проиллюстрируем далее на примере 2.2.	■

66 ” Раздел первый. Элементарная теория вероятностей и случайных процессов
'/////
а
{А' < а}	{а	^	А"	<	Ь}
Рис. 19
Всякая функция F(x), обладающая свойствами 1 3, 5, может быть
функцией распределения некоторой случайной величины.
Заметим, что формула (2.2) (свойство 4) справедлива и для н. с. в.,
и для д. с. в.
С помощью функции распределения можно вычислить вероятность
события {X ^ х}:
Р{Х ^х} = 1- F(x).	(2.3)
Можно дать более точное определение н. с. в.
Случайную величину X называют непрерывной, если ее функция
распределения непрерывна в любой точке и дифференцируема всюду,
кроме, может быть, отдельных точек.
Используя свойство 4 можно показать, что «вероятность того, что
н. с. в. X примет заранее указанное определенное значение а, равна ну¬
лю».
Действительно, применим формулу (2.2) к промежутку [а, х):
Р{а ^ X < х} = F(x)—F(a). Будем неограниченно приближать точку х
к а. Так как функция F(x) непрерывна в точке а, то lim F(x) = F(a). В
x—ta
пределе получим Р{Х = а} = lim F(x) — F(a) = F(a) — F(a) = 0. Если
x—>a
функция F(x) везде непрерывна, то вероятность каждого отдельного
значения с. в. равна нулю.
Следовательно, для н. с. в. справедливы равенства
Р{а ^ х < Ь} = Р{а < х < b} = Р{а ^ х ^ b} = Р{Х Е (а, Ь]}.
Действительно,
Р{а ^ х < Ь} = Р{Х = а} 4- Р{а < х < Ь} = Р{а < х < Ь}
и т.д.
Функция распределения д. с. в. имеет вид
F(x)=Y,Pi-	(2-4)
Xi
Здесь суммирование ведется по всем г, для которых Xi < х. Равен¬
ство (2.4) непосредственно вытекает из определения (2.1).

Глава 2. Случайные величины ■ 67
Пример 2.2. По условию примера 2.1 (п. 2.2) найти функцию распре¬
деления F(x) и построить ее график.
О	Будем задавать различные значения х и находить для них F(x) =
-	Р{Х < х}:
1.	Если	х	^ 0, то, очевидно, F(x) = Р{Х < 0} = 0;
2.	Если	0	< х ^ 1, то Ffe) = Р{Х < гг} = Р{Х = 0} =
оо
3.	Если	1	< х < 2, то F(x) = Р{Х = 0} + Р{Х = 1} = ^ +	=	Ц;
4.	Если	2	< х < 3, то F{x) = Р{Х = 0} + Р{Х = 1} + Р{Х = 2} =
__1_,15,30_46
56 + 56	56	56’
5.	Если 3 < ж, то F{x) = Р{Х = 0} + Р{Х = 1} + Р{Х = 2} + Р{Х =
,46	-
1	56
Итак,
= 3> = i + i = 1-
F(x) =
о,
если
х ^ 0;
1
56’
если
0 < х ^ 1
1 16
1 56'
если
1 < х ^ 2
46
56 ’
если
2 < х ^ 3
1,
если
3 < х.
(:2.5)
Строим график F(x), рис. 20.
Как видим, функция распределения д. с. в. X есть разрывная, со
скачками р\ в точках функция, «непрерывная слева» (при подходе
к точке разрыва слева функция F(x) сохраняет значение). Ее график
имеет ступенчатый вид.

68 1 Раздел первый. Элементарная теория вероятностей и случайных процессов
Отметим, что, пользуясь равенством (2.4), функцию распределения
можно сразу записать в виде (2.5)
Го,
56’
_L , 15
56	56	’
±+15 +
56	56
±+15 +
56	56
X
<
0,
0
<
х ^
1,
1
<
X ^
2,
30
56 ’
2
<
X ^
3,
30 ,
56
10
56’
3
<
X.
Упражнения
1.	Два стрелка делают по одному выстрелу в одну мишень. Вероят¬
ность попадания для первого стрелка равна 0.6. а для второго —
0,8. Найти и построить функцию распределения с. в. X — числа
попаданий в мишень.
2.	Убедиться, что функция
F(T\ = /°> х < °>
\l — е“х, если х > 0
является функцией распределения некоторой случайной величины.
Найти Р{0 ^ х < 1} и построить график F(x).
3.	Дана функция распределения
г0, при х ^ 0,
, при 0 < х ^ л/2,
к1, при х > л/2-
Найти вероятность того, что в результате четырех испытаний с. в.
X трижды примет значение, принадлежащее интервалу (0; 1).
Fx(x) =

Глава 2. Случайные величины ■ 69
2.4.	Плотность распределения и ее свойства
Важнейшей характеристикой непрерывной случайной величины
(помимо функции распределения) является плотность распределения
вероятностей. Напомним (см. п. 2.3), что: с. в. X называется непрерыв¬
ной, если ее функция распределения непрерывна и дифференцируема
всюду, кроме, быть может, отдельных точек.
Плотностью распределения вероятностей (плотностью распреде¬
ления, плотностью вероятностей или просто плотностью) непрерыв¬
ной случайной величины X называется производная ее функции рас¬
пределения.
Обозначается плотность распределения н.с. в. X через fx(%) (или
Рх{%)) или просто f(x) (или р(х)), если ясно о какой с. в. идет речь.
Таким образом, по определению
/(*) = F\x).	(2.6)
Функцию f(x) называют также дифференциальной функцией распреде¬
ления; она является одной из форм закона распределения случайной
величины, существует только для непрерывных случайных величин.
Установим вероятностный смысл плотности распределения. Из
определения производной следует
/w_ ,im ,im F<*+A;)-F(*).
Дх->о Ах Дх-*о	Ах
Но согласно формуле (2.2), F(x + Ах) — F(x) = Р\х ^ X < х + Д#}.
Р{х ^ X < х + Ах}
Отношение 	т	 представляет собой среднюю вероят-
/\х
ность, которая приходится на единицу длины участка [х,х + Дх), т. е.
среднюю плотность распределения вероятности. Тогда
„/ ч	Р{х ^ X < х	+ Ах}
f(x) = lim — ——х	s- ,	2.7)
' Дх->о	Ах	v '
т. е. плотность распределения есть предел отношения вероятности по¬
падания с. в. в промежуток [х; х + Ах) к длине Ах этого промежутка,
когда Ах стремится к нулю. Из равенства (2.7) следует, что
Р{х ^ X < х + Ах} « f(x)Ax.
То есть плотность вероятности определяется как функция f(x), удо¬
влетворяются условию Р{х ^ X < х -I- dx} « f(x)dx; выражение
f(x)dx называется элементом вероятности.

70 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Отметим, что плотность f(x) аналогична таким понятиям, как
плотность распределения масс на оси абсцисс или плотность тока в
теории электричества.
Плотность распределения обладает следующими свойствами:
1.	f(x) неотрицательная, т. е.
/(*) > 0.
2.	Вероятность попадания н. с. в. в промежуток [а; Ь] равна определен¬
ному интегралу от ее плотности в пределах от а до 6, т. е.
ь
Р{а^Х О} = J f(x) dx.	(2.8)
а
3.	Функция распределения н. с. в. может быть выражена через ее
плотность вероятности по формуле
х
F(x)= j
—	ОО
4.	Условие нормировки: несобственный интеграл от плотности веро¬
ятности н.с. в. в бесконечных пределах равен единице, т. е.
сю
I f(x)dx = 1.
—оо
Q 1. Плотность распределения f(x) — неотрицательная функция:
F(x) — неубывающая функция (п. 2.3), следовательно, F'(x) ^ 0, т. е.
f(x) ^ 0. Это означает, что график плотности /(ж), называемый кривой
распределения, не ниже оси абсцисс; плотность может принимать сколь
угодно большие значения.
2.	Так как F(x) есть первообразная для плотности f(x), то по фор¬
муле Ньютона-Лейбница имеем
ъ
J f(x) dx = F(b) — F(a).
a
Отсюда в силу свойства 4 функции распределения (формула (2.2)), по¬
лучаем
ъ
J f(x)dx = Р{а^ X О}.
а

Глава 2. Случайные величины ■ 71
Геометрически эта вероятность равна площади S фигуры, ограничен¬
ной сверху кривой распределения f(x) и опирающейся на отрезок [а; 6]
(рис. 21).
3.	Используя свойство 2, получаем:
х	х
F(x) = Р{Х < х} = Р{—оо < X < х} = j f(x) dx = J f(t)
dt
(буква t для ясности).
4.	Полагая в формуле (2.8) а — —оо и b — +оо, получаем достовер¬
ное событие X Е (—оо;+оо). Следовательно,
+со
J f(x) dx = Р{—оо < X < -foe} = P{Q] = 1.
Геометрически свойство нормировки означает, что площадь фигуры,
ограниченной кривой распределения f(x) и осью абсцисс, равна еди¬
нице.	■
Можно дать такое определение непрерывной случайной величины:
случайная величина X называется непрерывной, если существует не¬
отрицательная функция f(x) такая, что при любом х функцию распре¬
деления F(x) можно представить в виде
X
F(x) = J f(t) dt.
—	ОО
А затем получить, что f(x) = Ff(x). Отсюда следует, что F(x) и f(x)
являются эквивалентными обобщающими характеристиками с. в. X.

72 1 Раздел первый. Элементарная теория вероятностей и случайных процессов
Как отмечалось ранее (п. 2.3) для н. с. в. X, вероятность события
{X = с}, где с — число, равна нулю. Действительно,
с
Р{х = с} = Р\с^Х ^с} = J f(x) dx = 0.
С
Отсюда также следует, что
Р{Х Е [а;Ь)} = Р{Х Е [о;Ь]} = Р{Х Е (а;6)}.
Пример 2.3. Плотность распределения с. в. X задана функцией f(x) =
= —Q—z . Найти значение параметра а.
1	+ хг
о	Согласно свойству 4 плотности, имеем
-foo	d
I	—^—77 dx = 1, т. e. a lim / —0 = 1, т. e. a- lim arctgjcl^ 1
J 1 + x2	4^+coJ l + x2	«-+-	b lc
—OO	C-+-OC c	c-»-oo
или a •	Ijr))	=	1	наконец,	получаем	а7г	=	1,	т. е. а = ^ .	•
Упражнения
1.	Случайная величина X задана функцией распределения:
*
F(x) = <
0,	при х ^ — 1,
а(х + I)2,	при — 1 < х	^ 2,
k 1,	при х > 2.
Найти значение а, построить графики F(x) и /(ж).
2.	Кривая распределения н. с. в. X имеет вид, указанный на рис. 22.
Найти выражение для /л(^), функцию распределения Fx{x), веро¬
ятность события Е (И}-

Глава 2. Случайные величины ■ 73
N
О
1
2
х
Рис. 22
3- Является ли плотностью распределения некоторой с. в. каждая из
следующих функций:
2.5.	Числовые характеристики случайных величин
Закон распределения полностью характеризует случайную величи¬
ну. Однако при решении многих практических задач достаточно знать
лишь некоторые числовые параметры, характеризующие отдельные
существенные свойства (черты) закона распределения с. в. Такие чи¬
сла принято называть числовыми характеристиками с. в.
Важнейшими среди них являются характеристики положения: ма¬
тематическое ожидание (центр распределения с. в.), мода, медиана; ха¬
рактеристики рассеяния: дисперсия (отклонение значений с. в. от ее
центра), среднее квадратическое отклонение.
Математическое ожидание случайной величины
Математическим ожиданием (или средним значением) д. с. в. X,
имеющей закон распределения pi = Р{Х = ^}, г = 1,2,3,..., п, назы¬
вается число, равное сумме произведений всех ее значений на соответ¬
ствующие им вероятности.
■) я*)={
^аж2, при 0 ^ х ^ 2.
О, при х < 0 и х > 2,

74 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Математическое ожидание (сокращенно: м. о.) обозначается через
MX (или: М[Х\, М(Х), EX. тх. ах).
Таким образом, по определению
п
MX = J2xi‘Pi-	(2-9)
г=1
Если число возможных значений с. в. X бесконечно (счетно), то
оо
=	(2.10)
t=l
причем ряд в правой части предполагается сходящимся (в противном
случае с. в. X не имеет м. о.).
Формулы (2.9) и (2.10) можно записать в виде MX =
г
Вероятностный смысл математического ожидания состоит в том,
п
что оно является средним значением с. в. Действительно, т. к. ^ p* = 1,
г—1
ТО
п
п	&iPi
MX — ^ ^ &iPi —	“	— ^среднее •
Е Pi
г—\
Математическим ожиданием н.с. в. X с плотностью вероятности
/(#), называется число
оо
МХ= J X -f{x)dx.	(2.11)
—	ОО
Интеграл в правой части равенства (2.11) предполагается абсолютно
сходящимся, т. е.
ОС
J \х\ • f(x) dx < оо
—оо
(в противном случае н.с.в. X не имеет м.о.).
Формула (2.11) является интегральным аналогом формулы (2.9).
Заменяя в ней «прыгающий» аргумент Xi на непрерывно меняющий¬
ся ж, вероятность р^ — элементом вероятности f(x)dx (f(x)dx =
= F,(x)dx — dF(x) « ДР(я) = F(x + Ax) — F(x) = P{x < X < x + Ax}),
получим равенство (2.11).

Глава 2. Случайные величины ■ 75
Отметим, что MX имеет ту же размерность, что и с. в. X.
Свойства математического ожидания.
1.	Математическое ожидание постоянной равно самой этой постоян¬
ной, т. е.
Мс = с.
2.	Постоянный множитель выносится за знак м. о., т. е.
М{сХ) = сМХ.
3.	М. о. суммы с. в. равно сумме их м. о., т. е.
М(Х + У) = MX + MY.
4.	М. о. отклонения с. в. от ее м. о. равно нулю, т. е.
М(Х - MX) - 0.
5.	М. о. произведения независимых с. в. равно произведению их м. о.,
т. е. если X и Y независимы, то
М{Х • Y) = MX • MY.
Q 1. Постоянную с можно рассматривать как д. с. в. X, принимающую
лишь одно значение с вероятностью 1. Поэтому Мс = с • Р{Х — с} =
= с • 1 = с.
2.	Так как д. с. в. сХ принимает значения сх{ (i = 1 , п) с вероятно¬
стями pi, то
п	п
МсХ = ^ CXi - Pi — С ^2 xiPi — сМХ.
г—1	г=1
3.	Так как д. с. в. X + Y принимает значения Xi+yj с вероятностями
Pij = Р{Хг =Xi,Y = г/j}, то
пт	пт	пт
м{х+y) = 53 53 +yjjpij = 53 53+53 53 yjpij=
г=1 ji=]	z=l	jz=l	i=l	j=1
пт	т п	п	т
= 53Хг 53^+53% 53ру = 53®i -л+53% - pj =	+му-
г=1	1	.7=1 г—1	г—1	j=l
При доказательстве воспользовались, в частности, тем, что
т	п
53 fti=Pi и 53 Pij=Pj-
j—1	г=1

76 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Действительно: так как
т	т
= Xi;Y = Vj} = {Х = x.i} ]Г{У = yj} = {X = Xi}-n = {X = Xi}.
i=1	j'=i
TO
Pi = P{X = Xi] =	=	X*Y	=	%•>)	=
m	m
= Y1P{X = XuY = Vj} =
3=1	3=1
аналогично получаем
n
Рз = 'E.Pij-
i=1
Свойство 3 распространяется на произвольное конечное число слагае¬
мых.
4.	Согласно свойствам 1 и 3, имеем: М(Х—МХ) = MX — М(МХ) =
= MX — MX = 0. Отметим, что разность X — MX (или X — тх) назы¬
вается отклонением с. в. X от ее м. о. MX и обозначается символом X:
х=х-мх.
Эта с. в. X называется также центрированной с. в.
5.	Так как с.в. X и У независимы, то = Р{Х = Xf,Y = ?/j} =
= Р{Х = Xi) • Р{У =	= pi • pj. Следовательно,
п т
MXY = ЕЕ аэд	= Xj;Y = yj}
> | j=i ~v'
Pij
71	Til
E E	p{y	=	%•}	=	EE УзРз = MX ■MY-
Свойства м. о., доказанные для д. с. в., остаются справедливы и для
непрерывных с. в. Так, например,
оо	оо
МсХ — J cxf(x)dx = c j xf(x)dx = cMX.

Глава 2. Случайные величины ■ 77
Пример 2.4. В лотерее имеется 1000 билетов, из них выигрышных: 10
по 500 руб, 50 по 50 руб, 100.no 10 руб, 150 по 1 руб. Найти математи¬
ческое ожидание выигрыша на один билет.
О	Ряд распределения с. в. X — суммы выигрыша на один билет таков:
X
500
50
10
1
0
p
0,01
0,05
0,1
0,15
0,69
(Контроль:	=	1.)	Находим	MX:
MX = 500 • Ofil + 50 • 0,05 + 10 • 0,1 + 1 • 0,15 + 0 • 0,69 = 8,65 руб.
Дисперсия
Дисперсией (рассеянием) с. в. X называется математическое ожи¬
дание квадрата ее отклонения от своего математического ожидания.
Обозначается дисперсия через DX (или D[X], D\, D(X)). Таким
образом, по определению
DX = М(Х - MX)2,
(2.12)
или DX = MX2, или DX = М(Х — т\)2. Дисш')х:ия характеризует
разброс значений с. в. X относительно ее м. о. Из определения диспер¬
сии следуют формулы для ее вычисления:
DX =	—	MX)2	•	pi — для д. с. в. X,
г
4-оо
DX = J (х — MX)2 • f(x) dx — для н. с. в. X.
—оо
На практике дисперсию с. в. удобно находить по формуле
DX = MX2 - (MX)2.
(2.13)
(2.14)
(2.15)
Она получается из формулы (2.12): DX = М(Х2 — 2Х-МХ + (МХ)2) =
= MX2 - М(2Х ■ MX) + М{МХ)2 = MX2 - 2MX ■ MX + (MX)2 =
= MX2 - (MX)2.

78 1 Раздел первый. Элементарная теория вероятностей и случайных процессов
Это позволяет записать формулы для ее вычисления ((2.13) и
(2.14)) в другом виде:
DX = ^x?Pi-(MX)\	(2.16)
i
+оо
DX= J x2 ■ f(x)dx-(MX)2.	(2.17)
—	OO
Свойства дисперсии.
1.	Дисперсия постоянной равна нулю. т. е.
Dc = 0.
2.	Постоянный множитель можно выносить за знак дисперсии, возве¬
дя его в квадрат, т. е.
DcX -- c2DX.
3.	Дисперсия суммы независимых с. в. равна сумме их дисперсий, т. е.
если X и Y независимы, то
D(X + Y) = DX + DY.
4.	Дисперсия с. в. не изменится, если к этой с. в. прибавить постоян¬
ную, т. е.
D{X + с) = DX.
5.	Если с. в. X и Y независимы, то
D(XY) = MX2 ■ MY2 - (MX)2 ■ (MY)2.
□ 1. Dc = M(c - Me)2 = M(c - c)2 = MO = 0.
2.	DcX = M(cX -M(cX))2 = M(cX-cMX)2 = M(c2(X-MX)2) =
= <?M(X - MX)2 = <?DX.
3.	Используя формулу (2.15), получаем D(X + У) = M(X + У)2 —
~(M(X + Y))2 = MX2+2М XY+MY2 — (MX)2 — 2M X ■ MY — (MY)2 =
= MX2 - (MX)2 + MY2 - (MY)2 + 2(MXY - MX ■ MY) = DX + DY +
+ 2(MX • MY - MX ■ MY) = DX + DY.
Отметим, что если с. в. X и У зависимы, то
D(X + Y) = DX + DY + 2М((Х - MX) ■ (У - MY)).
4.	D(c + X)= M((c + X)~ M(c + X))2 = M(X - MX)2 = DX.
Доказательство свойства 5 не приводим.	■
Свойства дисперсии, доказанные выше для дискретных случайных
величин, остаются справедливыми и для непрерывных с. в.

Глава 2. Случайные величины ■ 79
Среднее квадратическое отклонение
Дисперсия DX имеет размерность квадрата с. в. X, что в сравни¬
тельных целях неудобно. Когда желательно, чтобы оценка разброса
(рассеяния) имела размерность с. в., используют еще одну числовую
характеристику — среднее квадратическое отклонение (сокращенно:
с. к. о.).
Средним квадратическим, отклонением или стандартным откло¬
нением с. в. X называется квадратный корень из ее дисперсии, обозна¬
чают через ох (или оХ, сг[Х], а). Таким образом, по определению
ах - VDX.
(2.18)
Из свойств дисперсии вытекают соответствующие свойства с. к. о.:
ас = 0, асХ = |с|сгд', <т(с + X) = аХ-
Для изучения свойств случайного явления, независящих от выбо¬
ра масштаба измерения и положения центра группирования, исходную
случайную величину X приводят к некоторому стандартному виду: ее
центрируют, т. е. записывают разность X — MX (геометрически озна¬
чает, что начало координат переносится в точку с абсциссой, равной
м. о.), затем делят на с. к. о. ах-
X-MX
Случайную величину Z =
ох
называют стандартной слу¬
чайной величиной. Ее м. о. равно 0, а дисперсия равна 1. Действительно,
'X -MX'
MZ
= м(-
ох
DZ = \D{X - MX) =
) = ^М(Х - MX) = о,
DX
DX
= 1.
То есть Z — центрированная (MZ = 0) и нормированная (DZ = 1)
случайная величина.
Пример 2.5. Д. с. в. X задана рядом распределения.
X
-1
0
1
2
р
0,2
0,1
0,3
0,4
Найти MX. DX, (Jх •
О Используем формулы (2.9), (2.13), (2.18): MX = —1 • 0,2 + 0 • ОД +
+ 1-0,3+2-0,4 = 0,9; DX = (-1-0,9)2-0,2+(0-0,9)2-0,1 + (1-0,9)2-0,3+
+ (2—0,9)2-0,4 = 1,29 (или, используя формулу (2.16). DX = (—1)2-0,2+
+ О2 • 0,1 + I2 • 0,3 + 22 • 0,4 - (0,9)2 = 1,29); ах = л/Щ* ~ М4.	•

80 ■ Раздел первый- Элементарная теория вероятностей и случайных процессов
Мода и медиана. Моменты случайных величин. Асимметрия
и эксцесс. Квантили
Модой д. с. в. X называется ее значение, принимаемое с наибольшей
вероятностью по сравнению с двумя соседними значениями, обознача¬
ется через MqX. Для н. с. в. MqX — точка максимума (локального)
плотности fx{x)-
Если мода единственна, то распределение с. в. называется унимо¬
дальным, в противном случае — нолимодальным (рис. 23).
Медианой МеХ н.с. в. X называется такое ее значение хр, для ко¬
торого
Р{Х < хр} = Р{Х > хр} = ± ,	(2.19)
т. е. одинаково вероятно, окажется ли с. в. X меньше хр или больше хр
(рис. 23).
С помощью функции распределения F(x) равенство (2.19) можно
записать в виде F(MeX) = 1 — F(MeX). Отсюда F(MeX) = ~ .
Для д. с. в. медиана обычно не определяется.
Математическое ожидание и дисперсия являются частными случа¬
ями следующих более общих понятий — моментов с. в.
Начальным моментом порядка к с. в. X называется м. о. fc-й сте¬
пени этой величины, обозначается через а*.
Таким образом, по определению
ак = М(Хк).
Для д. с. в. начальный момент выражается суммой:
ак = ^2	- pi,
г

Глава 2. Случайные величины "81
а для н. с. в. — интегралом:
оо
ак= J хк - f(x) dx.
В частности, а\ = MX, т. е. начальный момент 1-го порядка есть м. о.
Центральным моментом порядка к с. в. X называется м. о. вели¬
чины (X — МХ)к, обозначается через
Таким образом, по определению
цк = М(Х - МХ)к.
В частности, //2 — DX, т. е. центральный момент 2-го порядка есть
дисперсия; j.i\ = М(Х — MX) — U (см. свойство 4 м. о.).
Для д. с. в.:
Рк =	“	МХ)к	•	Рг,
г
а для н.с. в.:
ос
I	dx.
fik= J (x-MX)k-f(x)>
Центральные моменты могут быть выражены через начальные
моменты. Так, = DX = 02 ~ (действительно: //2 — DX =
= MX2 — (MX)2 = 02 — of); //3 = 03 — 3aia2 + 2af, //4 = 04 — 4aia% +
+ 6a202 — 3oj и т. д.
Среди моментов высших порядков особое значение имеют цен¬
тральные моменты 3-го и 4-го порядков, называемых соответственно
коэффициентами асимметрии и эксцесса.
Коэффициентом; асимметрии («скошенности») Ас.в. X называ¬
ется величина
^3 _M{X-MXf
Л —	—	о
(М)2
Если А > 0, то кривая распределения более полога справа от М$Х
(рис. 24).
Если А < 0, то кривая распределения более полога слева от М$Х
(рис. 25).
Коэффициентом эксцесса («островершинности») Е с. в. X назы¬
вается величина

82 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Рис. 25
Величина Е характеризует островершинность или плосковершин-
ность распределения. Для нормального закона распределения (см.
п. 2.7) А = 0 и Е = 0; остальные распределения сравниваются с нор¬
мальным: если Е > 0 — более островершинные, а распределения «плос¬
ковершинные» имеют Е < 0 (рис. 26).
Кроме рассмотренных выше числовых характеристик с. в., в при¬
ложениях используются так называемые квантили.
Квантилью уровня р с. в. X называется решение уравнения
Fx(xp)=p,
где р — некоторое число, 0 < р < 1.
Квантили #0,25 > хо,5 и #о,75 имеют свои названия: нижняя квартиль,
медиана (МеХ = #0,5)- верхняя квартиль соответственно. Они делят
числовую прямую на 4 части, вероятности попадания в которые равны
0,25 (рис. 27).

Глава 2. Случайные величины ■ 83
f(x)
О
3*0,25 #0,5 #0,75
X
Рис. 21
Упражнения
1	- Производится 3 независимых выстрела по цели. Вероятности по¬
падания при разных выстрелах одинаковы и равны р = 0,9. Найти
м. о. числа попаданий. Решить задачу в случае, если вероятности
попадания при разных выстрелах различны: а) р\ = 0,7, б) р2 = 0,8,
в) Рз = 0,9.
2.	Непрерывная случайная величина задана плотностью распределе¬
ния
Найти математическое ожидание случайной величины X.
3.	Пусть X и У — независимые д. с. в., причем MX = 2, MY = —3,
DX = 2, DY = 9. Найти MZ и DZ, если Z = 5X - ЗУ + 2.
4.	По условию упражнения 2 найти DX и ах-
5.	С. в. X задана функцией распределения
При X ^ 0 И X > 7Г,
При 0 < X ^ 7Г.
г 0,	при	х ^ А,
F(x) = < 0,25а;2, при А < х ^ В,
1,	при	В	<	£•.
Найти значения А и MX и <тд'.
6- Пусть X]. Х2	Хп — последовательность независимых с. в.
с MXi = а и Z)X< = <т2, г = 1,2,	Найти	м.о. и дисперсию
среднего арифметического п независимых с. в. X*.

84 1 Раздел первый. Элементарная теория вероятностей и случайных процессов
2.6.	Производящая функция
Нахождение важнейших числовых характеристик д. с. в. с целыми
неотрицательными значениями удобно производить с помощью произ¬
водящих функций.
Пусть д. с. в. X принимает значения 0,1> 2,..., fc,... с вероятностя-
ми po,Pi,P2,---,Pfc = р{Х = к},....
Производящей функцией для д. с. в. X называется функция вида
оо
4>(z) = 'YhPk ■ zk = РО +P1Z + P2Z2 + ...,	(2.20)
fc=о
где г — произвольный параметр, 0 < г ^ 1.
Отметим, что коэффициентами степенного ряда (2.20) являются
вероятности закона распределения д. с. в. X.
Дифференцируя по г производящую функцию, получим
оо
ffi'(z) = '^2k-pk-zlc~l.
fc=q
Тогда
DO
<p,(l)='£k-pk = MX = al,
k-0
т. e.
ai = MX = v'(l).	(2.21)
Взяв вторую производную функции <p(z) и положив в ней z = 1, полу¬
чим:
оо	оо	оо
v"(z) ^^2k(k-l) ■ Pk • zk~2,	Ч>"{	1)	=	k2	•	Pk	-	Ylk • pk = g2 - ai,
k=0	k—0	k—0
где с*2	и ai	—	начальные моменты соответственно 2-го	и	1-го порядков
(a2 =	MX2,	«1	=	MX). Тогда DX = MX2 - {MX)2	=	a2 - af =
= (a2 - ai) + «1 - a2 = tp"(I) + <p’(l) ~ (<p'(l))2, т.е.
DX = </(1) + ¥>'(1) ~ (/(l))2.	(2-22)
Полученные формулы (2.21) и (2.22) используются для нахождения
м. о. и дисперсии рассматриваемого распределения.
Пример 2.6. Найти дисперсию с. в. X — числа попаданий в упражне¬
нии 1 (п. 2.5).

Глава 2. Случайные величины ■ 85
о Ряд распределения с. в. X:
X
Р
О
0,01
1
0,027
0,243
0,729
Найдем DX. используя формулу (2.22). Производящая функция <p(z) =
= 0,01 + 0,027z + 0,243z2+0,729z3. Тогда <pf(z) = 0,027+ 0,4862 +2,187г2.
Полагая z = 1, находим tp'( 1) = 2,7 = MX (упражнение 1 из п. 2.5).
<p"{z) = 0,486 + 4,374г. Поэтому у?"(1) = 4,46 и DX = 4,86 + 2,7-(2,7)2 =
= 0,27 (формула (2.22)).
Аналогично решаем во втором случае, когда вероятности при раз¬
ных выстрелах различны (п. 1.20, пример 1.31). <p(z) = 0,006 + 0,092;: +
+ 0,398-г2 + 0,504.г3. <p'(z) = 0,092 + 0,796z + 1,512г2,	= 2.4 = MX.
ip"{z) = 0,796 + 3,024г, ip”{l) = 3.82. Поэтому DX = 3,82 + 2,4- (2,4)2 =
= 0,46.	•
2.7.	Основные законы распределения случайных
величин
Биномиальный закон распределения
Среди законов распределения д. с. в. наиболее распространенным
является биномиальное распределение, с которым мы уже встречались
(п. 1.20).
Дискретная с. в. X имеет биномиальное распределение (или рас¬
пределена по биномиальному закону), если она принимает значения
0,1,2,3,..., п, с вероятностями:
Рт = Р{Х = т} = С™р™Я"
(2.23)
где 0 < р < 1, q = 1 — р, га = 0,1,	п.
Случайная величина X, распределенная по биномиальному закону,
является числом успехов с вероятностью р в схеме Бернулли проведе¬
ния п независимых опытов.
Если требуется вычислить вероятность «не менее га успехов в п
независимых опытах», т. е. Р{Х ^ га}, то имеем : Рт = Р{Х ^ га} =
—	Р{Х = га} + Р{Х = га +1} + *.. + Р{Х — п}. Вероятность Ргп бывает

86 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
удобно находить через вероятность противоположного события: Рт =
= Р{Х ^ т} = 1 — Р{Х < га}; та из двух формул лучше, где меньше
слагаемых.
Ряд распределения д. с. в. X, имеющей биномиальное распределе¬
ние, имеет вид:
X = т
0
1
2
т
П
рт = Р{Х = т}
дп
сйрУ"1
Clvlqn~2
г^т г.т пп—'т
У Ч
рп
п
Контроль: Y, Рт = (р + Я)п = 1-
771=0
Функция распределения с. в. X, распределенной по биномиальному
закону, имеет вид:
г0,	при х ^ 0
Fix) = < £ C™pmqn~m, при 0 < х ^ п
v 7 т<х
при П < X.
Найдем числовые характеристики этого распределения. Производящей
функцией биномиального распределения является
ф) = £ C™pmqn~mzm = £ С™ (jpz)m qn~m = (q + pz)n,
т—0	m=0
т. е. (/?(г) = (<7 + pz)n. Тогда
4>{z) = n(g +	=	n(n	“ 1)Р2(? + pz)n~2.
Следовательно (см. п. 2.6), MX = </?'(1) = пр, т. к. p + tf = 1, DX —
=	+	V^U)	~~ (v^7(l))2 — п(п — 1)р2 + пр — п2р2 = npg. Итак,
MX = np, DX = npQ.	(2.24)
Эти формулы полезно знать.
Пример 2.7. По условию упражнения 1 из п. 2.5 найти MX и DX, где
X — число попаданий в цель.
О С. в. X имеет биномиальное распределение. Здесь п = 3, р = 0,9,
q = 0,1. По формулам (2.24) находим MX и DX: MX = 3 • 0,9 = 2,7,
DX = 3*0,9-0,1 = 0,27.	•

Глава 2. Случайные величины ■ 87
Распределение Пуассона
Pd	Дискретная	случайная	величина	X	имеет	распределение Пуассона,
если ее возможные значения: 0,1,2,..., га,... (счетное множество зна¬
чений), а соответствующие вероятности выражаются формулой Пуас¬
сона
рт = Р{Х = т} = ат^а,	(2.25)
где га = 0,1,2,..., а — параметр.
Распределение Пуассона является предельным для биномиального,
когда п —» ос и р -» 0 так, что пр = а — постоянно. Примерами слу¬
чайных величин, имеющих распределение Пуассона, являются: число
вызовов на телефонной станции за время t\ число опечаток в большом
тексте; число бракованных деталей в большой партии; число а-частиц,
испускаемых радиоактивным источником, и т.д. При этом считается,
что события появляются независимо друг от друга с постоянной сред¬
ней интенсивностью, характеризующейся параметром а = пр.
Случайная величина X, распределенная по закону Пуассона, имеет
следующий ряд распределения
X = т
0
1
2
т
Рт
е“°
а ■ е~°
а'2 ■ е~а
...
ат ■ е~а
1!
2!
ш!
г г171
Контроль:	рш	= е а —у — е а • еа = 1.
т=0	in—0
Найдем м. о. и дисперсию с. в. X, распределенной по закону Пуас¬
сона. Производящей функцией распределения Пуассона будет
ф) = у*	zm = е-а g	= е-а . e«z =
т\	^	т!
т=0	т—О
т. е. ip(z) = ea^z~l\ Тогда <pf(z) = а •	<pf,(z)	=	а2 • еа(г_1). Стало
быть, MX = у/(1) =a,DX = у/#(1) + <^'(1) - (<^'(1))2 = а2 + а - а2 = а.
Итак,
MX = DX = a,	(2.26)
т. е. параметр а пуассоновского распределения равен одновременно м. о.
и дисперсии с. в. X, имеющей это распределение. В этом состоит отли¬
чительная особенность изучаемого распределения, которая использу¬
ется на практике (на основании опытных данных находят оценки для
м. о. и дисперсии; если они близки между собой, то есть основание счи¬
тать, что с. в. распределена по закону Пуассона).

88 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 2.8. Вероятность попадания в цель при одном выстреле равна
0,01. Какова вероятность того, что число попаданий при 200 выстрелах
составит не менее 5 и не более 10?
О	Вероятность р = 0,01 очень мала, а число выстрелов (опытов)
достаточно велико. Поэтому искомую вероятность будем находить,
используя формулу Пуассона. С. в. X — число попаданий. Требу¬
ется найти Р{5 ^ X ^ 10}. По теореме сложения вероятностей
Р{5	^	X	^	10}	=	Р{Х	= 5}	+ Р{Х = 6} +	... + Р{Х =	10}.	Име¬
ем: а — пр =	200	•	0,01 = 2. е~а	= еГ2 « 0,135,
(о5	об о7 о8	о9	оЮ \
l	+ l + lr + l	+	l	+	T0!jK0 053-	*
Геометрическое распределение
Рм	Дискретная с. в. X имеет геометрическое распределение, если ее
возможные значения: 1,2,3,4,..., а вероятности этих значений:
Pm = P{X = m} = qm-lp,	(2.27)
где m = 1,2,3,	
Геометрическое распределение имеет с. в. X, равная числу опы¬
тов в схеме Бернулли, проведенных до первого успеха, с вероятностью
успеха р в единичном опыте. Примерами реальных случайных величин,
распределенных по геометрическому закону, являются: число выстре¬
лов до первого попадания, число испытаний прибора до первого отказа,
число бросаний монеты до первого выпадения решки и т. д.
Ряд распределения случайной величины X, имеющей геометриче¬
ское распределение, имеет вид
X = т
1
2
3
Рт
Р
qp
2
ЯР
ои	cxj	Л	Г)
Контроль: £ pqm~l = р £ qm~l = р • у— = - = 1.
m= 1	171=	1	1	q	у
Вероятности рт образуют геометрическую прогрессию р, qp, q2p,
qspy	По этой причине распределение (2.27) называют геометриче¬
ским.

Глава 2. Случайные величины ■ 89
Найдем математическое ожидание и дисперсию геометрического
распределения. Производящей функцией для с. в. X является функ¬
ция
оо	оо
Ф) = £ Г-V” = г* Е <«*)”*■'=
т— 1	ш=1
т.е. у>(г) = 1	•	Для	нее	у/(г)	=	-	Р	^"(г)	=	—',■ Стало
1	— <7г	(1 — <72)z	(1 — qz)*
быть,
1
р->
МХ = ^(1) =	2
(1 -я) г
DX = у>"(1) + ^(1) - (*/(1))2 = ^Г + £- ^ = ^
(2.28)
Г V
т.е. MX = i, DX = 4, и, значит, <тх =
Пример 2.9. Вероятность попадания в цель при отдельном выстре¬
ле для данного стрелка равна 0,1. Найти математическое ожидание и
дисперсию с. в. X — числа выстрелов по цели до первого попадания.
о С. в. X имеет геометрическое распределение с параметром р — ОД.
По формулам (2.28): МХ = ^ = 10, DX =	=	90	(а*	=	\/§0	=
7	(иД,)
-	3\/10).	•
Замечание. Геометрическое распределение является частным слу¬
чаем так называемого распределения Паскаля: Р{Х = т} ~
= Cl-^tfq™- \ т = 1,2,3,..., г > 0 — целое; MX = DX = Г~.
При г = 1 распределение Паскаля совпадает с геометрическим; при
г > 1 — совпадает с распределением суммы независимых с. в., имею¬
щих геометрическое распределение; при натуральном г оно описывает
число опытов в схеме Бернулли, необходимых для того, чтобы полу¬
чить значение 1 ровно г раз. Распределение Паскаля имеет приложение
к статистике несчастных случаев и заболеваний и т. д.

90 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Гипергеометрический закон распределения
Дискретная с. в. X имеет гцпергеометрическое распределение, если
она принимает значения 0.1.2... .jnv.. ,min(n,M) с вероятностями
г^т rvn—m
пгу \ М ' ^N-M	/0 оп\
рт = Р{Х = т) =	—	,	(2.29)
где т = 0,1,..., min(n, М), М ^ N, т ^ n, п ^ N; n, М, N — нату¬
ральные числа.
Гипергеометрическое распределение возникает в случаях, подоб¬
ных следующему: в урне N шаров, из них М белых, а остальные чер¬
ные; из нее вынимается п шаров. Требуется найти вероятность того,
что среди извлеченных шаров будет ровно т белых (остальные чер¬
ные). Случайная величина X — число белых шаров среди извлеченных
из урны.
В п. 1.7 разобран пример 1.6 подобного типа.
Математические ожидания д. с. в. X, имеющей гипергеометриче¬
ское распределение, есть
М1 = п-|,	(2.30)
а ее дисперсия
M=n^yv-^-n)	(2М)
Гипергеометрическое распределение определяется тремя параметрами
N, М, п. Если п мало по сравнению с N (практически при п < JqN),
он приближается к биномиальному распределению с параметрами п и
М N—M ^ ^ут ш т п-т \
Сп	Р Я у
Гипергеометрическое распределение используется при решении задач,
связанных с контролем качества продукции (и других).
Пример 2.10. В группе из 21 студентов 5 девушек. Из этой группы
наудачу отбирается 3 студента. Составить закон распределения д. с. в.
X — числа девушек из отобранных студентов. Найти MX.
о	С. в. X принимает значения ОЛ.2,3. Вероятности этих значений
Сг •
находим по формуле (2.29): ро = Р{Х = 0} =	5	16-	«	0,4211,
^21

Глава 2. Случайные величины ■ 91
Р! = Р{Х - 1} =	«	0,4511,	Р2	=	Р{Х = 2} = ~^6 « 0.1203.
^21 21
(73 • (7°
Рз = Р{X = 3} =	5	3	16	«	0,0075.	Ряд	распределения:
^21
X =.771
0
1
2
3
Pm
0,4211
0.4511
0,1203
0,0075
Значение MX найдем двумя способами: а)по ряду распределения:
MX = 0 • 0,4211 + 1 • 0,4511 -I- 2 - 0,1203 + 3 • 0,0075 = 0,7142; б)по форму-
ле (2.30) MX = 3 • ^ = | « 0,7142.	•
Равномерный закон распределения
Непрерывная с. в. X имеет равномерное распределение на отрезке
[а, 6], если ее плотность верюятности f(x) постоянна на этом отрезке, а
вне его равна нулю:
tbh-
\о.
/(*) = <»-«’ "Р«*6М'	,2.32)
при х$[а,Ь],
(т. е. f(x) = с при х €Е [а, 6], но
+00
J cdx = 1,
—ОО
> 1
= 1, с = 		; вместо отрезка [а,Ь] можно
о — а
ь
отсюда следует, что сх
писать (а,Ь) или (а,Ь], [а,Ь), так как с. в. X — непрерывна.)
График плотности f(x) для равномерного распределения н. с. в. X
изображен на рис. 28.
Равномерное распределение с. в. X на участке [а, Ь] (или (а, Ь)) бу¬
дем обозначать: X ~ Д[а,Ь].
Найдем функцию распределения F(x) для X — R[a,b], Согласно
формуле (см. п. 2.4)
F(x)= J

92 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
f{x)
1
b — а
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
0
а Ъ х
Рис. 28
имеем
X
F(x) = [	=	j-*-
J b — а о — а
— х ~ а
a b — а
при а < х ^ b; F(rz:) = 0 при х ^ а, и
а	Ь	х
^) = /ол + /^ + /ол = ^
= 1
при х >Ъ. Таким образом,
0,	при а; ^ а,
F(:r) = I ^_7д ^ ПРИ а < х ^
1,	при Ъ < х.
График F(^) изображен на рис. 29.
Рис. #0
Определим MX и DX с. в. X ~ JS[a, Ь].
Согласно формуле (2.11),
Ч-оо
+ 6
(2.33)
MX = J х -Odx + J j^dx + J x-0dx = -
(Ожидаемый результат: математическое ожидание с. в. X ~ R[a, b\
равно абсциссе середины отрезка; MX совпадает с медианой, т. е.
MX = МеХ.)

Глава 2. Случайные величины ■ 93
Согласно формуле (2.14),
ь
М = /(х-^)2.^ = ^.1(х-^)3[=
а
1	((Ь-а)3	(а-Ь)3\_ (6-а)2
3(6 - о) V 8	8	у	12	'
Таким образом, для н. с. в. X ~ R[a, Ь] имеем
MX =	DX	=	—~2а~.	(2.34)
\
Пример 2.11. Пусть с. в. X ~ Д(а,Ь). Найти вероятность попадания
с. в. X в интервал (о, /3), принадлежащий целиком интервалу (а, Ь).
О Согласно формуле (2.8), имеем
&	Р
Р{X € (а,/8)} = J f(x)dx =	dx	=
а	а
т.е. Р{1б(а^)} = ^.
Геометрически эта вероятность представляет собой площадь пря¬
моугольника, заштрихованного на рис. 30.	•
К случайным величинам, имеющим равномерное распределение,
относятся: время ожидания пассажиром транспорта, курсирующего с
определенным интервалом; ошибка округления числа до целого (она
равномерно распределена на отрезке [—0,5; 0,5]). И вообще случайные
величины, о которых известно, что все ее значения лежат внутри не¬
которого интервала и все они имеют одинаковую вероятность (плот¬
ность).

94 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Дискрет,пая случайная величина X имеет равномерное распреде¬
ление, если она принимает целочисленные значения 1, 2, 3, п с
вероятностью рт — Р{Х = т\ — где т = 1,2,3,..., ?г.
п2 — 1
В этом случае MX =
rv
1	+ п
DX =
Так, при п = 5,
2	' 12
многоугольник распределения имеет вид, представленный на рис. 31.
MX = 3.
V
0,2
0
1 2 3 4 5 *
Рис. 31
Показательный закон распределения
Непрерывная случайная величина X имеет показательный (или
экспоненциальный) закон распределения, если ее плотность вероятно¬
сти имеет вид
' \е~Хх, при х > О,
при х < 0.
я*)
fAe
где Л > 0 — параметр распределения.
График плотности f(x) приведен на рис. 32.
(2.35)
Функция распределения показательного распределения имеет вид
F (х)
-G;
е Л:г, при х ^ U,
при х < 0.
(2.36)

Глава 2. Случайные величины ■ 95
х	их
Q F(x) = J f(t)dt= J Odt + J Xe
~Xx dt = 1 - e~A*.
График F(x) представлен на рис. 33.
Найдем математическое ожидание и дисперсию показательного
распределения:
оо	b
MX = J х • \е~Хх dx = ^lim J х • \е~Хх dx = [интегрируем по частям] =
= lim ( —
6—ЮС у
о
а; • е АН —• е~Хх
о А
DX = J x2f(x) dx — (MX)2 - [формула (2.17)] = A Jх2е~Хх dx— =
— ОО	О
= [дважды интегрируем по частям] —
-	* (& (-£-*■+? {-г*- - зК*)) 0 ■- *■-
= Л(0+1 (0 + 0_ ЗР(0_1)))	=	^“	=	\2'
Таким образом,
MX = ±, ЯХ =	<х*	=	4-	(2.37)
А2'
1
А’
Найдем вероятность попадания случайной величины X, распределен¬
ной по показательному закону, в интервал (а,Ь). Используя форму¬
лу (2.2) и формулу (2.36), получаем
Р{а < X < Ь} = F(b) - F(a) = (1 - е~хь) - (1 - е“Ао) - е~Ло - е~Л6,
т. е. Р{а < X < Ь} = е_Ла — е~ль.

96 а Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 2.12. Случайная величина Т — время работы радиолампы
имеет показательное распределение. Найти вероятность того, что лам¬
па проработает не менее 800 часов, если среднее время работы радио¬
лампы 400 часов.
о МТ — 400, значит (формула (2.37)), Л = Искомая вероятность
800
Р{Т > 800} = 1 -Р{Т < 800} = 1—F(800) = 1—(1—е“400) = е"2^ 0,135.
•
Показательное распределение используется в приложениях теории
вероятностей, особенно в теории массового обслуживания (ТМО), в фи¬
зике, в теории надежности. Оно используется для описания распреде¬
ления случайной величины вида: длительность работы прибора до пер¬
вого отказа, длительность времени обслуживания в системе массового
обслуживания и т.д.
Рассмотрим, например, н.с. в. Т — длительность безотказной ра¬
боты прибора. Функция распределения с. в. Г, т. е. F(t) = Р{Т < t},
определяет вероятность отказа за время длительностью t. И, значит,
вероятность безотказной работы за время t равна R(t) = Р{Т > t} =
= 1 — F(t). Функция R(t) называется функцией надежности.
Случайная величина Т часто имеет показательное распределение.
Ее функция распределения имеет вид F(t) = 1 — e~xt (формула (2.36)).
В этом случае функция надежности имеет вид R(t) = 1 — F(t) =
= 1 — (1—e~xt) = е-Л*, т. е. R(t) = е~л*, где А — интенсивность отказов,
т. е. среднее число отказов в единицу времени.
Показательный закон — единственный из законов распределения,
который обладает свойством «отсутствия последствия» (т. е. если про¬
межуток времени Т уже длился некоторое время г, то показатель¬
ный закон распределения остается таким же и для оставшейся части
Т\ = Т — г промежутка).
Нормальный закон распределения
Нормальный закон («закон Гаусса») играет исключительную роль
в теории вероятностей. Главная особенность закона Гаусса состоит в
том, что он является предельным законом, к которому приближаются,
при определенных условиях, другие законы распределения. Нормаль¬
ный закон наиболее часто встречается на практике.

Глава 2. Случайные величины ■ 97
Непрерывная с. в. X распределена по нормальному закону с пара¬
метрами а и а > 0, если ее плотность распределения имеет вид
(х —
fix) =	е__2о^ , ж € .К.	(2.38)
<7 • V27T
Тот факт, что с. в. X имеет нормальное (или гауссовское) распреде¬
ление с параметрами а и <т, сокращенно записывается так: X ~ N(a, о).
Убедимся, что f(x) — это функция плотности. Очевидно, f(x) > 0.
оо
Проверим выполнение условия нормировки J f(x)dx = 1. Имеем:
—оо
J о • у/27г	а - у/27г J	\л/2-сг/
—оо	—оо
оо
= -i= [ e-t2dt=-±- yfr= 1.
\Дг J	у/7Г
—оо
Здесь применили подстановку и использовали «интеграл Пуассона»
оо
J e~l2dt = у/тг.	(2.39)
—оо
Из равенства (2.39) следует:
оо	оо	0
Jdt =	J е~~%	dz=yj^= J	еГ 2 <£г.	(2.40)
0	0	—ОО
Я
Функция распределения F(x) = J /(f) dt н. с. в. X ~ N(a, ст)
имеет
—ОО
ВИД
*	(t-a)2
F(x) =	Ц= / е 2<r2 dt.	(2.41)
<7 - \/27Г J
—oo
Если о = 0 и fj = 1, то нормальное распределение с такими параме¬
трами называется стандартным. Плотность стандартной случайной
величины имеет вид
1
</?(ж) = е~Т.
V 27Г

98 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
С ней мы уже встречались в п. 1.21, формула (1.37).
Функция распределения с. в. X ~ N(0,1) имеет вид
Х 2
Ф(ж) = ■ \	 I е~ 2 dt
\/Ък J
—ОО
и называется, как мы уже знаем (п. 1.21, формула (1.42)), функци¬
ей Лапласа. Она связана с нормированной функцией Лапласа Фо(ж)
(п. 1.21, формула (1.40)), как мы уже знаем (формула (1.43) п. 1.21),
равенством
Ф(х) = 0,5 + Фо{х).	(2.42)
Действительно,
х	0	2	х	2
Ф(ж) = -4= / еГ2 dt =	е	^ dt + —L= / е~Т dt =
х/2тг J	л/2^	J	v/2tt	J
—оо	—оо	0
= vfe' V5 +	=	5	+	*»<*>•
(см. (2.40)).
Установим смысл параметров а и а нормального распределения.
Для этого найдем математическое ожидание и дисперсию с. в. X ~
~ N(a,a).
7	7	(т~п)2
MX = I х ■ f(x) dx =	— I x ■ в 2<т2 dx =
J V a ■ V2-K J
—oo	—oo
oo
= подстановка ~ ~ = t =		 	 I (\/2at + a)e~t2 V2a dt
[	s/2o	J	a	• y/2ir J
-OO
OO
—	a^. f ie f2 (if _|_ f e t2 fa — 0 4-	•	v/7r = a,
V?r J	V7T	J	фr
— oo	—oo
т. e. MX = а. Первый интеграл равен нулю, так как подинтегральная
функция нечетная, а пределы интегрирования симметричны относи¬
тельно нуля, а второй интеграл равен yJH (см. равенство (2.39)). Таким
образом, параметр а — математическое ожидание.
При нахождении дисперсии с. в. X ~ N(a, а) снова сделаем подста-
х — а,
новку	—	t	и	применим	метод	интегрирования	по	частям:
V 2 а

Глава 2. Случайные величины ■ 99
DX — f (х — a)2f{x) dx = f (х — а)2—7= е 2<т2 dx =
J	J	о у/1
(х — а)2
,— *
ту/2п
ОС	оо
= -±= [ 2a2t2e-t2aV2dt = ^ ( t2e~{2 dt =
ау/Ъх J	у/к	J
—00	—00
—ос
Таким образом, Z?X = сг2, а сг — Сиднее квадратичное отклонение.
Можно показать, что для с. в. X ~ N(a,cr): MqX — МеХ = а.
л = ^ = о, я = ^ - з = о.
сг4*	сг
Исследуем дифференциальную функцию (2.38) нормального за¬
кона:
1.	/(гг) > 0 при любом х £ (—ос, оо); график функции расположен
выше оси Ох,
2.	Ось Ох служит асимптотой графика функции /(ж), так как
lim f(x) = U.
х—>±оо
3.	Функция f(x) имеет один максимум при х = а, равный /(а) =
—	^	=.	Действительно,
ту/Ъх
(ж — а)2
/'(ж) =	“1е~	2	^
(а; — а) -
Отсюда /'(ж) = 0 при х = а, при этом: если ж < а, то /;(а:) > 0, a
если х > а, то /'(ж) < 0. Это и означает, что х — а точка максимума
и /max = / (а) =	1
а у/2тх
4.	График функции f(x) симметричен относительно прямой х — а,
так как аналитическое выражение f(x) содержит разность х — а в
квадрате.
5.	Можно убедиться, что точки
Mi
(а —	сг. —е 2 ]	и М2 ( а	+ сг, —в 2 ]
V ау/Ъх	)	\	а у/Ъх	)
являются точками перегиба графика функции f(x).
Пользуясь результатами исследования, строим график плотности
расщюделения вероятности нормального закона — кривую распреде¬
ления, называемую нормальной кривой, или кривой Гаусса (рис. 34).

100 * Раздел первый. Элементарная теория вероятностей и случайных процессов
Как влияет изменение параметров а и а на форму кривой Гаус¬
са? Очевидно, что изменение а не изменяет форму нормальной кривой
(графики функции /(гг) и f(x — а) имеют одинаковую форму; график
f(x —а) получается из графика функции f(x) путем сдвига последнего
на а единиц вправо, если а > 0, и влево, если а < 0). С изменением о
максимальная ордината точки кривой изменяется. Так как площадь,
ограниченная кривой распределения, равна единице при любом значе¬
нии <т, то с возрастанием о кривая Гаусса становится более пологой,
растягивается вдоль оси Ох	
На рис. 35 изображены нормальные кривые при различных значе¬
ниях о (<т 1 < о < (72) и некотором значении а (одинаковом для всех
трех кривых).
Нормальному закону подчиняются ошибки измерений, величины
износа деталей в механизмах, рост человека, ошибки стрельбы, вес
клубней картофеля, величина шума в радиоприемном устройстве, ко¬
лебания курса акций и т. д.

Глава 2. Случайные величины ■ 101
Найдем вероятность попадания с.в.Х~ N(a,cr) на заданный уча¬
сток (а,/3). Как было показано,
Р{а<Х <
ь
Ь}= J f(x)dx
(п. 2.4, формула (2.8)). В случае нормального распределения
£ (х - о)2
Р{а<Х </3} =	е	2<т2	dx	=	=	t	=
ау 27г J	J
а
Р - а	Р	—	а	а	-	а
= _J= /	/	e-4dt-^= [ е-?Л.
\/2тг J	л/2тг J	л/2тг J
а — а	0	0
ег
Используя функцию Лапласа (см. п. 1.21, формула (1-40))
Ф0(я) = -р= / е~у
у27Г J
о
получаем
Р{« < X < £} = Ф0	-	Фо (^) •	(2.43)
Через функцию Лапласа Фо(^) выражается и функция распределения
F(;r) нормально распределенной с. в. X.
}	1	Jlz^L
F(x) = / -4= е 2сг2 d* = Р{-оо < X < х} =
J <7 V 27Г
—оо
= Фо (^) - Ф„ (=^) = Ф„ (£^5) + 1,
т. е.
Р(х) = ± + ф0(^).	(2.44)
Здесь воспользуемся формулой (2.43), нечетностью функции Фо(х) и
тем, что Ф0(оо) = действительно

102 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Если функция Лапласа (или «интеграл вероятностей») есть
yfbi J
—оо
ТО
F(x) = Ф ( V1)
(2.48)
(непосредственно вытекает из равенств (2.42)) и (2.44)).
Равенство (2.43) можно переписать и так:
Р{а < X < 0} = Ф	-	Ф	(V2)	•
(2.46)
На практике часто приходиться вычислять вероятность попада¬
ния нормально распределенной случайной величины в интервал, сим¬
метричный относительно центра рассеяния а. Пусть таким интер¬
валом будет (а — 1,а 4- /) длины 21. Тогда Р{ а -1<Х<а + !} =
= Р{\Х - а| < /} = Ф0 ('а + 1а а) - Фо (- I а) = 2Фо (i), т.е.
Полагая в равенстве (2.47) / = За. получим Р{\Х — а\ < 3а} = 2Ф0(3).
По таблице значений для Фо(#) находим: Фо(3) = 0.49865. Следова¬
тельно, Р{\Х — а\ < Зет} 0.9973, т. е. отклонение с. в. X от своего
математического ожидания меньше, чем 3о — почти достоверное собы¬
тие.
Важный вывод: практически достоверно, что с. в. X ~ N(a, о) при¬
нимает свои значения в промежутке (а — Зет, а 4 За). Это утверждение
называется «правилом трех сигм».
Пример 2.13. При измерении детали получаются случайные ошибки,
подчиненные нормальному закону с параметром о — 10 мм. Произво¬
дится 3 независимых измерения детали. Найти вероятность того, что
ошибка хотя бы одного измерения не превосходит по модулю 2 мм.
о	По формуле (2.47) находим:
Вероятность того, что эта ошибка (погрешность) превышает 2 мм в
одном опыте (измерении), равна
Р{\Х - а\ < 1} = 2Ф0 (|) = 2Ф (I) - 1.	(2.47)
Р{\Х - а\ < 2} = 2Ф0 и 2 • 0,07926 = 0,15852.
Р{\Х - а\ > 2} = 1 - Р{\Х - а\ < 2} = 0,84148.

Глава 2. Случайные величины "103
По теореме умножения вероятность того, что во всех трех опытах ошиб¬
ка измерения превышает 2 мм. равна 0,841483 ~ 0,5958. Следовательно,
искомая вероятность равна 1 — 0,5958 = 0,4042.	•
Упражнения
1	- Известно, что с. в. X ~ N(3,2). Найти
Р{-3 < X < 5}, Р{Х < 4}, Р{\Х - 3| < 6}.
2- Нормально распределенная с. в. X задана плотностью вероятно¬
стей
Найти: а) вероятность попадания с. в. в интервал (1,3); б) симмет¬
ричный относительно математического ожидания интервал, в ко¬
торый с вероятностью 0,8926 попадет с. в. X в результате опыта;
в) ЛIqX и МеХ. Построить нормальную кривую f(x).
3.	Найти коэффициенты асимметрии и эксцесса нормального распре¬
деления с параметром а = 0.

Глава 3
Системы случайных величин
&
3.1. Понятие о системе случайных величин и законе
ее распределения
При изучении случайных явлений часто приходится иметь дело с
двумя, тремя и большим числом случайных величин. Совместное рас¬
смотрение нескольких случайных величин приводит к системам слу¬
чайных величин. Так, точка попадания снаряда характеризуется си¬
стемой (X, У) двух случайных величин: абсциссой X и ординатой У;
успеваемость наудачу взятого абитуриента характеризуется системой
п случайных величин (Xi, Х2,.. •, Хп) — оценками, проставленными в
его аттестате зрелости.
Упорядоченный набор (Xi,Х2,..., Хп) случайных величин Xi (г =
= 1,п), заданных на одном и том же ПЭС ft, называется п-мерной
случайной величиной или системой п случайных величин.
Одномерные с. в. Xi,X2l. • • ,Хп называются компонентами или
составляющими n-мерной с. в. (Х\, Х2,. • -, Хп). Их удобно рассма¬
тривать как координаты случайной точки или случайною вектора
X = (Х\.Х2, • •., Хп) в пространстве п измерений.
На многомерные случайные величины распространяются почти без
изменений основные понятия и определения, относящиеся к одномер¬
ным случайным величинам. Ограничимся для простоты рассмотрени¬
ем системы двух случайных величин; основные понятия обобщаются
на случай большего числа компонент.
Упорядоченная пара (X, У) двух случайных величин X и У назы¬
вается двумерной случайной величиной или системой двух одномерных
случайных величин X и У.
Систему (X, У) можно изобразить случайной точкой М(Х,У) или
случайным вектором ОМ (рис. 36 и 37).
Система (X, У) есть функция элементарного события: (X, У) =
= <p(w). Каждому элементарному событию w ставится в соответствие
два действительных числа х и у (или х\ и #2) — значения X и У (или
Х\ и Х2) в данном опыте. В этом случае вектор х = (жь^г) называется
реализацией случайного вектора X — (Х\,Х2).

Глава 3. Системы случайных величин ■ 105
M(X,Y)
Пример 3.1. Бросаются две игральные кости. Пусть с. в. X — число
выпавших очков на первой кости, с. в. Y — на второй; ПЭС состоит
из 36 элементов: Q = {(1,1), (1,2),... , (L 6), (2,1), (2,2),... , (6,4), (6,5),
(6,6)}. Элементарному событию, например, (6,5) = соответствует
пара чисел х = 6 и у = 5. Совокупность этих значений — функция
элементарного события w.
Системы случайных величин могут быть дискретными, непрерыв¬
ными и смешанными в зависимости от типа случайных величин, обра¬
зующих систему. В первом случае компоненты этих случайных систем
дискретны, во втором — непрерывны, в третьем — разных типов.
Полной характеристикой системы (X, Y) является ее закон рас¬
пределения вероятностей, указывающий область возможных значе¬
ний системы случайных величин и вероятности этих значений. Как
и для отдельных случайных величин закон распределения системы
может иметь разные формы (таблица, функция распределения, плот¬
ность, ...).
Так, закон распределения дискретной двумерной с. в. (X. Y) можно
задать формулой
Pij = р{х - xh Y = Уjb	* = 1, п, j = l,m
или в форме таблицы с двойным входом:
(3.1)
X\Y
У\
У2
Уз
Ут
Х\
Рп
Р\2
Р\з
Plm
Х2
Р21
Р22
Р23
Р2т
Рп1
Рп2
РпЗ
Рпт
Причем, сумма всех вероятностей рц, как сумма вероятностей полной
группы несовместных событий {X = х^ Y = у^}, равна единице:
п т
ЁЕру = L
*= 1 з=1

106 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
На рис. 38 приведен примерный график распределения двумерной
дискретной случайной величины (X, Y).
Зная закон распределения двумерной дискретной случайной ве¬
личины, можно найти законы распределения каждой из компонент
(обратное, вообще говоря, неверно). Так, рХ] = Р\Х = Xi} — рц
+ Р\2 + • * * + Pim: что следует из теоремы сложения несовместных со-
бытий {X - Y = */i}, {X = XI.Y = */2}- {X = XI, Y = ут}.
Аналогично можно найти
т	п
Рхг — Р{Х — .Тг} = ^^Piji Pyj — P{Y — y.j} — Pij •
j-1	i-1
Пример 3.2. В урне 4 шара: 2 белых, I черный, 1 синий. Из нее наудачу
извлекают два шара. Пусть с. в. X — число черных шаров в выборке,
с. в. Y — число синих шаров в выборке. Составить закон распределения
для системы (X, Y). Найти законы распределения X и Y.
О С. в. X может принимать значения 0, 1; с. в. Y — значения 0, 1.
Вычислим соответствующие вероятности: рц = Р{Х = 0, Y = 0} =
£2 = 6 (или: 4	3 = 6^’ рп = Г^х = °,F = ^ =	= 6’
Р21 = Р{Х = 1,г = 0} = |; Р22 = Р{X = 1,у = 1} = i Таблица
распределения системы (X, Y) имеет вид:
X\Y
0
1
0
1
2
б
6
1
2
1
1
б
6

Глава 3. Системы случайных величин "107
Отсюда следует: Р{Х = 0} = | + | = Р{Х = 1} = | + ^
12 1 2 11
Р{У = 0} = g + g = 2? P{Y — 1} =	=	2'	Законы	распределения
составляющих X и Y имеют вид:
0
1
1
1
р
2
2
и
У
0
1
1
1
Р
2
2
3.2.	Функция распределения двумерной случайной
величины и ее свойства
Универсальной формой задания распределения двумерной случай¬
ной величины является функция распределения (или «интегральная
функция»), пригодная как для дискретной, так и для непрерывной слу¬
чайной величины, обозначаемая Р\,у(х,у) или просто F(x,y).
РЭ	Функцией	распределения	двумерной	случайной	величины	(X. У) на¬
зывается функция F(iг, ?/), которая для любых действительных чисел х
и у равна вероятности совместного выполнения двух событий {X < х}
И {Y < у).
Таким образом, по определению
F(x,y) = P{X <x,Y <у}	(3.2)
(событие {X < х, У < у} означает произведение событий {X < о:} и
{У < ?;})•
Геометрически функция F(x< у) интерпретируется как вероятность
попадания случайной точки (X, У) в бесконечный квадрант с вершиной
в точке (х, ?у), лежащий левее и ниже ее (рис. 39).
Функция распределения двумерной дискретной случайной величи¬
ны (X, У) находится суммированием всех вероятностей piдля кото¬
рых Х{ < ж, ijj < ?/, т. е.
pfay) = 5Z ру-	(3-3)
Xi<xy3<y
Геометрическая интерпретация функции распределения позволяет
наглядно иллюстрировать ее свойства.

108 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Свойства функции распределения двумерной случайной
величины:
1.	Функция распределения F(x,y) ограничена, т.е.
0	< F(x, у) ^ 1.
2.	F(x,y) не убывает по каждому из своих аргументов при фиксиро-
ванном другом, т. е.
F(x,2,y) ^ F(xi,y) при х'2 > XI;
F(x,yi) ^ F(x,yi) при У2>У1-
3.	Если хотя бы один из аргументов обращается в —оо, то функция
распределения F(x,y) равна нулю, т.е.
F( гг, —оо) = F(—оо, у) = F(—оо, —оо) = 0.
4.	Если оба аргумента обращаются в +оо, то F(x^y) равна 1, т.е.
F(+oo, +оо) = 1.
5.	Если один из аргументов обращается в +оо, то функция распреде¬
ления системы случайных величин становится функцией распре¬
деления с. в., соответствующей другому элементу, т. е.
F(x, +оо) = Fi(x) = Fx(x), F(+oo,y) = F2(y) = Fr(y).	(3.4)
6.	F(x,y) непрерывна слева по каждому из своих аргументов, т.е.
lim ^F(x,y) = F(x0,y), lim F{x,y) = F(x,y0).
X—>Xq—0	y—*yo — {j

Глава 3. Системы случайных величин ■ 109
Q 1. F(x, у) есть вероятность, следовательно, 0 ^ F(x,y) ^ 1.
2.	При увеличении какого-либо из аргументов (х, у) заштрихован¬
ная на рис. 39 область увеличивается; значит, вероятность попадания
в нее случайной точки (Х7 У) не может уменьшаться.
3.	События {X < —оо}, {У < —оо} и их произведение невозмож¬
ны: попадание в квадрант с отодвинутой в —оо границей невозможно.
Вероятность такого события равна нулю.
4.	Событие {X < -hoo} * {У < -Ьоо} достоверно, следовательно, его
вероятность равна единице.
5.	{X < +оо} — достоверное событие, следовательно {X < +оо}х
х{У < у\ - {У < у} и F(+oo,j/) - Р{Х < +оо;У < у} = Р{У < у} =
= FY{y).
Аналогично, F(x, -Ьоо) = РдЧ#)-	■
Подчеркнем: зная совместное распределение двух случайных вели¬
чин X и У, можно найти одномерные распределения этих случайных
величин, но обратное, вообще говоря, неверно.
Отметим, что с геометрической точки зрения F(x, у) есть некото¬
рая поверхность (ступенчатая для двумерной дискретной случайной
величины), обладающая указанными свойствами.
С помощью функции F(x.y) легко можно найти вероятность по¬
падания случайной точки (X, У) в прямоугольник D со сторонами, па¬
раллельными координатным осям:
P{xi < X ^ Х2,У\ ^ У < у2} =
= F(x2,35/2) - F(xi,y2) - F(x2,yi) + F(x\,yi). (3.5)
Приведем «геометрическое доказательство», см. рис. 40.

110" Раздел первый. Элементарная теория вероятностей и случайных процессов
Здесь F(x2,y2) — вероятность попадания случайной точки в
область, заштрихованную косыми линиями, F(x\,y2) — вертикальны¬
ми. F(x2, у\) — горизонтальными, F(x,\, у\) — косыми, вертикальными,
горизонтальными (эту область дважды вычли, следует один раз при¬
бавить).
Пример 3.3. По таблицам распределения системы (X,Y) компонент
X и Y примера 3.2 п. 3.1 найти F\(x), F2(y), F(x,y).
О Используя формулу (2.4), находим функции распределения F\(x)
И F2{y):
0,	при х	^ 0,
0.5,	при 0	< х ^	1,	F2(y)	=
1,	при х	> 1,
Fi(x)=<
Используя формулу (3.3), находим функцию распределения F(x,y):
'0, при у ^ 0,
0.5,	при 0 < у ^ 1,
1.	при^>1.
X\Y
2/^0
0 < у < 1
1 < у
х ^ 0
0
0
0
0 < х ^ 1
0
1
6
И-И)
1 < х
0
1 (= 1 + 2)
2 \ 6 + 6)
1 (=| + 1 + | + б)
3.3.	Плотность распределения вероятностей
двумерной случайной величины и ее свойства
Исчерпывающей характеристикой непрерывной двумерной случай¬
ной величины является плотность вероятности. Вводится это понятие
аналогично тому, как это делалось при рассмотрении плотности рас¬
пределения вероятностей одной случайной величины (п. 2.4).
ф	Двумерная случайная величина называется непрерывной, если ее
функция распределения F(x*y) есть непрерывная функция, диффе¬
ренцируемая по каждому из аргументов, у которой существует вторая
смешанная производная F"y(x*y).
Рч]	Плотностью распределения вероятностей (или совместной плот¬
ностью) непрерывной двумерной случайной величины (А", У) называ¬
ется вторая смешанная производная ее функции распределения.

Глава 3. Системы случайных величин ■ 111
Обозначается совместная плотность системы двух непрерывных
случайных величин (X, У) через /(#, у) (или р(х,у)).
Таким образом, по определению
d2F{x,y) „
Пх'у) = дх&у =
(3.6)
Плотность распределения вероятностей непрерывной двумерной слу¬
чайной величины (X, Y) есть предел отношения вероятности попадания
случайной точки (X, F) в элементарный прямоугольник со сторонами
Ах и Ду, примыкающий к точке (ж, у), к площади этого прямоуголь¬
ника. когда его размеры Ах и Ау стремятся к нулю (рис. 41).
Действительно, используя формулу (3.5), получаем: средняя плот¬
ность вероятности в данном прямоугольнике равна
/ср —
Р{х ^ X < х 4- Ах, у ^ У < у 4- Ау}
Ау ‘ (
Ах • Ау
F(x + Ах, у 4- Ay) — F(x. у + Ay)	F(x + Ах, у) - F(x, у)
Ах	Ах
Переходя к пределу при Ах —> 0 и Ау 0, получаем
й)
*	,•	•р,(^5	У	+	&у)	-	J/)
lim Тс р = 11П1 	
Ах—>0	А?/-+0
Ау
т.е. f{x,y) = (F'x(x,ij))'y = F%y(x,y).
По аналогии с плотностью вероятности одномерной непрерывной
случайной величины, для двумерной случайной величины (X, F) плот¬
ность вероятности определяется как функция /(#, у), удовлетворяю¬
щая условию
Р{х ^ X < х + dx, у < У < у + Ау} ~ /(ж, у) dxdy:	(3.7)
выражение f(x, у) dxdy называется элементом вероятности двумер¬
ной случайной величины (X, F).

112" Раздел первый. Элементарная теория вероятностей и случайных процессов
Геометрически плотность распределения вероятностей f(x,y) си¬
стемы двух случайных величин (X, Y) представляет собой некоторую
поверхность, называемую поверхностью распределения (рис. 42).
Плотность распределения f(x,y) обладает следующими
свойствами:
1.	Плотность распределения двумерной случайной величины неотри¬
цательна, т. е.
f(x, у) ^ 0.
2.	Вероятность попадания случайной точки (X, У) в область D равна
двойному интегралу от плотности по области D. т. е.
Р{(Х,К)6£>} = //'< x,y)dxdy.	(3.8)
D
3.	Функция распределения двумерной случайной величины может
быть выражена через ее плотность распределения по формуле:
х у
F(x, у) = // /л\у (^iv) dudv.	(3.9)
4.	Условие нормировки: двойной несобственный интеграл в бесконеч¬
ных пределах от плотности вероятности двумерной с. в. равен еди¬
нице, т. е.
LAJ
J J f(x,y)dxdy = 1.

Глава 3. Системы случайных величин ■ 113
5.	Плотности распределения одномерных составляющих X и Y могут
быть найдены по формулам:
+оо	+оо
J f(x,y)dy = fi{x) = fx(x), j f(x,y)dx = f2(y) = fv(y)- (3.10)
—oc	—оо
Ql 1. Следует из того, что F(x,y) есть неубывающая функция по каж¬
дому из аргументов.
2.	Элемент вероятности f(x,y) dxdy (см. (3.7)) представляет собой
вероятность попадания случайной точки (X, Y) в прямоугольник со
сторонами dx и dy (с точностью до бесконечно малых более высокого
порядка по сравнению с произведением dxdy). Разбив область D на пря¬
моугольники и применив к каждому из них равенство (3.7), получаем,
по теореме сложения вероятностей, при стремлении к нулю площадей
прямоугольников (т. е. dx —> 0 и dy 0), формулу (3.8). Геометрически
эта вероятность изображается объемом цилиндрического тела, ограни¬
ченного сверху поверхностью распределения /(.т, у) и опирающегося на
область D.
3.	Выражение функции распределения F(x,y) системы случайных
величин {ХЛГ) через плотность f(x,y) получим, используя форму¬
лу (3.8) (область D есть прямоугольник, ограниченный абсциссами
—оо, х и ординатами — оо, у):
F{x-> у) — Р{Х < x,Y < у} = Р{—оо < X < х, — оо < Y < у} =
х у
41 f(u,v) dudv.
—оо —оо
4.	Положив в формуле (3.9) х = у = +оо и учитывая, что
F(-foo,+оо) = 1, получим
ОО X)
F(+oo,+oo) = J j f(x,y)dxdy = 1.
—оо —оо
Геометрически свойство 4 означает, что объем тела, ограниченного по¬
верхностью распределения и плоскостью Оху, равен единице.
5.	Найдем сначала функции распределения (зная совместную плот¬
ность двумерной случайной величины (Jf, F)), составляющих X и Y:
1’ +ОС
F\ (х) = F(x,+ оо) -	//	/(w,	y)dudy,

114 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
+оо у
Щу) = F(+oo,y) - II f(x,v) dxdv,
—оо —оо
т. е.
х	/ -foo	\	у	/ -foo	\
Fi(x) = II f(u,y)dy\du, F2(y) = II f(x, v) dx J dv.
—oo	\—oo	/	—oo	Voo	/
(3.11)
Дифференцируя первое равенство по #, а второе по у, получим плот¬
ности распределения случайных величин X и Y:
оо
/г (®) = -Fi(®) = / f(x,y)dy
—ОО
И
оо
/г(у) =	=	J f(x,y)dx.
—	ОО
Отметим, что решение обратной задачи (восстановить закон распреде¬
ления системы (X, У) по известным законам распределения составля¬
ющих системы) в общем случае невозможно.	■
Пример 3.4. Двумерная случайная величина (X, Y) задана плотно¬
стью распределения вероятностей f{x,y) = 		г-. Найти:
(1 + х )(1 + у )
1)	А: 2) F(x,y); 3) Р{Х < 1 ,Г < 1}; 4) /,(*) и /2(у).
О	1) Постоянную А найдем, используя условие нормировки:
оо оо
/ / (l+^Kl+y*)^ =
-оо —оо
оо	оо
f	dx f  
J 1 + х2 J 1
OO	oo
A I I M^2=1’
—oo	—oo
1+00
• arctg у
— oo
A • 7Г2 = 1.
H-oo
= 1,
—oo
Следовательно, Л = Дт.

Глава 3. Системы случайных величин ■ 115
2)	Используя формулу (3.9), находим:
^ -/(/?■ тЫ ТТ? - ?	+Й
—ос \—оо	/
= (iarctgx-t- 1) (:1 агс!.ду + i) .
3)	Р{Х < 1,Y < 1J = F(l.l) = (I.!+>)(!.1 + 1) = £
(использовали формулу (3.2), можно воспользоваться формулой (3.8)).
4)	По формуле (3.10) получаем:
оо
ш = / 5
<1у	1
IOO
•arctgy =
2	(1	+	ж2)(1	+	у2)	7г2(1	+	а;2)	i-ос
—ос
—	1	/	7Г	,	7[\	_	1
7Г2 (1 “h Я'2)	2/	7г(1+Х2)’
ОО
f2(v)= [ -L	dx		=	1		#
1ЛУ} J 7Г2 (1 + гс2)(1 +у2)	тг(1	+	г/)
Упражнения
1	- Двумерная случайная величина (X, К) задана совместной плотно¬
стью распределения
/(*,!/> = 1^"’"”’ “Р"1»0'»»0'
[0;	в	противном случае.
Найти: 1) коэффициент А\ 2) Fx.\ {х, у)\ 3) Fx{x) и Fv (?/); 4) fx{x)
и /> Ы; 5) Р{Х>0,Г<1}.
2- Непрерывная двумерная случайная величина (X, У) равномерно
распределена (т/е. /(ж, у) — с) внутри треугольника с вершина¬
ми 0(0,0), Л(0,4), В(4,0). Найти: 1) совместную плотность /(х*,у);
2)	/л (ж) и fy(y); 3) вероятность события
Л = {0 < X < 1,1 < К < 3}.

116" Раздел первый. Элементарная теория вероятностей и случайных процессов
3.	Вероятность попадания в цель при одном выстреле для I стрел¬
ка равна 0,4, для II — 0,6. Оба стрелка, независимо друг от дру¬
га, делают по два выстрела в цель. Найти: 1) закон распределе¬
ния случайных величин X иУ; 2) совместный закон распределе¬
ния системы случайных величин (X, У); 3) функцию распределе¬
ния JF\',y(x, у), если случайная величина X — число попаданий I
стрелка, случайная величина У — II стрелка.
3.4.	Зависимость и независимость двух случайных
величин
Зная законы распределения случайных величин X и У, входящих
в систему (X, У), можно найти закон распределения системы только в
случае, когда случайные величины X и У — независимы. С понятием
независимости случайных величин мы уже встречались в п. 2.2: две
случайные величины называются независимыми, если закон распреде¬
ления каждой из них не зависит от того, какие значения принимает
другая. В противном случае случайные величины называются зависи¬
мыми.
Теперь дадим общее определение независимости случайных вели¬
чин.
Случайные величины X и У называются независимыми, если не¬
зависимыми являются события {X < х} и {У < у} для любых действи¬
тельных х и у. В противном случае случайные величины называются
зависимыми.
Сформулируем условие независимости случайных величин.
Теорема 3.1. Для того, чтобы случайные величины X и У были незави¬
симы, необходимо и достаточно, чтобы функция распределения системы
(X, У) была равна произведению функций распределения составляющих:
F(x, у) = Fi(x) • F2{y).	(3.12)
□	Если случайные величины X и У независимы, то события {X < х}
и {У < у} независимы. Следовательно, Р{Х < х, У < у} = Р{Х < х}х
xP{Y < у}, т.е. F(x,y) = F\(x) • ^2(1/)- Если же имеет место равен-

Глава 3. Системы случайных величин ■ 117
ство (3.12), то Р{Х < х, У < у} = Р{Х < ж} • Р{У < у). Значит,
случайные величины X и У независимы.	■
Заметим, что условие (3.12) есть иначе записанное условие неза¬
висимости двух событий: Р(А • В) = Р(А) • Р(В) (п. 1.15) для случая
событий А = {X < #}. В — {У < у}.
Теорема 3.2. Необходимым и достаточным условием независимости
двух непрерывных случайных величин, образующих систему (X, К), яв¬
ляется равенство
/И у) = ЛИ • Му)-	(3-13)
□	Если X и У независимые непрерывные случайные величины, то
имеет место равенство (3.12). Дифференцируя это равенство по гс, а
затем по у, получим /(х, у) = ^“^И-или /(ж, у) = ЛИ’ЛЫ-
И обратно. Интегрируя равенство (3.13) по а: и по у, получим
х у	х	у
II	f(u, г?) dudv = / fi(u)du' J f2(v)dv,
т. e. F(x, у) = F\(x) • F2(y). На основании теоремы 3.1 заключаем, что
случайные величины X и У независимы.	■
Теорема 3.3. Необходимым и достаточным условием независимости
двух дискретных случайных величин X и F, образующих систему (X, F),
является равенство
Р{Х =Xi,Y = Уэ} = Р{Х = Хг} • Р{У = Vj}	(3.14)
для любых г = 1,2,..., n, j = 1,2,..., ш
(или, что то же самое: pij = pXi •	,	г	=	1,	n,	j	=	1, m).
Пример 3.5. Зависимы или независимы случайные величины X и F,
рассмотренные в примерах а) 3.2 п. 3.1; б) 3.4 п. 3.3?

118 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
О а) р„ =P{X = 0,Y = 0} = i а Р{Х = 0} ■ P{Y = 0} = \ • \
т. е. рц ф рХ1 • рУх. Случайные величины X и У зависимы.
б) fix,у) =	*	2	,а/х(а;)	=	=	5Г7~Ц-
7Г (1 -h X	У	)	1+Хг	1	+У
Так как f{x,y) = fi{x) • /2(2/), то, согласно условию (3.13), заключаем:
случайные величины X и У независимы.	•
3.5.	Условные законы распределения
Если случайные величины X и У, образующие систему (X, У), за-
висимы между собой, то для характеристики их зависимости вводится
понятие условных законов распределения случайных величин.
|нч|	Условным	законом распределения одной из случайных величин, вхо¬
дящей в систему (X, У), называется ее закон распределения, найден¬
ный при условии, что другая случайная величина приняла определен¬
ное значение (или попала в какой-то интервал).
Пусть (X, У) — дискретная двумерная случайная величина и р^ =
= Р{Х = Xi,Y = yj}, z — 1,n, j — l,m. В соответствии с определени¬
ем условных вероятностей событий (напоминание: Р(£?|Л) =
п. 1.14), условная вероятность того, *гто случайная величина У при¬
мет значение yj при условии, что X ~ Х{ определяется равенством
P{y = VllX = Xi) = P{Xp~*fx^K
i	= 1,2,... ,n; j = 1,2,... ,m (3.15)
Pij
(или коротко: р(ад|жг) = p“)-
Совокупность вероятностей (3.15), т. е. p(yi\xi),p(y2\xi), • • *
• • * ?Р(Ут|#г)? представляет собой условный закон распределения случай¬
ной величины У при условии X ~ х\. Сумма условных вероятностей
p(yj\xi) равна 1, действительно:
т	т	т
Х>№*) = Ей =	= й = 1
i=i	j=i	j=i
т
(см. п. 3.1: Ра* = E^j)-
j=i
Р(АВ)
Р(Л) ’

Глава 3. Системы случайных величин ■ 119
ч
Аналогично определяются условная вероятность, условный закон
распределения случайной величины X при условии Y = yf
, Р{Х = Xi,Y = уЛ
г = 1,2	?г; j = l,2, ...,ш (3.16)
п
(или коротко: р(ж*|ад) = где = У^Рц).
3	? = 1
Пример 3.6. Задана таблица распределения двумерной случайной ве¬
личины (X,Y):
X\Y
1
2
3
0,1
0,12
0,08
0,40
0,2
0,16
0,10
0.14
Найти: а) безусловные законы распределения случайных величин X и
Y; б) условный закон распределения случайной величины X при Y = 2.
ш	п
Q а) Так как pxi =	и	pyj	=	то
г=1
Y
1
2
3
Р
0,28
0,18
0,54
X
0,1
0,2
Р
0,60
0,40
б) С учетом формулы (3.16) имеем: Р{Х = 0,1|F = 2} =
Р{Х = 0,2|У = 2} =	Таким образом, условный закон распре¬
деления случайной величины X при Y = 2 таков
0,1
0,2
Py=2
4
9
5
9
2
= 1). Очевидно несовпадение условного и безусловного законов
г=1
распределения случайной величины X. Следовательно, случайные ве¬
личины X и Y зависимы.	•
Пусть теперь (X^Y) — непрерывная двумерная случайная вели¬
чина с плотностью /(#,?/); f\(x) и /2(2/) — плотности распределения
соответственно случайной величины X и случайной величины Y.

120 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Плотность вероятности условного распределения (или условная
плот.ность) случайной величины Y при условии X = х определяется
равенством
t( I ч /(я-у)	/(я,у)	,	V	,	,
=	=	~Бо	’ где Л 0*0 ^ °-	(3л?)
/ f(x,y)dy
—оо
Условная плотность обладает свойствами плотности распределения,
оо
так: /(у|ж) ^ 0, J f{y\x)dy = 1.
—ОО
Аналогично определяется условная плотность распределения слу¬
чайной величины X при условии Y = у:
,, | ч /(®, у) /(ж, у)
= ~f2(yj =		’ Л(У) ^ 0.	(3.18)
J f{x,v)
dx
Используя соотношения (3.17) и (3.18), можно записать
f(x,y) = h('x) - f(y\x) = /2(у) • /ОФ),	(3.19)
т. е. совместная плотность распределения системы случайных величин
равна произведению плотности одной составляющей на условную плот¬
ность другой составляющей при заданном значении первой.
Равенство (3.19) называют теоремой (правилом) умножения
плотностей распределений (она аналогична теореме умножения ве¬
роятностей ДЛЯ СобыТИЙ, П. 1:15).
Пример 3.7. Двумерная случайная величина (X, У) задана плотно¬
стью совместного распределения
, * _ fCxy, при (ж, у) € D,
\0, при (x,y)<£D,
где D — область на плоскости
У > -х,
У < 2,
ж < 0.
Найти безусловное и условное распределение составляющей X. Убе¬
диться, что случайные величины X и Y зависимы.

Глава 3. Системы случайных величин ■ 121
Q Область D изображена на рис. 43.
Сначала найдем коэффициент С из условия нормировки:
оо оо
// f(x,y)dxdy = 1.
-ОО —ОО
оо эо	0	2	0	^
J J f(xi y)dxdy — J dx J Cxy dy = С J xdx^-^ ^j =
-ОС —OO	—2	—X	—2
= cj*(2-f)dx = c(x>-
= -2 a
-2
Поэтому С = — 2* Теперь находим
оо	2	^
fi{x) = [ J /(®,y)rfyj = J {-^xv)dy = -^rr-Y
т. e. /г (ж) = ^(ж3 — 4ж), ж G (—2,0) (контроль:
оо	0
J Л(x)<fe = J l(a;3-4i)<iT = | (д -2.с'Л
"2= i(-4 + 8) = | = D-
Для нахождения /(ж|у) воспользуемся формулой (3.18), предваритель¬
но найдя /2(у):
оо	0
/г(у) = [ J f(x,y) dxj = J dx =	•	у
0 =у!
4’

122 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
у е (0,2). Тогда f(x\y) =
(контроль:
,,3
оо	О
/ Mv)<b = J (-^)dx = -^4
-оо	-2/
° =--1(0-у2) = 1).
-2/ ?/z
Как видно, безусловный закон распределения случайной величины Л"
(/i(#)) не совпадает с условным законом распределения случайной ве¬
личины X (f(x\y)); случайные величины X и У — зависимы.	•
В случае произвольного типа случайных величин функция распре¬
деления F(x,y) системы зависимых случайных величин (Х,У) может
быть записана в виде:
F(x,y) = F,(x) ■ F2{y\X <х) = F2{y) • ВДУ < у),
где Fz(y\X < х) — Р{У < у\Х < х} — условная функция распреде¬
ления случайной величины У при условии {X < a?}; F\(x\Y < у) -
= р\ X < х\У < у} — условная функция распределения случайной
величины X при условии {У < у}-
3.6.	Числовые характеристики двумерной случайной
величины. Математическое ожидание и дисперсия
Для системы случайных величин также вводятся числовые харак¬
теристики, подобные тем, какие были для одной случайной величины.
В качестве числовых характеристик системы (X, Y) обычно рассмат¬
ривают моменты различных порядков (см. п. 2.5). На практике чаше
всего используются моменты 1 и II порядков: математическое ожидание
(м. о.), дисперсия и корреляционный момент. Математическое ожида¬
ние и дисперсия двумерной случайной величины служат соответствен¬
но средним значением и мерой рассеяния. Корреляционный момент вы¬
ражает меру взаимного влияния случайных величин, входящих в си¬
стему (X, У).
Р£|	Математическим	ожиданием	двумерной	с.	в.	(X,	Y)	называется
совокупность двух м. о. MX и MY, определяемых равенствами:
п т	п т
МХ = тг = ££ Xipij, MY = ту = ^^2 УзРч' (3-2,J)
г-1 j=\	i~ 1 j—1

Глава 3. Системы случайных величин "123
если (X, У) — дискретная система с. в. (здесь = Р{Х — x^Y = yj});
оо оо	оо оо
мх = II х f(x->v) dxdy, MY = // yf(x,y)dxdy,	(3.21)
— OO —OO	—OO —OO
если (X, Y) — непрерывная система с. в. (здесь f{x,y) — плотность
распределения системы).
Дисперсией системы с. в. (X,Y) называется совокупность двух
дисперсий DX и DY, определяемых равенствами:
пт	пт
DX = ^ ^(ж* - mx)2pij, DY = ^ ^2(Уз ~ my)2Pij>	(3-22)
i~ 1 j=]	7—1 j=l
если (X, У) — дискретная система с. в. и
ос оо	ОС оо
DX = J J (x-mx)2f(x,y)dxdy, DY = J J (у - my)2 f(x,y)dxdy,
—	OO —ОС	—OO —oc
(3.23)
если (X, Y) — непрерывная система с. в.
Дисперсии DX и DY характеризуют рассеяние (разброс) случай¬
ной точки (X, Y) в направлении осей Ох и Оу вокруг точки {тх,ту)
на плоскости Оху — центра рассеяния.
Математические ожидания тх и ту являются частными случаями
начального момента a^s порядка k + s системы (X, У), определяемого
равенством
afc,s = M(XkYs);
m* = M(X'Y°) = «1,0 и my = M(X°Yl) = <*>,i-
Дисперсии DX и DY являются частными случаями центрального
момента ц^3 порядка k + s системы (X, У), определяемого равенством
/zfe>s = М{(Х - mx)k{Y - ШуГУ,
DX = М(Х - тх)2 = //2,о и DY = М(У - т,,)2 = цоа.
Математическое ожидание с. в. <р(Х, У), являющейся функцией
компонент X и У двумерной с. в. (X, У) находится, аналогично, по фор¬
мулам:
оо ОС
М(<р(Х, У)) = // <^(ж, у)/(ж, у) dxdy	(3.24)
—	ОО —ОО

124 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
для непрерывного случая и
71 т
М(<р(Х, П) = £ £ v(xi, уМз	(3-25)
i= 1 j-1
для дискретного случая.
Начальный момент II порядка охд = MXY часто встречается в
приложениях. Вычисляется по формуле
п т
MXY — ЕЕ xiVjPij	(3.26)
*=i j=1
для дискретных с. в.
ОО	ОО
MXY = // ХУ /(ж? 2/) dxdy	(3.27)
—оо —оо
для непрерывных с. в.
3.7.	Корреляционный момент, коэффициент
корреляции
Особую роль играет центральный смешанный момент второго по¬
рядка = М(Х—mx)(Y—ту) — MXY, называемый корреляционным
моментом или моментом связи.
Корреляционным моментом (или ковариацией) двух случайных ве¬
личин X и У называется м. о. произведения отклонений эти£ с. в. от их
м. о. и обозначается через Kxy или cov(X, У).
Таким образом, по определению
КХу = cov(X У) = М[{Х - rnx)(Y - my)] = MXY	(3.28)
При этом: если (X, У) — дискретная двумерная с. в., то ковариация
вычисляется по формуле
п т
Kxy = ^2	-	пгх)(у3	~	mu)Pij\	(3-29)
г-\ з=1
если (X, У) — непрерывная двумерная с. в., то
оо	оо
Kxy = J J (ж - тх)(у - my)f(x, у) dxdy	(3.30)

Глава 3. Системы случайных величин ■ 125
(формулы (3.29) и (3.30) получены на основании формул (3.24) и
(3.25)).
Ковариацию часто удобно вычислять по формуле
КХу = cov(X, Y) = MXY - MX • MY,	(3.31)
которая получается из определения (3.28) на основании свойств мате¬
матического ожидания:
Кху = М[(Х — mx)(Y — ту)\ — M(XY — Хту — Ymx + тхту) =
= MXY — гпуМХ — mxMY + тхту = MXY — тхту — тхту + тхту =
= MXY - МХ • MY.
Формулу (3.30) можно записать в виде
ОО ОО
Kxy = // ху fix, у) dxdy - тхту.	(3.32)
—	ОО —ОО
Свойства ковариации:
1.	Ковариация симметрична, т. е.
К ху = Кух-
2.	Дисперсия с. в. есть ковариация ее с самой собой, т. е.
Кхх - DX, Куу = DY.
3.	Если случайные величины X и Y независимы, то
К ху - 0.
4.	Дисперсия суммы (разности) двух случайных величин равна сумме
их дисперсий плюс (мин} , удвоенная ковариация этих случайных
величин, т. е.
D{X ±Y) = DX + DY ± 2КХу.
5.	Постоянный множитель можно вынести за знак ковариаций, т. е.
Ксх,у = сКху = Кх,су или соv(cX,Y) = c-cov(X, Y) = cov(X, cY).
6.	Ковариация не изменится, если к одной из с. в. (или к обоим сразу)
прибавить постоянную, т. е.
Кх+с,у = Kxy = Кх,у+С — ^л+с,у+с

126 а Раздел первый. Элементарная теория вероятностей и случайных процессов
или
cov(X + г\ У) = cov(X, У) = cov(X, У + с) = cov(X + с, У + с).
7. Ковариация двух случайных величин по абсолютной величине не
превосходит их с. к. о., т. е.
|-^Cyv| ^
Q| 1. Следует из определения (3.28) ковариации.
2.	Кхх = М[(Х - тх)(Х - тх)] = М(Х - тх)2 = DX.
3.	Из независимости с. в. Л" и У следует независимость их откло¬
нений X — тх и У — ту. Пользуясь свойствами м. о. (п. 2.5), получаем
КХу = М(Х - тх) • М(У - ту) = 0.
4.	£>(Jf -Ь У) = А/((А' + У) - М(Х + У))2 =
= M({X-MX)+(Y-MY)f = M(X-MX)2+2M(X-MX)(Y-MY)+
+ M(Y - MY)2 = DX + DY + 2KXy,
D(X — Y) = DX + D(-Y) + 2M(X - MX)(-Y - M(-Y)) =
= DX + DY — 2 KXy-
5.	#ca',v = M(cX - McX)(Y - MY) = M[c(X - MX)(Y - MY)] =
= cKxv-
6.	Доказывается аналогично.
^ гг	„	X - mx Y - my
7.	Применяя свойство 4 к двум стандартным с. в. ——	и ——	
(см. п. 2.5), получаем:
В(^±Ц=*).В(*^)+С(^)±
± 2Л/ [(^ - М (*£*))	-	М	(1^))]	.
= i + l±2M(x^.l
Любая дисперсия неотрицательна, поэтому
2 (*± ш) ^ °-	(з':’з,
Отсюда следует, что —охоу ^ К\у ^ т. е. \Кхг\ < <тхау.	U

Глава 3. Системы случайных величин ■ 127
Из свойства 3 следует, что если Кху ф 0, то с. в. X и Y зависимы.
Случайные величины X и Y в этом случае (Кху ф 0) называют корре¬
лированными. Однако из того, что Кху — 0, не следует независимость
с. в. X и Y. В этом случае {Кху =0)с.в.1иУ называют некоррелиро¬
ванными. Из независимости вытекает некоррелированность; обратное,
вообще говоря, неверно.
Как следует из свойств ковариации, она (Кху) характеризует и
степень зависимости случайных величин, и их рассеяние вокруг точки
(nix,my). Размерность ковариации равна произведению размерностей
с. в. X и У. В качестве числовой характеристики зависимости (а не
рассеяния) с. в. X и Y берут безразмерную величину — коэффициент
корреляции. Он является лучшей оценкой степени влияния одной с. в.
на другую.
Коэффициентом корре.аяции гху двух с. в. X и Y называется от¬
ношение их ковариации (корреляционного момента) к произведению их
с. к. о.:
*АХ _ CQv(^ ^0
ТУ ~ \HJX\/DY'
Очевидно, коэффициент корреляции равен ковариации стандартных
X - mx „ Y - ту
с. в. Z\ = —=	и Z2 = —^	, т. е. txy = cov(Z1,Z2).
X	у
Свойства коэффициента корреляции:
1.	Коэффициент корреляции по абсолютной величине не превосходит
1,	т. е.
\rXY\ < 1 или - 1 ^ ГХУ < 1.
2.	Если X и Y независимы, то
гХу = 0.
3.	Если с. в. X и Y связаны линейной зависимостью, т. е. Y = аХ + 6,
а ф 0, то
Irvvl = 1,
причем гху = 1 при а > 0,
гху = —1 при а < 0.
4.	Если |г\у| = 1, то с. в. X и Y связаны линейной функциональной
зависимостью.
□	1. Так как	^	ох	■	&у	(свойство 7 ковариации), то

128 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
2.	К ху = 0 в случае независимости X и Y. Следовательно,
rXY = *201 = о.
'ЛУ СГхСГу
3.	Согласно свойствам ковариации, имеем
cov(A:, Y) = cov(X,аХ + Ь) = cov(aX + 6, X) = acov (х + |, х) =
= acov(X,X) =aDX
и DY = D(aX + Ъ) = o?DX. Поэтому
соv(X,Y) _	aDx	а	_fl, при а > О,
= -« =/1’
И 1-1,
s/DXyfDY y/DX ■ \а\ ■ VDX Н 1-1, при а < 0.
4.	Пусть txy — 1- Тогда из равенства
D { X - тх _ У ~~ тпу \ _ ? Л _ Kxy \
^ °х	Оу	)	ОхОу }
(см. свойство 7 ковариации) получаем
D(XjZr^_Yj-my\
и\ ох	сту	) и’
X — Т11х У ^У	тт
т. е. —				— с — постоянная. Но
Ох	Оу
= 0-0 = 0,
т. е. с = 0. Значит, ^ Шх — —-—-, т. е. Y = ~^(Х — mx) + mv. При
СГу	с;ж 4	га
txy = — 1 получаем
Y = -^(X-mx)+my.
Таким образом, при г\у — ±1 с- в. X и Y связаны линейной функцио¬
нальной зависимостью.	■
Итак, для независимых случайных величин txy — 0, для линейно
связанных \txy\ = I, а в остальных случаях —1 < txy < 1; гово¬
рят, что с. в. связаны положительной корреляцией, если г \у > 0; если

Глава 3. Системы случайных величин "129
гху <0 — отрицательной корреляцией. Чем ближе |гху| к единице,
тем больше оснований считать, что X и Y связаны линейной зависи¬
мостью. Отметим, что корреляционные моменты и дисперсии системы
с. в. обычно задаются корре.аяционной матрицей:
(Кхх КХу\	(DX Кху\
\Кух Куу) ИЛИ {	DYJ.
Пример 3.8. Закон распределения дискретной двумерной с. в. задан
таблицей:
X\Y
-1
0
1
0
0,15
0,40
0,05
1
0,20
0,10
0,10
Найти коэффициент корреляции гХу •
о	Находим законы распределения составляющих X и Y:
Y
-1
0
1
Р
0,35
0,50
0,15
X
0
1
Р
0,6
0,4
Находим математическое ожидание составляющих: тх = 0:0,6+1*0.4 =
= 0,4. ту — — 1 • 0,35 + 0 * 0,50 + 1 • 0,15 = —0.20 (их можно было бы
найти, используя формулу (3.20); так
2 з
тх	= °'°> 15+0 0,40+0-0,05+1-0,20+1-0,10+1-0,10 = 0,4).
г-1 j-1
Находим дисперсии составляющих:
DX - [MX2 - {MX)2} - (О2 • 0,6 + I2 - 0,4) - (0,4)2 = 0.24,
DY - ((-I)2 • 0,35 + О2 * 0,50 + I2 • 0,15) - (-0,20)2 = 0,46.
Стало быть: ох — \/0,24 « 0,49, ау = >/0,46 « 0,68. Находим MXY,
используя формулу (3.26): MXY = 0* (—1) *0,15 + 0*0-0,40 + 0* 1-0,05 +
+ 1 * (—1) *0,20+ 1 *0*0,10+ 1 • 1 -0,10 = —0.10 (можно было бы составить
закон распределения Z = XY, а затем найти MZ = MXY:
Z = XY
-1
0
1
Р
0,20
0,70
0,10

130 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
MZ = MXY = —1 *0,20 + 0*0,70 + 1 -0,10 = —0,10). Находим корреляци¬
онный момент, используя формулу (3.31): K\y = [MXY — MX •MY] =
= —0,10—0,4* (—0,20) = —0,10+0,08 = —0,02 ф 0. Находим коэффициент
корреляции (формула (3.34)):
гху =
Ку
ХУ
х'-'у
отрицательная корреляция.
-0,02
0,49 • 0,68
-0,06,
Упражнения
1.	Двумерная с. в. (X, Y) задана законом распределения.
X\Y
1
2
3
4
1
0,07
0,04
0.11
0,11
2
0,08
0,11
0,06
0,08
3
0,09
0,13
0,10
0,02
Проверить, зависимы ли с. в. X и У. Найти cov(X, Y).
2.	Плотность совместного распределения с. в. X и Y задана формулой
t(l — ху3), при |ж| ^ 1, |у| ^ 1,
/<*■») = {„(
в остальных случаях.
Найти постоянную а и коэффициент корреляции.
3.	Непрерывная двумерная с. в. (X, У) задана плотностью распреде¬
ления вероятностей
/(*.») = {„(i
Зависимы ли с. в. X и Y? Найти м. о. и дисперсию с. в. X и Y.
'с(х + у), при 0 ^ ж ^ 1, O^y^l,
в противном случае.

Глава 3. Системы случайных величин ■ 131
3.8.	Двумерное нормальное распределение
Среди законов распределения двумерной с. в. (А", У) на практике
чаще всего встречается нормальное (гауссовское) распределение веро¬
ятностей. Оно применяется, в частности, для описания 2-х результатов
изменения, абсциссы и ординаты точки попадания (А", У) при стрельбе
и т.д.
Двумерная с. в. (А", У) называется распределенной по нормальному
закону, если ее совместная плотность f(x,y) имеет вид
ffay) =
‘l-na^Oys/X — г2
1	/	(х - тх)2 2г(х - тх)(у - ту) (у - ту)2 \
2(1 — г2) у *	~	+	а»	)	(3	35)
х е
II
где тх, ту, ох, оу, г = тху — параметры этого распределения.
Распределение (3.35) называется также нормальным законом рас¬
пределения на плоскости или двумерным нормальным (гауссовским)
распределением.
Можно доказать, что /(.т,у) — это функция плотности, т. е. спра¬
ведливо равенство
оо	оо
f(x,y)dxdy - 1:
—ОО —ОО
тх = MX, ту = MY; ах и ау — средние квадратические отклонения
(с. к. о.); г — коэффициент корреляции с. в. X и У.
Это означает, что двумерное нормальное распределение полностью
определяется заданием его числовых характеристик, что удобно на
практике (опытным путем находят эти параметры-характеристики и
получают совместную плотность /(ж, у) двух нормально распределен¬
ных с. в. А" и У).
Выясним смысл, например, параметров тх и сгХ1 найдя распреде¬
ление вероятностей составляющей X (т. е. плотность вероятностей од¬
номерной с. в. Аг). Согласно формуле (3.10) (п. 3.3) имеем:
ос
fl(x) = J f{x,y)dy =
2naxayVl - г2
(ж - тпх)2
е~2а2х{\-т2)х
оо	1	/	(у	-	ту)2	2г(х~тпх)(у у%)\
х / в”2(1_г2)''	~	ахау	'	ЛУ	=

132 " Раздел первый. Элементарная теория вероятностей и случайных процессов
Ж — 7ПХ
= *,
у/2ах
t2
ОО
Г _
1 — Г2
/ е
подстановки
у/2ау
=	-==е
2жохоул/\ - г2
— ОО
—	Ц	в 1-г2 / е !-
y/2ircrxy/l — г2	./
у -ту
уДс
= г
~^(z2-2rtz)
dz =
~2((г-г«)2-г2е2)
г	«г	=
t2	r2t2	оо
= е 1-га 1-г2	f
\/27ГсГд.л/1 — ^2 •/
(2 — г£)2
z — rt
подстановка —:■ — - = и
е 1 у/1 — г2
оо
/
\/1 — Г2
e_tt du =
-л/тг =
\/27I (7:v \f 1 - 7’^ ./	у/2п<Тх
—oo
oo
так как j e~u2 du — ypir — интеграл Пуассона, формула (2.39)
—oo
1
(re — mx)2
2a2
s/2ttox
t. e.
(x - тж)2
/,(х) = -J-(;
.	(3.36)
л/2тгсгх
Случайная величина X имеет нормальное распределение с м. о. тж и
дисперсией гг2. Аналогично получаем
_(у-т.у)2
/г(у) = в 2<Т»
V
(3.37)
т. е. У ~ N(my, сгу).
График плотности /(ж, у) нормального распределения двумерной
с. в. (X, У) представляет собой холмообразную поверхность, верши¬
на которой находится в точке I mx,my,
, т. е. макси-
у У’ 2тгстх(ту у/1 - г2 у
мум функции /(ж, у) достигается в точке (m^m^), рис. 42. Сечения
поверхности распределения плоскостями, проходящими через точку

Глава 3. Системы случайных величин ■ 133
[ тх,ту,	^ — — ] перпендикулярно плоскости Оху, предста-
\	<1'КОхОуу	\	— г1)
вляют собой кривые Гаусса вида г = ЪеГа^х ШхУ. Пересекая поверх¬
ность распределения £ = f(x,y) плоскостью г = zq, где 0 < zq < zm&x,
параллельной плоскости Оху, получим в сечении эллипс, уравнение
проекции которого	на плоскость Оху, имеет вид
(ж - Шт)2	(х* — тх)(к/ — га„) (v — mv)2	0
V		_	2r V	^ + U/	_y)_ = ^	з	38
^	^	oj
где /г2	= —2(1 — г2)	1 п (2 тт zo п;,, <т?у \/l - г2). (В силу ограничения на zq ар-
гумент логарифма меньше 1, следовательно, значение логарифма отри¬
цательно.) Если осуществить преобразование параллельного переноса
и поворота осей по формулам
{
х = тх + хо cos о; — j/о sin а,
у =	4-	хо	sin	а	+	уо	cos	а,
2г<7хСГу
где угол а подобран так, что tg 2а — —	-, то уравнение (3.38) пре-
< -
образуется в каноническое уравнение эллипса. Эллипс (3.38) называют
эллипсом рассеяния; оси симметрии эллипса (они образуют с осью Ох
углы аи^ + а) — главными ося.м,и рассеяния, а центр эллипса (точка
(тх,ту)) — центром рассеяния.
Если компоненты двумерной нормально распределенной с. в. (X, У)
некоррелированы (г = гху = 0), то функция плотности (3.35) прини¬
мает вид
I	/(х-тх)2	(з/-г%)2\
-в Ч «2 oi ) =
f(x,y) = ^	е
2тгахау
(х — тх)2	(у-тпу)1
= J- е 2ст* •	-А. е 2сгу =fi(x)-f2(y),
V	ZllGy
т. е. /(х,	у)	=	fi(x) • /2(2/)? где /lO^) — плотность распределения с. в. X.
/2(2/)	—	С.	в.	У. Отсюда следует вывод:	некоррелированные нормально
распределенные случайные величины являются также и независимы¬
ми. Справедливо и обратное утверждение. Таким образом, для нор¬
мально распределенных с. в. термины «независимость» и «некоррели¬
рованность» эквивалентны.

134 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Уравнение эллипса рассеяния для некоррелированных с. в. запи¬
сывается в виде
(X - тх)2	(у - Шу)2 _
(hax)2	(hay)2
(следует из равенства (3.38) при г = 0). На рис. 44 изображен один
такой эллипс (при h = 1).
Рис. 44
Утверждение. Если с. в. X и У независимы, то вероятность попа¬
дания случайной точки (X, У), распределенной по нормальному закону,
в прямоугольник Ft — {a ^x^b.c^y^d}, находится по формуле:
Р{« « X < 6,с « г 4 <1) = (фо (Цг*) - «о (Чг*)) х
1 Х _ ^
где Ф0(х) =		: f е 2 dz — функция Лапласа.
v27t о
Q Используя формулу (3.8),находим:
Р{а ^ X ^ Ь, с ^ d} —
£	_(s-mg)2	d	(у-т у)2
= “7=— / е	dx	-	-pL— I е 2(Ту dy =
\j2ircjx J	y/2it(Ty	J
- (*. (ц?о - -о №)) ■ (*»	(Чг*))	■
Произвольную область D можно приближенно заменить областью,
составленной из прямоугольников. На этом основано применение так
называемых «сеток рассеивания».

Глава 3. Системы случайных величин ■ 135
Можно показать, что вероятность попадания такой же точки (A", У)
в один из эллипсов рассеяния равна
р {<; --?*>- + <у ziyl ^ А = 1 _ е-К
I *2	^	J
Пример 3.9. Найти вероятность попадания точки (А", У) в прямо¬
угольник {|ж| ^ 1, |у| ^ 2}, если плотность совместного распределения
J х2 + 43/2
с. в. А и У равна f(x,y) =	6
О	Функцию /(.г*,у) можно переписать в виде
(у — О)2
1	-(
/(ж, у) =	■	е	2(^)2	•	—	е V 2 / = /](ж) • f2(y).
x^Vfrr
Значит, с. п. А’ и У независимы и А' ~ ДГ(0. \/3), У ~ N ^0.	•	Поэто¬
му Р{|х| ^ 1, \у\ < 2} - 2Ф0	•	2Ф0	=	4Ф0(0,58)	•	Ф0(2,31)	=
= 0,428.	•
3.9.	Регрессия. Теорема о нормальной корреляции
При изучении двумерной случайной величины рассматриваются не
только числовые характеристики одномерных компонент А и У (см.
п. 3.6), но и числовые характеристики условных распределений: услов¬
ные м. о., условные дисперсии.
Рч]	Условным	математическим	ожиданием	одной	из с. в., входящих
в систему (А", У) называется ее м. о., вычисляемое при условии, что
другая с. в. приняла определенное значение (или попала в данный ин¬
тервал). Обозначается: M(Y\X = х) или M(Y\x) и М(Х\у).
Вычисляются эти характеристики по обычным формулам м. о., в
которых вместо плотности распределения и вероятностей событий ис¬
пользуются условные плотности распределения или условные вероят¬
ности (п. 3.5).

136 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Для непрерывных с. в.
оо	оо
M(Y\x)= j yf(y\x) dy, M(X\y) — J xf(x\y)dx,	(3.39)
—	ОС	—ОО
где f(y\х) и f{x\y) — условные плотности распределения с. в. X и У,
определяемые равенствами (3.17) и (3.18) п. 3.5.
Для дискретных с. в. X и У условные м. о. вычисляются по фор¬
мулам
га	п
M(Y\xi) = У^Уз P(yj\xi) M(X\yj) = y^Xipi'Xilyj),	(3.40)
j-1	7-1
где p(yj\xi) = P{Y - yj\X = Xi}, p(Xi\yj) = P{X = Xi\Y = yj}, форму¬
лы (3.15) и (3.16).
Условное математическое ожидание с. в. У при заданном X = х, т. е.
M(Y\x) — (р(х), называется функцией регрессии или просто регрессией
У на х (или У по х). Аналогично, М(Х\у) = ip(y) — регрессия X на у
(или X по у).
Графики этих функций называются соответственно линиями (или
«кривыми») регрессии У на х и X на у.
Если обе функции регрессии У на х и X на у линейны, то говорят,
что с. в. X и У связаны линейной корреляционной зависимостью.
Теорема 3.4 (Теорема о нормальной корреляции). Если двумерная
с. в. (А", У) распределена по нормальному закону, то с. в. X и У связаны
линейной корреляционной зависимостью.
□	Условное м. о. M(Y\x) (т. е. функцию регрессии У на х) найдем,
используя условный закон распределения с. в. У при X = х, кото¬
рый определяется условной плотностью распределения f(y\x). Соглас-
J (х, у)
но формуле (3.17) f(y\x) = —-/—• . Совместная плотность f(x,y) зада-
j 1 (*£/
на формулой (3.35), а плотность распределения составляющей X есть
(X - 7ПХ)2
Ы'х) = ~7=—е	2<Т"
V27T ах
(см. формулу (3.36)). Имеем

Глава 3. Системы случайных величин ■ 137
х / (х - тпх)2 2г(х - тх)(у -ту) (у - т,;)2 \
v р 2(* - г2) \	^	<Г:Е<Т!'	/	V	27ГСТх.
е
Произведем упрощение показателя экспоненты в последней формуле.
Так как
1	({х-тх)2 2г(х - тх)(у - ту) ( (y-m^)2^
1	о	I	+
2(1 - г2) V сг2	а2у
(х-гпх)2 1-г2 ^	1
2а2	1 — г2 2(1 — г2)
(х~тх)2	2г{х - тх){у - ту) (х-тх)2(1-г2)	(у~ту)
2
+
сР1 ^х^у	sjZ
1	({у-	ту)2	2r ix - ГПХ) (у - ту)	Г2 (ж - тх)2
2(1 -r2)\ а2у а2х j
1	{У~ту х-т^
—	Т2) \ °у Г'	)
ТО
2(1
(у- (m„ + г-^(х-тпх))^
f(y\x) - - —=		е.	%py\f\	г2)2
у/2тг(ауу/1 - г2)
Как видно, условный закон распределения является тоже нормальным
с условным м. о. и условной дисперсией, определяемыми формулами
M(Y\x) = my + r•	и	D(Y\x) = <т2(1 — г2).	(3.41)
Аналогично можно получить функцию регрессии X на у:
v(y) = ЩХ\у) = ГПХ + г • a^ybJny^ И D(X\y) = £Т2( 1 - г2). (3.42)
Так как обе функции регрессии (3.41) и (3.42) линейны, то корреляция
между с. в. А" и Y линейная.	■

138 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 3.10. Построить линию регрессии У на х для двумерной с. в.
(X, У), закон распределения которой задан таблицей:
X\Y
-1
0
1
£
0
0,10
0,15
0,20
0,45
1
0,15
0,25
0,15
0,55
Е
0.25
0,40
0,35
1
О Находим условные ряды распределения с. в. У при х\ ~ 0, х2 — 1,
используя формулы (3.15).
рЫхх) = Р{У = -1\х = 0} = 5^ = g,
р(У2Ы - P{Y	= 0\х	=	0}	=	= ||,
p(y3|xi) = P{Y	= 1|Х	=	0}	=	[£§	= Ц.
Согласно (3.40) имеем M(Y\x\) = —l-^jr + 0- ^ + l- |jr = ||г.
Далее:
p(l/i|x2) = P{Y = -1|Х = 1} = Щ = 1|,
PM**) -	= °1х	=	1}	=	бЦ	= §»
р{у3\х2) =	= 1|*	=	1}	=	Щ	= ||.
Следовательно, М(У(2:2) = —1-“~+0-Щн-1-^ = 0.
ОО	00	оо
Линия регрессии У на х изображена на рис. 45.	•
М(Г\х>>
1-

Глава 3. Системы случайных величин *139
Упражнения
1	- Построить линию регрессии X на у по условию примера 3.10.
2.	Пусть (X, Y) — двумерная нормальная с. в. с параметрами тх =
= т.у = 0, ох — cry = 1. Найти условную плотность распределения
с. в. X при условии, что Y = у и с. в. Y при условии, что X = х
(воспользоваться формулами (3.36) и (3.37)).
3.10.	Многомерная (n-мерная) случайная величина
(общие сведения)
Систему п случайных величин называют п-мерной (многомерной)
с. в. или случайным вектором X = (Xi, Х2, • • •, Хп).
Многомерная с. в. есть функция элементарного события ги:
(Ai, Хг, ..., А”п) = <£>(ги); каждому элементарному событию ги ста¬
вится в соответствие п действительных чисел х\, х2, 		—
значения, принятые с. в. Х\, Х2, Хп в результате опыта. Век¬
тор х = (xi,x2,... ,хп) называется реализацией случайного вектора
(Аг, А2,... , Хп) = А.
Закон распределения вероятностей n-мерной с. в. задается ее функ¬
цией распределения
FXi,X2,---,Xn (^х, Х2, • • • 5 хп)	Х2	•	•	•	ч	Хп	<с j-.
Функция распределения F(x\, х2,..., хп) обладает такими же свойства¬
ми, как и функция распределения двух с. в. F(x,y). В частности: она
принимает значения на отрезке [0,1], F(+оо, +оо,..., оо) = 1, F^Xj) =
—	F(+oo, +оо, жз, +ос,..., +ос).
Плотность распределения системы п непрерывных с. в. (A"i, А2	
... , Хп) определяется равенством
,	,	„	х	_	dnF{xux2,...	,хп)
fxux29...jcn{xux2, ...,хп) dxidx2 uaadXn •
При этом: f(x\,x2,..., хп) ^ 0 и
оо оо	оо
I J	I f^xl,x2'--^xn^<lxl(ix2---,dxn = 1-

140 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Вероятность попадания случайной точки (Х\, Х2,.. •, Хп) в область
D из го-мерного пространства выражается го-кратным интегралом
Р{{хих2,...,хп)ею} =	f(x\,X2i - • • ,хп) dx\dx2 .. •,dxn.
(b)
Функция распределения F(x\, Х2,. • •, хп) выражается через плотность
f(xi,x2, • • -, хп) го-кратным интегралом
Х\ X 2	Х-п
F(xx,x 2,...,Х„) =	f(x 1,гг2,... ,^n) dx\dx2 ...,
-00 —00	—00
Необходимым и достаточным условием взаимной независимости
го с. в. является равенство:
FxiyX2y--^xn(xijX2,‘-‘jXn) Fxi(x\) ’ Fx^ix1^) • • • Fxn{xfi),
для го непрерывных с. в.:
/Л',,Л'2,...,Л'ЛХ1’Ж2, . . . ,®„) = /л', (г-ч) • /х2(ж2) • • • fxjxn)-
Основными числовыми характеристиками многомерной с. в. (Хь
Х2,.. -, Хп) являются:
1.	го м.о. составляющих Xi, т.е. mi = МХ\, rri2 = ЛОГ2, .шп —
= МХп;
2.	го дисперсий составляющих А"*, т.е. D\ = DA'i, D2 = £^2,
Dn = DXn, при этом Di = M(Xi — mz-)2, г = l,ro;
3. ro(ro —1) ковариаций, т. e. = M(Xi-Xj), г ф j, при этом = Kji,
= Д.
Ковариации образуют ковариационную матрицу
/Х>1	К12	К1п\
^2	*	*	*	^2п
f#ll
#12
#1гЛ
#21
#22
#2п
V ^711
#п2 ••
#пп/
или
\
DnJ
3.11.	Характеристическая функция и ее свойства
Наряду с функцией распределения, содержащей все сведения о с. в.,
для ее описания можно использовать так называемую характеристи¬
ческую функцию. С ее помощью, в частности, упрощается задача на¬
хождения распределения суммы независимых с. в., нахождения число¬
вых характеристик с. в.

Глава 3. Системы случайных величин ■ 141
Характеристической функцией с. в. X называется м. о. с. в. егЬХ,
обозначается через <px(t) или просто cp(t).
Таким образом, по определению
<px(t) = Me ,
где / — параметр, г = >/—1 — мнимая единица.
Для д. с. в. X, принимающей значения х\, Х2, • *с вероятностями
= Р{Х = ж*.}, А; = 1,2,3,..., характеристическая функция опреде¬
ляется формулой
оо
<px(t) = Y^ettXkPk-	(3.43)
k=1
для н.с. в. с плотностью f(x) — формулой
ОО
V>x{t) = J eit3 f(x)dx.	(3.44)
—оо
Заметим, что:
1.	Характеристическая функция н. с. в. есть преобразование Фурье от
плотности ее распределения; плотность f(x) выражается через cp(t)
обратным преобразованием Фурье:
оо
/(*) = ^ j <p(t)e~itx dt.
2.	Если с. в. принимает целочисленные значения 0,1,2,..., то (p(t) =
= ~ф{г), где г = elt. Тогда
ОО
^(г) = zkVk
k=О
(см. и. 2.6).
Свойства характеристической функции:
1.	Для всех t Е R имеет место неравенство
M*)l < v(o) = 1.
2.	Если Y = а А + 6, где а и 6 — постоянные, то
= eltb(px(at.).

142 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
3.	Характеристическая функция суммы двух независимых с. в. X и Y
равна произведению их характеристистических функций, т. е.
<px+Y(t) = vx{t) ■
4.	Если для некоторого к существует начальный момент А;-го порядка
с. в. X, т. е. а*. = М(Хк)у то существует к-я призводная характери¬
стической функции и ее значение при t = О равно о^., умноженному
на г*, т. е.
v4fe)(0) = ikM{Xk) = ikak.
Q 1. \(p(t)\ = |MetiX\ ^ M\eztx\ = Ml = 1. (Действительно, \eltx | =
—	| costX + isinfX| = \fcos2 tX + sin2 tX — 1); ip(0) = Me0 = Ml = 1.
2.	(pY(t) = MeitY = Me**(flA4b) = M{emeiaXt) = eitbMeiatx =
= elibipx{at).
3.	v?A’i+x2(t) = Mett<Xl+X2> = M(eitXleiiX*) = Meif-Yl • Meitx* =
—	¥>Xi (*) *	Свойство справедливо для суммы п независимых с. в.
4.	—- = г*М(Х*ег*Л). Поэтому при i = 0 имеем
dt
^W(O) = ikM(Xk) = ikak.	■
Из свойства 4 следует	Отсюда как частный случай,
можно получить:
ttl = мх = - V(u) (г1 = I = i = -ij =	,
а2 = MX2 = -У'(0),	(3.45)
DX = а,2 - о? = -</(0) + (у>'(0))2.
Пример 3.11. Найти характеристическую функцию с. в. X, распреде¬
ленной по биномиальному закону. Найти МХ и DX.
О	С. в. X принимает значения 0,1,2	пс вероятностями
Рк = Р{Х = к} = Скркдп-к.
Поэтому, используя формулу (3.43) и формулу бинома Ньютона, нахо¬
дим, что
Ф) = £ rakc№<f'-k = Cn(eil • p)kqn-k = (еир + q)n,
к=0 к=()

Глава 3. Системы случайных величин "143
т. е. (p(t) — (eltp + q)n. Используя формулы (3.45), находим:
MX = — i(n(eltp + q)n 1 • регТ * г)
t=о
= пр, DX = npg.
Упражнения
1.	Найти характеристическую функцию с. в. X, равномерно распре¬
деленной на отрезке [а, &].
2.	Найти MX с. в. X, распределенной по закону Пуассона с параме¬
тром а, используя характеристическую функцию (px(t)-
3.	Дано /л(х) = к2х' при Х е	-	плотность	с.в. X.
[О, в противном случае
Найти (px(t)-
3.12.	Характеристическая функция нормальной
случайной величины
Согласно формуле (3.44), характеристическая функция с. в. X
N(a.a) равна
°$	(х — a)2	°S х2 — 2(а + ita2 )х + а2
w(t) = J_ I eltxe 2<т2 dx =	I	e	2a2	dx =
\fbxo J	s/Ъко	J
—	OO
OO
= —[
y/bia J
—oo	—OO
00 x2 — 2x(a -f iter2) + (o 4- ita2)2 + o2 — (a + iter2)2
e	2cr2	dx	=
—oo
^O' 	 (П _L	Ол^/т2	_i_	I
(ж — (а + ita-'))2 2aita2 + (iter2)2
- I «-
V^c
-	—	I	e	2a2	• e 2a2 dx =
\Пжа J
y/2na
2aito2 -t2a4 y? (I- - (a + iter2))2

144 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
оо / х — (а + На2) \ 2
= ^Le“-¥ f е~\	'Я*	)	ri(X~(^‘to2)).^	=
\/2па
—оо
1	'* t2*?2 ■* t2(j2
= -==е	2	v/тг	=	е	2
у/ТТ
(7 - *	\
I	I е 11 du =	—	интеграл	Пуассона	1
— —г • 1 • га = а,
t=o
7 _
Таким образом, v?x(^) — ^ а 2 ? если X ~ N(a,a).
Пример 3.12. Найти с помощью характеристической функции м. о. и
дисперсию с. в. X ~ iV(a, &)•
О Применим формулы (3.45):
.	t2<r2
MX = [—г<^/(0)] = —2>ш	2	(га — to )
т. е. MX = а;
DX = [-</(0) + (v/(0))2] =
= - ^-а2еш~1-¥ + (ia - ta2)2eiat~^f^ |t_0+M2 =
■ 2 -2 2 , -2 2 2
= а — г а + га = о ,
т. е. DX = гг2. Получили известные нам результаты: a — м. о., a —
с. к. о.; эти параметры полностью определяют с. в., распределенную по
нормальному закону.	в

Глава 4
Функции случайных величин
Часто возникают задачи, в которых по известному закону распре¬
деления (или числовым характеристикам) одной (или нескольких) слу¬
чайной величины требуется определить распределение другой (или не¬
скольких) случайной величины, функционально связанной с первой.
Построение законов распределения функций некоторых д. с. в. (сХ7
X + У, X • У) уже было рассмотрено в п. 2.5 (свойства 2, 3, 5 мате¬
матического ожидания).
4.1.	Функция одного случайного аргумента
F0	Если каждому возможному значению с. в. X по определенному пра¬
вилу соответствует одно возможное значение с. в. У, то У называют
функцией случайного аргумента X, записывают У = (р(Х).
Пусть X — д.с.в. с возможными значениями Ж1,Ж2,жз,... ве¬
роятности которых равны соответственно	•	■	•	,Рт т.е. pi =
= Р{Х = ж*}, г = 1,2,3,..., го. Очевидно, что с. в. У = <р{Х) так¬
же д.с. в. с возможными значениями у\ = (р(хi), у2 — (р(х2), j/з =
= <р{х%),..., уп = <р{хп), вероятности которых равны соответственно
Р\ 1 Р2>Рзj• • • 1 Рп 1 Т.е. если у* = <p(xi), то р,- = Р{У = у*} = Р{Х = ж*},
г = 1, п.
Отметим, что различным значениям с. в. X могут соответствовать
одинаковые значения с. в. У. В этом случае вероятности повторяющих¬
ся значений следует складывать.
Математическое ожидание и дисперсия функции У — <р{Х) опреде¬
ляются соответственно равенствами
п	п
МУ = <p(xi)pi, DY =	-	myfpi.
г—l	i~ 1

146 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Пример 4.1. Задан закон распределения д. с. в. X:
X
-1
1
2
р
0,1
0,3
0,6
Найти МУ, если: а) У = X2, б) У - 2Х + 10.
О а) С. в.	У принимает значения у\ —	<р(х\)	= (—I)2 =	1,	у2	= I2 =
= 1, уз =	22 = 4, т. е. она принимает	два значения: 1	и	4,	причем
Р1 = р{у	= 1} = Р{Х = -1} + Р{Х	= 1}	= 0,1 + 0,3	*=	0,4, Р2 -
-	Р{У = 4} = Р{Х =2} = 0,6. Следовательно, МУ = 1 • 0,4 + 4 • 0,6 =
= 2,8.
б) Закон распределения У имеет вид
У
8
12
14
Р
0,1
0,3
0,6
Значит, МУ = 8 • ОД + 12 • 0,3 + 14 • 0,6 = 12,8.	•
Пусть X — непрерывная с. в. с плотностью распределения f(x). а
с. в. У есть функция от X, т. е. У = <р(Х). Найдем закон распределе¬
ния с. в. У. Будем считать функцию У = <р{Х) непрерывной, строго
возрастающей и дифференцируемой в интервале (а, Ь) (он может быть
бесконечным: (—ос, ос)) всех возможных значений с. в. X. Тогда суще¬
ствует функция х = ф(у), обратная функции у = (р(х) (случайная точка
(Х,У) лежит на кривой у = <р(х)). Функция распределения Gy (у) (ее
можно обозначить и так: G(y), Fy(y) или Fy (ж)) с. в. У определяется по
формуле G(y) = P{Y < у}. И так как событие {У < у} эквивалентно
событию {X < ф{у)} (рис. 46), то

Глава 4. Функции случайных величин • 147
i’(y)
G{y) = P{Y < у} ~ Р{Х < ф(у)} = Fx(4>(y)) = J f{x)dx. т.о.
а
Ф(у)
G(y) = J f{x) dx.
Дифферешшруя полученное равенство по у, найдем плотность распре¬
деления с. в. У:
9 (у) =	= /(^М) •	=
т. е.
<?Ы = f№(yW(y)-	(4.1)
Если функция у = vKx) в интервале (а, Ь) строго убывает, то событие
< у} эквивалентно событию {X > *ф(у)}. Поэтому
О	Ч’У
G(y) = J f(x) dx = — J /(x) dx.
iP(y)	Ь
Отсюда следует, что
в(у) = -fMvWiv)-	(4-2)
Учитывая, что плотность распределения не может быть отрицательной,
формулы (4.1) и (4.2) можно объединить в одну
9(у) = fbP(y)№'(y) I-	(4-3)
И, наконец, если функция у = (р(х) немонотонна в интервале (а,Ь), то
для нахождения д(у) следует разбить интервал на п участков монотон¬
ности, найти обратную функцию на каждом из них и воспользоваться
формулой
п
9(у) = 5^/Шу)МЧу)1-	(4-4)
i—	1
Если с. в. X является непрерывной и f(x) — плотность ее распреде¬
ления, то для нахождения числовых характеристик с. в. У = ^(Х)
необязательно находить закон ее распределения, можно пользоваться

148 * Раздел первый. Элементарная теория вероятностей и случайных процессов
формулами
оо
MY = M(ip(X)) = J ф)/(х) dx,
"оГ	(4-5)
DY = D(tp(X)) = J (ф) - myff{x) dx.
Пример 4.2. Найти плотность распределения функции У = — ЪХ +2,
считая X н.с. в. с плотностью f(x).
О Функция у = —5х + 2 монотонно убывает в интервале (—ос, оо).
Обратная функция есть х = ^(2 — у) = ^(у), ^(у) ~ —По форму-
ле (4.3) имеем у (у) = /	'	“g	= 5/ (^5^)’ У G (-00»00)- •
Проиллюстрируем на примере 4.2 вывод формул для функции и
плотности распределения:
G(y) = P{F < у} = Р{-5Х +2 < у} = р|х >	=
= 1_р{х<^»} = 1_(р{х<^»}+р{х = 1^}) =
= 1_F{X<^1} = 1-FX(2-^).
так как Р	^	-	0, т. е. G(y) = 1 — Fx ^”5”^) — функция
распределения с. в. У. Тогда
*»> - *<»> - (1 - ^ (v))',=-л (v) ■ м;=
т.е. д(у) = |/ (^5^)’ У G (_00’00)-
Отметим, что линейное преобразование У — аХ + 6 не меняет ха¬
рактера распределения: из нормальной с. в. получается нормальная; из
равномерной — равномерная.

Глава 4. Функции случайных величин ■ 149
Пример 4.3. Пусть с. в. X имеет равномерное распределение в интер¬
вале	• Найти математическое ожидание с. в. Y = cos X: а) най¬
дя плотность с]{у)\ б) не находя д{у).
О	Имеем:
f(x) = <
г4-
/	7Г яЛ
V	2’ 2/ ’
[о. **(-§.§).
В интервале	функция	у	cos	ж	не	монотонна:	в	Функ¬
ция возрастает, в ^0,	—	убывает.	На	первом	участке	обратная	функ¬
ция х\ = — arccosу = ipi{y), на втором —	=	arccosу = •ф‘г(у)- По
формуле (4.4) имеем
д{у) = f(^i(y)M[(y)\ + /(^Ы)|^2Ы1 =
+
7Г
\/1 - У2
7Г^/1 — У2
т. е.
Тогда
{ТТЛ
0,
д(у) = < тг\/1 - у2
, при 0 < у < 1,
при у ^ 0 или у ^ 1.
МУ
ОО	1
= [/уаЫ^]=/у^==^ =
=	\ У (1 - у2)^ d(l - у2) = -± • 2\/Г—^
7Г ’
т.е. MY = f.
б) Используем формулу (4.5):

150 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Упражнения
1	- Дискретная с. в. X задана законом распределения
X
-2
-1
0
1
2
3
р
0,10
0,20
0,30
0,25
0,10
0,05
Найти закон распределения случайных величин: а) У = 2Х2 — 3;
б) У = урГ+2; в) У = sing*.
2.	Дискретная с. в. X задана своим рядом распределения
*
0
1
2
3
Р
0,3
0,4
0,2
0,1
Построить многоугольник распределения с. в. X и У = cos2 —X.
Найти МУ и сгУ.
3- Найти плотность распределения и дисперсию с. в. У = X + 1, если
Д[-2,2].
4.	Случайная величина X ~ iV(0,1). Найти плотность распределения
с. в.: а) У = ЗХ3; б) У = \Х\.
5.	Пусть X — н. с. в. с плотностью
( .	(е~х, при х ^ О,
^0, при ж < 0.
Найти функцию распределения и плотность распределения с. в. У,
если а) У = 2Х - 1; б) У = X2.
4.2.	Функции двух случайных аргументов
Для решения ряда практических задач необходимо знать закон рас¬
пределения (или числовые характеристики) случайных величин вида
Z = X±Y, Z = X • У, Z = VX2 + У2, Z - max(X. Y) и других.
Рч|	Если каждой паре возможных значений с. в. X и У по определен-
ному правилу соответствует одно возможное значение с. в. Z, то Z
называют функцией двух случайных аргументов X и У, записывают
Z = <p(X,Y).

Глава 4. Функции случайных величин *151
Найдем закон распределения суммы двух случайных величин (наи¬
более важный на практике), т. е. закон распределения с. в. Z = X + Y.
Пусть система двух непрерывных с. в. (X,Y) имеет совместную
плотность распределения /(ж,у). Найдем по формуле (3.8) функцию
распределения с. в. Z = X + У.
Fz(z) = P{Z <z} = Р{Х + Y < z} — JJ f(x,y) dxdy.
Dz
Здесь Dz — множество точек плоскости Оху, координаты которых удо¬
влетворяют неравенству х + у < г, см. рис. 47.
Имеем
ОО Z—X
Fz{z) = /(/ f{x,y)dySjdx.
-ОО —ОО
Дифференцируя полученное равенство по переменной г, входящей в
верхний предел внутреннего интеграла, получаем выражение для плот¬
ности распределения с. в. Z = X + Y:
оо
fz(z)= J f(x,z-x)dx.
(4.6)
Если с. в. X и Y независимы, то, как известно (п. 3.4), f{x,y) =
—	fi(x)h(y)- Формула (4.6) примет вид
ОО
Jz{z) = fx+r(z) = J fi{x)fe{z - x) dx.	(4.7)
Закон распределения суммы независимых с. е. называется компо-
зициеи или сверткой законов распределения слагаемых.

152 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Плотность распределения с. в. Z можно записать в виде fx+Y -
—	fx * Jy, где * — знак свертки, а формулу (4.7) называют формулой
свертки или формулой композиции двух распределений.
Записав Z в виде Z — У+Х, можно получить другое представление
для /z(^), а именно
оо
fz(z) = J f{z-y,y)dy
—	ОС
ОО
fz(z) = J h(z - y)f2{y) dy
в случае независимости с. в. X и У.
Задача нахождения закона распределения с. в. вида Z = X — У,
Z = X У и других решаются аналогично.
Пример 4.4. Пусть с. в. X ~ ЛГ(0,1), с. в. У ~ N(0,1). Найти закон
распределения с. в. Z = X -1- У, считая X и У независимыми с. в.
О Используя формулу (4.7), получаем
/°°	1 ZL 1	(* ~ж)2	-I	Г* 2ж2 - 2zs + z2
Же~2 Же~ 2 dx=2iJe~ 2 ■fa =
-оо	—оо
оо 2 Г(х —	оо
D-
—оо	—оо
9 22
2^ - —	
ОО
„2
е du — y/ir — интеграл Пуассона^.
т. е.
w(г)=^fee"2,'/5,5'
Сумма независимых нормальных с. в. (с m = 0, сг = 1) имеет нормаль¬
ное распределение (cm = 0,a = V2)-	•

Глава 4. функции случайных величин "153
RTJ Пример 4.5. Совместное распределение с. в. X и У задано плотностью
распределения вероятностей
/<*.»>-{Г*
Найти плотность распределения вероятностей с. в. Z = X — У.
при
в противном случае.
о	Найдем сначала функцию распределения F(z) с. в. Z, а затем — ее
производную F'z(z) = fz(z).
Fz(z) = P{Z <z} = P{X -У <z} = j j {x + y) dxdy,
Dz
где Dz — множество точек, координаты которых удовлетворяют нера¬
венству х — у < z, т. е. у > х — z (они находятся выше прямой у = х — z),
где £ — произвольное число. Очевидно, если 2 ^ —1, то F(z) = 0; так
как /(ж, у) = 0 вне квадрата	Область интегриро¬
вания Dz при —1 < z ^ 0 изображена на рис. 48, при 0 < z ^ 1 — на
рис. 49).
При — 1 < z 0 имеем
1+Z	1	1+Z	2 '
F(z) = JJ(x + у) dxdy = J dx J (x +у) dy= J dx (xy + у)

154 " Раздел первый. Элементарная теория вероятностей и случайных процессов
При 0 < z ^ 1 имеем
Z	1	11
F(z) = j j (х + у) dxdy = J dx J(x + y) dy + J dx / (X +y)dy =
Dz	0	0	г	x—z
=/*■ {xy+£) C+fdi fa+j-) L=
0 2
= J (x+^J dx + J (ж + i - ж2 + xz —	2—) dx =
—	z2 I z I 1 z2 I 1 z 1 . z3 . z z3	(1	z)3 n —z2 + 2z +1
2	“*"2	2	2	2	2	3	3	2	2	6	2
При z > 1 имеем
г i
F(^) = JJ(X + У) dxdy = J dx j(x + y)dy =
= / (Ж + Й dx = (%" + I)
0 0
= 1.
Таким образом,
fo,
(z + I)2
2 ’
-z2 + 2z + 1
2
1,
при z ^ — 1,
при — 1 < г ^ 0,
при 0 < z ^ 1,
при z > 1.
Следовательно,
r0, при z ^ —1, 2 > 1,
F'z(z) = /z(*0 = {z + 1, при - 1 < z ^ 0,
1	— г, при 0 < 2 ^ 1.
Контроль:

Глава 4. Функции случайных величин ■ 155
оо	—I	(J	1	оо
/,{z)dz= / О dz + j(z + 1) dz + J(1 — z) dz + J 0 dz' =
-oo	—oo	—10	1
(z + 1)2|0	(l-*)2
1-1
= i-0-0+£ = l. •
0	2	I
Пример 4.6. Независимые с. в. X и У распределены равномерно
ДМ, У i?[0.1]. Найти плотность распределения вероятностей
с. в. Z = X + Y (рис. 50).
Рис. 50
О Система с. в. (X, Y) равномерно распределена в прямоугольнике
D = {0 < х ^ 4,0 ^ у < 1},
[0, х $ [0,4],	10,
у £ [о, 1]>
у г [0,1].
Так как с. в. X и Y независимы, то f(x,у) = fi(x) • /2(у) = ^ • 1 =
Fz(*)=P{X + y <*}= JJ \dxdy = \sDz,
Dz
(x+y<z)
где *S'd2 — площадь области — части прямоугольника, лежащей
ниже прямой а; + у = г. Если 2 ^ 0, то	=	0; если 0 < £ ^ 1, то
F(z) = ^ (так как Spz — 7} • z - z); если 1 < z ^ 4, то
*■(*) = | • (?r-2±z ■!) = \i2z -1);

156 " Раздел первый. Элементарная теория вероятностей и случайных процессов
если 4 < z ^ 5, то
F(z) =	\	- (l	•	4 -	1(5 - z)(b - z))	= |(8 - (5	- z)2);
если 5 < г, то F(z)	=	i	•	4	=	1. Итак,
!0,	z ^ 0, z >	5,
0,25z,	0 < £ ^ 1,
0,25,	1 < 2 < 4,
0.25(5 — z),	4<z<5.
Контроль:
оо	14	5
I f{z) dz = J dz + J\dz+\J (5 - z) dz = 1.
Полученную плотность распределения fz(z) можно найти другим
способом, используя формулу
оо
f(z) = J fi(x) ■ h{z - х) dx.
—	ОО
4	4
= J ~x)dx=^ J f2(z - x)dx.
Имеем
о	о
Функция под знаком интеграла отлична от нуля лишь в случае
г0 ^ х ^ 4,
х0 ^ ^ — X ^ 1,
т. е.
г0 ^ х ^ 4,
{!
{
z - 1 ^ X ^ 2.	^
Решение системы зависит от значения г.
1.	Если z ^ 0, система несовместна; отрезки [0.4] и [z — 1 ,z\ не
пересекаются, (см. рис. 51).
Следовательно, /2(2 — х) = 0 и fx+y{z) = 0.
2.	Если 0 < £ ^ 1, система (4.8) эквивалентна неравенству 0 ^ х < z
(см. рис. 52).
Поэтому

Глава 4. Функции случайных величин "157
		
2—1 г
X
0 4
X
Рис. 51
2—1 2
	
	
X
О 1	4
Рис. 52
3.	Если 1 < г ^ 4, система (4.8) эквивалентна неравенству z — 1 ^
^ х < z (см. рис. 53).
2—1
X
\>'/Y/Х////-/У///-Zt'/ZA-
0	1	4
Рис. 53
Поэтому
z
fx+y{z) = | J 1 dz =	±(z-z	+	1)	=
2-1
4.	Если 4 < 2 ^ 5, то z — 1 ^ ж < 4 (см. рис. 54).
	У////УХ—I-
2—1	2;	5
Рис. 54
Поэтому
fz(z) = \ J ldx = ±x\4 =1(4-г + 1) = 5	*
\Z-1
2-1
-J	У/&/Л
X
Рис. 55

158 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
5.	Если 5 < то система (4.8) несовместна (см. рис. 55), а, значит,
fz{z) = 0. Итак.
г0,
<
О
V
0,25z,
0
<
z ^ 1,
' 0,25,
1
<
z ^ 4,
,0,25(5 -
-z), 4
<
z ^5.
Упражнения
1.	По условию примера 4.5 найти функцию и плотность распределе¬
ния вероятностей с. в. Z = X + Y.
2.	По условию примера 4.5 найти плотность распределения вероятно¬
го
стей с. в. Z = —.
4.3. Распределение функций нормальных случайных
величин
Рассмотрим распределение некоторых с. в., представляющих функ¬
ции нормальных величин, используемые в математической статистике.
Распределение х2 (хи-квадрат или Пирсона)
Распределением Хп с п степенями свободы называется распределе-
ние суммы квадратов п независимых стандартных случайных величин,
т. е.
п
= Где О’1)’ * = 1,2,...,п.
»= 1
Плотность вероятности с. в. х2 зависит только от числа п, т. е. чи¬
сла слагаемых. Если п = 1, то \2 — X2, где X ~ N(0,1),
^2
Плотность распределения с. в. У = X2 равна (согласно (4.4))

Глава 4. Функции случайных величин ■ 159
Плотность распределения Хп имеет вид
22Г(!)
ж 2 е 2, при х > О,
при х < О,
где Т(р) = j tp le 1 dt — гамма-функция Эйлера (Г(р) = (р — 1)! для
о
целых положительных р). С возрастанием числа степеней свободы п
распределение х2 приближается к нормальному закону распределения
(при п > 30 распределение х2 практически не отличается от нормаль¬
ного);
Мх1 = П, Dxl = 2 п.
На практике, как правило, используют не плотность вероятности, а
квантили распределения х2*
Квант,илью распределения х2, отвечающей уровню значимости а,
называется такое значение х2 ^ Ха.т ПРИ котором
оо
Р{-Хп ^	^
X-CL.TI
С геометрической точки зрения нахождение квантили Ха,п заклю¬
чается в выборе такого значения х2 ~ Ха,т чтобы площадь заштрихо¬
ванной на рис. 56 фигуры была равна а.
Значения квантилей приводятся в специальных таблицах-прило¬
жениях.
Для стандартного нормального распределения квантили уровня
а обозначаются через ±wQ, причем иа является решением уравнения
Ф(иа) =
1-Q

160 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Распределение Стьюдента
Рм	Распределением	Стьюдента (или t-распределением) с п степенями
свободы называется распределение с. в.
гг _ Z
1	П —	.	1
\fbd
где Z ~ N(0,1) — стандартная нормальная величина, независимая от
Хп-распределения.
Плотность вероятности распределения Стьюдента имеет вид
( 71 + I \	Я- -J- 1
V	2 У (	f2\~	2
!тМ = г(|к^I1 +«)	;
При п сю распределение Стьюдента приближается (уже при п > 30
почти совпадает) к нормальному;
МТп = О, DTn =	п>	2.
Т), Z
На практике используют квантили ^-распределения: такое значение
t = ta . ЧТО
2	’
оо
P{\t\>tzn} = 2 J f(t)dt = a.
ta
2	,n
С геометрической точки зрения нахождение квантилей заключается в
выборе такого значения t = ta , чтобы площадь заштрихованной на
2,п
рис. 57 фигуры была равна а.
Рис. 57

Глава 4. Функции случайных величин "161
Распределение Фишера-Снедекора
Распределением Фишера Снедекора (или F-распределением) с т и
п степенями свободы называется распределение с. в.
F =
^Хт
1	2 5
nxi
где Хт и Хп — независимые с. в., имеющие х2-распределение соответ¬
ственно с т и п степенями свободы.
При п -> ос F-распределение стремится к нормальному закону.
MF =
п
п-Т
п > 2, .DF =
2n2(m + п — 2)
ш(п — 2)2(п — 4)’
п > 4.
На практике обычно используют квантили распределения: такое зна¬
чение F = FQ)m,n, что
сю
Р{Р > Fa,m,n} = J f(F)dF = a.
С геометрической точки зрения нахождение квантили заключается в
выборе такого значения F = FQ>mj71, чтобы площадь заштрихованной
на рис. 58 фигуры была равна а.
Рис. 58

Глава 5
Предельные теоремы теории вероятностей
Рассмотрим ряд утверждений и теорем из большой группы так на¬
зываемых предельных теорем теории вероятностей, устанавливающих
связь между теоретическими и экспериментальными характеристика¬
ми случайных величин при большом числе испытаний над ними. Они
составляют основу математической статистики. Предельные теоремы
условно делят на две группы. Первая группа теорем, называемая зако¬
ном больших чисел (коротко: ЗБЧ), устанавливает устойчивость сред¬
них значений: при большом числе исиытаний их средний результат пе¬
рестает быть случайным и может быть предсказан с достаточной точ¬
ностью. Вторая группа теорем, называемая центральной предельной
теоремой (коротко: ЦПТ), устанавливает условия, при которых закон
распределения суммы большого числа случайных величин неограни¬
ченно приближается к нормальному.
В начале рассмотрим неравенство Чебышева, которое можно ис¬
пользовать: а) для грубой оценки вероятностей событий, связанных со
с. в., распределение которых неизвестно; б) доказательства ряда теорем
ЗБЧ.
5.1.	Неравенство Чебышева
Теорема 5.1. Если с. в. X имеет м. о. MX = а и дисперсию DX, то для
любого е > 0 справедливо неравенство Чебышева
Р{\Х - MX I ^ е}< Щ-.	(5.1)

Глава 5. Предельные теоремы теории вероятностей ■ 163
□	Докажем неравенство (5.1) для непрерывной с. в. X с плотностью
f{x). Вероятность Р{|А" — а\ ^ е} есть вероятность попадания с. в. X в
область, лежащую вне промежутка [а — е, а + е]. Можно записать
Р{\Х - а| ^ £} = j' f(x) dx + j' f(x) dx = j f{x) dx =
—oc	a\-e	\x—a\^e
= I A -f(x)dx^ j — f(x) dx,
|a? -a\^e	\x—a\^e
так как область интегрирования \х — а| ^ е можно записать в виде
/	9	(Х ~~ а)2 тт
(х — а) ^ € , откуда следует 1 ^ 			. Имеем
ег
ос
Р{\Х - о| ^ е} ^ у (х - affix) dx ^ -L j" (х - affix) dx,
\ха\^£	— оо
так как интеграл от неотрицательной функции при расширении обла¬
сти интегрирования может только возрасти. Таким образом,
ОС
Р{\Х - а\ > £} ^ i J (х- affix) dx = ±DX,
т.е. Р{\Х - а\ > е} ^ Щ-.
Е1
Аналогично доказывается неравенство Чебышева и для дискрет¬
ной с. в. X, принимающей значения х\, хч* жз,... с вероятностями р\,р2,
Рз,..только интегралы (вида ^	)	заменяются	соответствующими
\х—а\^е
суммами (вида	).	■
\xi~ а\^е
Отметим, что неравенство Чебышева можно записать в другой
форме:
Р{\Х-МХ\<е}>\-Щ^.	(5.2)
В форме (5.2) оно устанавливает нижнюю границу вероятности собы¬
тия, а в форме (5.1) — верхнюю.

164 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Неравенство Чебышева справедливо для любых с. в. В частности,
для с. в. X = т, имеющей биномиальное распределение с математиче¬
ским ожиданием MX — а = пр и дисперсией DX = npq (п. 2.7), оно
принимает вид
Р{\т - пр\ < е} > 1 -	(5.3)
для частости Щ- (или п. 1.5) события в п независимых испы¬
таниях, в каждом из которых оно может произойти с вероятностью
р = М(^-)=а, дисперсия которых D (Щ) = неравенство Чебыше¬
ва имеет вид
<м)
Оценку вероятности попадания с. в. X в промежуток [е, оо) дает
неравенство Маркова.
Теорема 5.2 (Неравенство Маркова), Для любой неотрицательной
с. в. X, имеющей м. о. MX и е > 0, справедливо неравенство:
Р{Х > е} <	(5.5)
□	Р{Х ^ е} = J f(x) dx ^ J f/(ж) dx = | J xf(x)dx^
/»/<»> dr-W
О
Неравенство (5.5) можно записать в форме
Р{* < е} > 1 -	(5.6)
Пример 5.1. Оценить с помощью неравенства т1ебышева вероятность
того, что отклонение с. в. X от своего м. о. будет меньше трех с. к. о.,
т. е. меньше 3<тх.
о	Полагая е = 3<тж в формуле (5.2), получаем
2
Р{|Х - МХ| < За*} > 1 - -^-2 =1-1 = |» 0,8889.
(Заж)	У	У
Эта оценка, как известно (п. 2.7), называется правилом трех сигм; для
с. в. X ~ iV(a, а) эта вероятность равна 0,9973.	•

Глава 5. Предельные теоремы теории вероятностей ■ 165
5.2.	Теорема Чебышева
Основное утверждение ЗБЧ содержится в теореме Чебышева,. В ней
и других теоремах ЗБЧ используется понятие «сходимости случайных
величин по вероятности».
Случайные величины Х\, Х2,..., Хп,... сходятся по вероятности
к величине А (случайной или неслучайной), если для любого е > О
вероятность события {\Хп — А\ < е} при п —> оо стремится к единице,
т. е.
(или Р{\Хп — А\ <£:}—> 1). Сходимость по вероятности символически
записывают так:
Следует отметить, что сходимость по вероятности требует, чтобы
неравенство \Хп — А\ < е выполнялось для подавляющего числа членов
последовательности (в математическом анализе - для всех п > 7V, где
N — некоторое число), а при п -* схэ практически все члены последо¬
вательности должны попасть в е-окрестность А.
Теорема 5.3 (ЗБЧ в форме П. Л. Чебышева, 1886 г.). Если случай¬
ные величины Х\, Х2, •.., ХП7 ... независимы и существует такое число
С > 0, что DXi ^ (7, г = 1,2,..., то для любого е > О
т. е. среднее арифметическое этих случайных величин сходится по веро¬
ятности к среднему арифметическому их м.<>.:
71—^00
lim Р{\Хп - А\ < е} = 1
р
П-* оо
(5.7)
71
п

166 " Раздел первый. Элементарная теория вероятностей и случайных процессов
= ±(DXl+DX2 + ... + DXn)^±(C + C + ... + C) = ±Cn = £
п	nz	гг
Тогда, применяя к с. в.
* = ktn
г—1
неравенство Чебышева (5.2), имеем
'(*£*)
е	пе
Переходя к пределу при п —> оо и учитывая, что вероятность любого
события не превышает 1, получаем
р{|йЁ^-йЁмх(|<Л = 1.	а
U г—1	i—\	1	}
Следствие. Если с. в. Х\ , Х2, .... Хп, ... независимы и одинаково
распределены, MXi = а,	=	а2,	то	для	любого 6 > О
(5.9)
т. е. среднее арифметическое с. в. сходится по вероятности к математи¬
ческому ожиданию а:
п
р
г=1
Q Так как
п
i	MXi = ±(МХ1+МХ2 + .. . + МХп) = ±(e+a+. ..+а) = ±-rm =
а дисперсии с. в. Xj равны числу а2, т. е. ограничены, то, применив ЗБЧ
в форме Чебышева (5.7), получим утверждение (5.9).	■
Следствие (5.9) теоремы Чебышева обосновывает «принцип сред¬
него арифметического с. в. X*», постоянно используемый на практике.
Так, пусть произведено п независимых измерений некоторой величи¬
ны, истинное значение которой а (оно неизвестно). Результат каждого

Глава 5. Предельные теоремы теории вероятностей ■ 167
измерения есть с. в. AV Согласно следствию (5.9), в качестве прибли¬
женного значения величины а можно взять среднее арифметическое
результатов измерений:
Равенство тем точнее, 'тем больше п.
На теореме Чебышева основан также широко применяемый в ста¬
тистике выборочный метод, суть которого в том, что о качестве боль¬
шого количества однородного материала можно судить по небольшой
его пробе.
Теорема Чебышева подтверждает связь между случайностью и не¬
обходимостью: среднее значение случайной величины
Пример 5.2. Глубина моря измеряется прибором, не имеющим систе¬
матической ошибки. Среднее квадратическое отклонение измерений не
превосходит 15 м. Сколько нужно сделать независимых измерений, что¬
бы с вероятностью не меньшей 0,9, можно было утверждать, что сред¬
нее арифметическое этих измерений отличается от а (глубины моря)
по модулю меньше, чем на 5 м?
о	Обозначим че|жз результаты п независимых измерений глубины
моря. Нужно найти число п, которое удовлетворяет неравенству (5.8):
где М Х{ = а, что означает отсутствие при измерениях системати¬
ческой ошибки (т. е. измерения производятся с одинаковой точностью).
По условию е = 5, С = 225, так как а = yDX = 15м'. Отсюда
Т1
г=1
п
г= 1
практически не отличается от неслучайной величины,
п
т. е. 0,1 ^ п ^ 9U. Измерение нужно проводить не менее 9U раз. •

168 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
5.3.	Теорема Бернулли
Теорема Бернулли исторически является первой и наиболее про¬
стой формой закона больших чисел. Она теоретически обосновывает
свойство устойчивости относительной частоты (см. п. 1.5).
Теорема 5.4 (ЗБЧ в форме Я. Бернулли, 1713 г.). Если вероятность
появления события А в одном испытании равна р, число наступления
этого события при п независимых испытаниях равно пд, то для любого
числа е > 0 имеет место равенство
lim P<fe- р| <Л = 1,	(5.10)
11 —> ОО М	I
т. е. относительная частота Р*(А) события А сходится по вероятности к
Р
вероятности р события А: Р*(А) 	> Р(А).
ГО-* ОО
Q Введем с. в. Х\, Х2,..., Хп следующим образом: Xi~ 1, если в г-м
испытании появилось событие А, а если не появилось, то Х{ = 0. Тогда
число па (число успехов) можно представить в виде
п
ПА = ^2
М. о. и дисперсия с. в. Х? равны: MXi = 1 - /у + 0 ♦ (1 — р) — р,
DXi = (0 — р)2( 1 — р) + (1 — р)2р — р( 1 - р) = pq. Закон распределения
с. в. Хг имеет вид		
Xi
0
1
Р
1 -р
р,
при любом г. Таким образом, с. в. Х{ независимы, их дисперсии огра¬
ничены одним и тем же числом так как
p(i -р)=р-р2 = \ - (р-
Поэтому к этим с. в. можно применить теорему Чебышева (5.7):
< £ j = 1.
lim Р\
п—> оо

Глава 5. Предельные теоремы теории вероятностей *169
Но
п
?,= 1 ?,= 1
Следовательно, lim Р \ ^ — р < £* > = 1.	■
71—^ОО U П	I	)
Теорема Бернулли теоретически обосновывает возможность при¬
ближенного вычисления вероятности события с помощью его относи¬
тельной частоты. Так, например, за вероятность рождения девочки
можно взять относительную частоту этого события, которая, соглас¬
но статистическим данным, приближенно равна 0,485.
Неравенство Чебышева (5.2) для случайных величин
п
ПА = ^2 Xi
г=1
принимает вид
<*•»>
Обобщением теоремы Бернулли на случай, когда вероятности щ
появления события А в каждом из п испытаний различны, является
теорема Пуассона:
Р*(Л)-^-»1£>,	(5.12)
п—>оо 11 z—'
i—l
где pi -— вероятность события А в г-м испытании.
Пример 5.3. Вероятность наличия опечатки на одной странице руко¬
писи равна 0,2. Оценить вероятность того, что в рукописи, содержащей
400 страниц, частость появления опечатки отличается от соответству¬
ющей вероятности по модулю меньше, чем 0,05.
О	Воспользуемся формулой (5.11). В данном случае р = 0,2, q = 0,8,
п = 400, е = 0,05. Имеем
р{|^-0,2|<0,°5}> [l-ij
т.е. р^ 0,84.
= 1_j^
400 • 0,05

170 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
5.4.	Центральная предельная теорема
Центральная предельная теорема (ЦПТ) представляет собой вто¬
рую группу предельных теорем, которые устанавливают связь между
законом распределения суммы с. в. и его предельной формой — нор¬
мальным законом распределения.
Сформулируем ЦПТ для случая, когда члены суммы имеют одина¬
ковое распределение (именно.эта теорема чаще других используется на
практике, так в математической статистике выборочные случайные ве¬
личины имеют одинаковые распределения, так как получены из одной
и той же генеральной совокупности).
Теорема 5.5. Пусть с* в. Х2, ..., Хп независимы, одинаково распре¬
делены, имеют конечные математическое ожидание MXi — а и диспер¬
сию DXi = сг2, г = 1 ,п. Тогда функция распределения центрированной и
нормированной суммы этих случайных величин стремится при п -4 ос к
функции распределения стандартной нормальной случайной величины:
Т1	/ п	\	71
£ Xi - па
гу	i— 1	\г=1	/	г-1
Zjti
X	2
FZrXx) = P{Zn<x)	>Ф(Ж) = -кг [ e' V dt.
n->оо	V27T	J
—оо
Из соотношения (5.13) следует, что при достаточно большом п сумма Zn
приближенно распределена по нормальному закону: Zn ^ N(0,1). Это
означает, что сумма Sn = Х\ + Х2 4-... + Хп приближенно рас пределена
по нормальному закону: Sn ~ N(na, у/по). Говорят, что при п —оо с. в.
п
Y; Xi асимптотически нормальна.
i— 1
СГу/п
(5.13)
Напомним, что:
1.	С. в. X называется центрированной и нормированной (т. е. стан¬
дартной), если MX = 0, a DX = 1.
2.	Если с. в. Xi, г = l,7i независимы, МХ( = a, DXi — v1, то

Глава 5. Предельные теоремы теории вероятностей ■ 171
= yZDXi = о2 + о2 + ■ ■ ■ + о2 = п°2 ■
4=1	'
X
1	f —— 1
3.	Ф(ж) = ——:	е 2 dt — функция Лапласа; Ф(а:) = 7,+ Фо(#)^ ГДО
у 2т: J	^
—оо
X
1	f
ф0(х) = - — / е 2 dt — нормированная функция Лапласа.
\[Ък J
о
Формула (5.13) позволяет при больших п вычислять вероятности
различных событий, связанных с суммами случайных величин. Так,
перейдя от с. в.
71
г—1
к стандартной с. в., получим:
71
( п	^	—	па,
—	<	Р—рЛ	«
I	J	I	<чт	O v/n J
V	Gyjn )	\	Oyfn	J
или
P{c < 5„ < Д » Ф (M) - Ф (^) -	(..14,
формула для определения ве^юятности того, что сумма нескольких с. в.
окажется в заданных пределах (см. (2.43) и (2.46)). Часто ЦПТ исполь¬
зуют, если п > 10.
Пример 5.4. Независимые с. в. Xi распределены равномерно на отрез¬
ке [0,1]. Найти закон распределения с. в.
100
у = Т,х*’
i= 1
а также вероятность того, что 55 < Y < 70.
о Условия ЦПТ соблюдаются, поэтому с. в. Y имеет приближенно
плотность распределения
(у - ту )2
fy(y) « tJL- е 2(ТУ .
\fbii
'KOni

172 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
По известным формулам для м. о. и дисперсии в случае равномерного
распределения находим: MXi — 0-^-- = DXi = -—
crXi = —L= =	Тогда
л/12	2v/3
,100	ч	100
ту = М ( xi) = XIMXi = ЮО • | = 50,
4=1	'	г=1
(100	ч	100
£х<)=£м. = юо -£ = f.
г=1	7	г=1
аУ
5\/3 п
о у = —. Поэтому
3(у - 50)2
/у (у) ~ Z~7=e	50	■
OVD7T
Используя формулу (5.14), находим
Р{55 < У < 70} = Ф |	-	ф	\^77Г	j = Ф(4^} " Ф(^} =
= Ф(6,9282) - Ф(1,73) « 0,04.
т. е. Р{55 < Y < 70} « 0,04.
5.5.	Интегральная теорема Муавра-Лапласа
Следствиями ЦПТ являются рассмотренные ранее (п. 1.21) локаль¬
ная и интегральная теоремы Муавра Лапласа. Приведем вывод инте¬
гральной теоремы Муавра Лапласа.
Рассмотрим схему Бернулли. Пусть пд — число появления собы¬
тия А (число успехов) в п независимых испытаниях, в каждом из кото¬
рых оно может появиться с вероятностью р (не появиться — с вероят¬
ностью q — 1 — р). Случайную величину пд можно представить в виде
суммы п независимых с. в. Xi, Х2,..., Хп таких, что Х{ = 1, если в г-м
испытании событие А наступило, и Xi ~ 0 в противном случае, т. е.
п
ПА = YlXi-
i—1

Глава 5. Предельные теоремы теории вероятностей ■ 173
Так как MXi — р, DXi = pq (п. 5.3), то
МпА = м(^2хг\ =пр. DnA =	=
'*=] ' '*=1 '
npq
(да это уже давно известно, см. (2.24), ведь с. в. па имеет биномиальный
закон распределения). Тогда с. в.
11	/ п	\
г-1,	\г=1	/
у   г=1,	\г—1	/	  Па ^Р
"~~	'	~	s/m
ID\ ~	‘
{?/•)
представляет также сумму п независимых, одинаково распределенных
случайных величин. При этом Zn ~ N(0.1), действительно:
пр - пр = 0
np#
Следовательно, на основании ЦПТ (5.13) с. в. Zn при большом чи¬
сле п имеет приближенно нормальное распределение. Согласно свой¬
ству (2.46) нормального закона, записываем
P{z} ^ Zn ^ z2) « ф (^f^) - Ф	-	Ф(*1)
(Ф(^) — функция Лапласа). Полагая
к\ — пр	к,2	—	пр
zi = —^2 = ——
Jnpq
двойное неравенство в скобках
к] - пр ^ па - пр ^ к,2 — пр
л/гЩ ^ у/ЩЩ ^
можно переписать в эквивалентном виде к\ ^ пд ^ к-2• Таким образом,
получаем
Р{&1 ^ пд ^ /с2} = Ф{г2) - Ф(гх),
т. е. интегральную формулу Му авра-Лапласа.

174 а Раздел первый. Элементарная теория вероятностей и случайных процессов
Заметим, что: 1. Интегральная предельная теорема Муавра-Ла¬
пласа
(м5)
для схемы Бернулли непосредствнно вытекает из ЦПТ (5.13) с учетом
п
результатов, полученных в и. 5.3 (пд =	MXi	=	р,	ЮХг	=	pq:
г=1
тогда па = пр, а^/п = уДщу/п, — ^npq).
2.	Для подсчета сумм биномиальных вероятностей можно восполь¬
зоваться приближенной формулой
А:—О
Действительно,
У] /п,* = р W < rn} = Р{-ос <пА 4 т} =
к=о
„ Г пд — пр т-?гр) _/т-пр\	,,
= рГ<7й<^Г*ш)^м =
= ф/т-пр\
V	/
где Ф(.г) - функция Лапласа.
Пример 5.5. Машинистке требуется напечатать текст, содержащий
8000 слов, состоящих из четырех и более букв. Вероятность сделать
ошибку в любом из этих слов равна 0,01. Какова вероятность, что при
печатании будет сделано не болею 90 ошибок?
Q Применим формулу (5.16). Так как п ~ 8000, р = 0,01, q = 0,99,
т = 90, то
1 - 1 - 1 ~ 0.112.
y/rm v/8000 • 0,01 - 0,99 у/Ш
т - пр ю
у/гЩ 8,9
1,12,
уи
Ф(1,12) = 0,8686. Следовательно, Р{пд ^ 90} = Рп^ « 0,869.
к—О

Глава 5. Предельные теоремы теории вероятностей ■ 175
Упражнения
1	- Оценить с помощью неравенства Чебышева вероятность того, что:
а)	при бросании монеты 500 раз число выпадений герба будет за¬
ключено между 200 и 300; б) при бросании 10 игральных костей
сумма очков отклонится от м. о. меньше, чем на 8.
2.	Дисперсия каждой из данных независимых случайных величин не
превышает 5. Найти число этих величин, при котором вероятность
отклонения их средней арифметической от средней арифметиче¬
ской их м. о. менее чем на 0,1 превысит 0,9.
3-	Оценить вероятность того, что при бросании монеты 500 раз ча¬
стость появления герба отклонится от вероятности появления гер¬
ба при одном бросании по модулю менее чем на 0,1.
4.	Стрелок попадает при выстреле в мишень в десятку с вероятностью
0,5, в девятку — 0,3, в восьмерку — ОД, в семерку — 0,1. Стрелок
сделал 100 выстрелов. Какова вероятность того, что он набрал не
менее 940 очков?
5- Приживаются в среднем 70% числа посаженных саженцев. Сколь¬
ко нужно посадить саженцев, чтобы с вероятностью не меньшей 0,9
ожидать, что отклонение числа прижившихся саженцев от их м. о.
не превышало по модулю 40? Решить задачу с помощью неравен¬
ства Чебышева.

Глава 6
Основы теории случайных процессов
6.1.	Понятие случайной функции (процесса)
При изучении ряда явлений часто приходится иметь дело со слу¬
чайными величинами, изменяющимися в процессе наблюдения (опыта
испытания) с течением времени. Примерами таких случайных величин
могут служить: сигнал на выходе радиоприемника под воздействием
помех, загруженность студентов в семестре, длина очереди за билетом
в «Ленком», колебания напряжения в сети, рейтинг политической ищу
тии, траектория частиц в броуновском движении и т. д.
Такие случайные величины, изменяющиеся в процессе опыта, на¬
зывают случайными функциями.
Раздел математики, изучающий случайные явления в динамике их
развития, называется теорией случайных функций (случайных процес¬
сов). Ее методы используются, в частности, в теории автоматическо¬
го управления, при анализе и планировании финансовой деятельности
предприятий (и отдельной семьи), при обработке и передаче сигналов
радиотехнических устройств, в экономике, в теории массового обслу¬
живания.
Рассмотрим основные понятия теории случайных процессов (с. п.).
Если каждому значению t € Т, где Т — некоторое множество дей¬
ствительных чисел, поставлена в соответствие с. в. X(t), то говорят,
что на множестве Т задана случайная функция (с. ф.) X(t). Другими
словами, случайной функцией X(t) называют с. в., зависящую от не¬
случайного аргумента t.
Если параметр t интерпретируется как время, то с. ф. называется
случайным прогрессом. Другими словами, случайным процессом назы¬
вается семейство случайных величин X(t, ы), заданных на одном и том
же пространстве элементарных событий 1), зависящих от параметра
t Е Т. Обозначается через X(t,u>) или X(t), Xt.
Случайный процесс можно задать формулой, если вид случайной
функции известен, а случайные величины, определяющие параметры
с. ф., можно задать аналитически. Так, с. ф. X(t) = Y • sint, где t ^ О,

Глава 6. Основы теории случайных процессов ■ 177
Y ~ Д[0; 1] — с. в., имеющая равномерное распределение, является слу¬
чайным процессом.
При фиксированном значении t, т. е. при t = to € Т, случайный
процесс X(t,u>) обращается в с. в. X(to,u;), называемую сечением слу¬
чайного прогресса.
Реализацией или траекторией случайного процесса X(t,cj) назы¬
вается неслучайная функция времени x(t) = X{t,uJo) при фиксирован¬
ном ш = а;о, т. е. конкретный вид, принимаемый случайным процессом
в результате испытания.
Реализации с. п. обозначают через x\(t), Х2(£),..., где индекс ука¬
зывает номер опыта.
На рис. 59 показаны три реализации #i(£), хч(£), x$(t) случайного
процесса при oj = cji, ш = и?2* и = (они напоминают траектории
трех приходов домой подвыпившего после работы грузчика). Каждая
такая реализация (траектория) является обычной функцией x(t). В ви¬
де жирных точек на рисунке изображены значения с. в. X(tu,u) в трех
опытах.
Так, если в рассмотренном выше примере с. в. У в первом опыте
приняла значение 1, во втором — 2, в третьем — то получим три
реализации с.п.: x\(t) = sin£, Х2(t) = 2sin£, xs(t) = ^sint — неслучай¬
ные функции. Если же в этом примере зафиксировать момент времени,
например t = то получим сечение: X т*е* X = -Y — слу¬
чайная величина.
В заключение параграфа отметим, что функция Ft(x) —
= P{X{t) < х} — так называемый одномерный закон распределения
случайного прогресса X(t) — не является исчерпывающей характери¬
стикой случайного процесса. Случайный процесс X(t) представляет со¬
бой совокупность всех сечений при различных значениях ^ G Т, поэто¬
му для его полного описания надо рассматривать совместную функцию
распределения сечений процесса:
■Ftl,t2,...,*nOcbx‘2, •••,£«) = P{X(ti) < Xi,X(t2) < Х2 ,...,X(t,n) < хп} —

178 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
так называемый конечномерный закон распределения с. п. в моменты
ii, t2		 те- рассматривать многомерную с.в. (X(ti), Xfa),
X(tn)).
Таким образом, понятие случайного процесса является обобщением
понятия системы случайных величин, когда этих величин — бесконеч¬
ное множество.
6.2.	Классификация случайных процессов
Случайный процесс, протекающий в любой физической системе S,
представляет собой случайные переходы системы из одного состояния в
другое. В зависимости от множества этих состояний Иг, от множества Т
значений аргумента t, все случайные процессы делят на классы:
1.	Дискретный процесс (дискретное состояние) с дискретным време¬
нем.
2.	Дискретный процесс с непрерывным временем.
3.	Непрерывный процесс (непрерывное состояние) с дискретным вре¬
менем.
4.	Непрерывный процесс с непрерывным временем.
В 1-м и 3-м случаях множество Т дискретно, т. е. аргумент t при¬
нимает дискретные значения to,	•	• * (обычно t = 0,1.2.3,...); в 1-м
случае множество значений случайной функции X(t), т.е. X(t,o) — .то,
X(t\) = .£i,... — дискретное множество (множество W конечно или
счетно), а в 3-м случае — множество W несчетно, т. е. сечение случай¬
ного процесса в любой момент времени t представляет собой непрерыв¬
ную случайную величину.
Во 2-м и 4~м случаях множество Т непрерывно, во 2-м случае
множество состояний системы W конечно или счетно, а в 4-м случае —
W несчетное.
Примерами случайных процессов 1- 4 классов являются соответ¬
ственно:
1.	Футболист может забить или не забить один или несколько голов
в ворота соперника во время матчей, проводимых в определенные
моменты (согласно расписанию игр) времени ti,i2?	Случайный
процесс X (/) — число забитых голов до момента t.
2.	Х(£) — число просмотренных телепрограмм на первом канале от
начала работы телевизора до момента £.

Глава 6. Основы теории случайных процессов ■ 179
3.	В определенные моменты времени £о?*ъ^2	измеряется темпера¬
тура Y(t) заболевшего человека. Y(t,) — случайный процесс непре¬
рывного типа с дискретным временем.
4.	Давление воздуха P(t) в шине колеса автомобиля.
Отметим, что рассматриваются и другие, билее сложные классы
случайных процессов. Для различных классов случайных процессов
разработаны различные методы их изучения.
6.3.	Основные характеристики случайного процесса
Для случайного процесса также вводятся пр(х:тейшие характери¬
стики, аналогичные основным характеристикам случайных величии.
Знание их оказывается также достаточно для решения многих задач
(напомним, полная характеристика случайного процесса дается ее мно¬
гомерным (конечномерным) законом распределения). В отличие от чи¬
словых характеристик с. в., представляющих собой определенные чи¬
сла, характеристики с. и. представляют собой в общем случае не числа,
а функции.
Математическое ожидание случайного процесса
Математическим, ожиданиелt с. п. X(t) называется неслучайная
функция тх(£), которая при любом фиксированном значении аргу¬
мента t равна математическому ожиданию соответствующего сечения
случайного процесса:
Математическое ожидание с. и. обозначают кратко и гак: m(<), a(t).
Функция m\(t) характеризует поведение с. п. в среднем. Геомет¬
рически математическое ожидание m(t) истолковывается как «сред¬
няя кривая», около которой расположены кривые — реализации (см.
Опираясь на свойства математического ожидания случайной вели¬
чины и учитывая, что X(t) - случайный процесс, a f(t) — неслучайная
функция, получаем свойства математического ожидания случайного
процесса:
тх(Т) = MX(t).
(6.1)
рис. 60).

180 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
1.	Математическое ожидание неслучайной функции равно самой
функций, т. е.
М (/(*)) = f(t).
2.	Неслучайный множитель можно выносить за знак математического
ожидания случайного процесса, т. е.
м (/(*) • X(t)) = f(t) • inx(t).
3.	Математическое ожидание суммы (разности) двух случайных про¬
цессов равно сумме (разности) математических ожиданий слагае¬
мых, т. е.
М (X(t) ±Y(t)) = rrix(t) ± rny(t).
Отметим, что, зафиксировав аргумент t и переходя от случайного
процесса к случайной величине (т. е. к сечению случайного процесса),
можно найти математическое ожидание этого процесса.
Так, если сечение с. п. X(t) при данном t есть непрерывная с. в. с
плотностью f(t,x), то его математическое ожидание можно найти по
формуле
сю
MX(t) = J X ■ f(t. x) dx.	(6.2)
—OO
Пример 6.1. Случайный процесс определяется формулой
Y(t) = X ■ е“*, t > О, X ~ N(3; 1),
т. е. X — с. в., распределена по нормальному закону с а = 3, о = 1.
Найти математическое ожидание случайного процесса Y(t).

Глава 6. Основы теории случайных процессов ■ 181
о Так как f(x) =
находим:
1 • \/2тг
(*-з)2
е 2 • L2 ? то, используя формулу (6.2),
ту
х • р
■(<)= [ х • е~1 - ^	dx	=- е~1 [
J	у/2п	\/bi J
—ос	— оо
=	4=	/ (ж - 3 + 3)	• е V V2	;	dx =
%/2тг J
—ОО
_, / г	? -f—V
=	у	у	(ж	—	3)	•	е	V	\/2	/	^	-f	3	J	е. Л \/2 / dx
— ОО	—оо
-С—У
• Р \ у/2 ) (1х =
n-t
-1
\/2 • \/7Г I (*~3)2
\е 2
+ 3 • >/2 • v^r 1 —
(О + 3 * л/2 • у/п) = 3 • е
т. е. ?тгу(£) = 3 • е *.
Приведем другое решение: согласно свойству 2
my(t) = М (X • е“*) - е“* • МХ,
но X ~ iV(3; 1), следовательно, МХ = 3 и my(t) = Зе“4.
Дисперсия случайного процесса
Дисперсией случайного процесса X(t) называется неслучайная
функция Dx{t), которая при каждом значении t равна дисперсии соот¬
ветствующего сечения:
Dx(t) = £>X(t) - MX2{t) - m2x{t).	(6.3)

182 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Дисперсия D\(t) с. п. характеризует разброс (рассеяние) возмож¬
ных значений с. п. относительно его математического ожидания.
Наряду с дисперсией с. п. рассматривается также среднее квадра¬
тическое отклонение о(£) (коротко с.к.о.), определяемое равенством:
<rx{t) =	(6.4)
Размерность функции стх(£) равна размерности с. п. X(t).
Значения реализаций с. п. при каждом t отклоняются от математи¬
ческого ожидания mx(t) на величину порядка cr\(t) (см. рис. 60).
Очевидны следующие свойства дисперсии с. п.
1.	Дисперсия неслучайной функции равна нулю, т. е.
D(f(t))= 0.
2.	Дисперсия с. п. неотрицательна, т. е.
Dx(t) = a2x(t).
3.	Дисперсия произведения неслучайной функции на случайную
функцию равна произведению квадрата неслучайной функции на
дисперсию случайной функции, т. е.
DX (J(t)-X(t)) = f(t)-DxX(t).
4.	Дисперсия суммы с. п. и неслучайной функции равна дисперсии
с. п., т. е.
Dx(X(t)±f(t)) = Dx(t).
Пример 6.2. Используя условие примера 6.1, найти дисперсию и сред¬
нее квадратическое отклонение с. п.
о Находим дисперсию, используя ее свойство 3: Dy(t) = D(X *е *) =
= (е~1)2 * DX — e~2t • DX. Но X ~ N(3,1), следовательно, [DX = сг2,
см. с. 99] DX = I2 = 1.
Поэтому Dy(t) = e~2t • 1 = e-2t, т.е. Dy(t) ~ e~2t.
ay(t) = \fe~2i — e т. e. ay(t) = e f.

Глава 6. Основы теории случайных процессов ■ 183
Корреляционная функция случайного процесса
Математическое ожидание и дисперсия не являются исчерпываю-
щими характеристиками случайного процесса.
В частности, зная математическое ожидание и дисперсию, ничего
нельзя сказать о зависимости двух (и более) сечений с. п.
Для определения связи между различными сечениями с. п. исполь¬
зуется корреляционная функция — аналог ковариации
КХу = М((X - тх) ■ (У - my)) = MXY - тх ■ ту,
характеризующей степень связи между двумя с. в. X и Y.
Р\1	Корреляционной	(ковариационной, автоковариационной, автокор¬
реляционной) функцией с. п. X(t) называется неслучайная функция
двух аргументов	которая	при	каждой	паре значений t\ и
#2 равна корреляционному моменту (ковариации) соответствующих се¬
чений X(t\) и X(t2):
Kx(h;t2) = M({X(h) -m(ti)) • (X(h) -m(t.2)))
ИЛИ
Kx(ti;t2) = M(X(h) • X(h)) = M(X(U) • X(t2)) - mx(ti) • mx(h),
где X(t) — X(t) — m(t) — центрированная случайная функция.
Приведем основные свойства корреляционной (автоковариацион¬
ной) функции K\(ti;t2) случайного процесса X(t).
1.	Корреляционная функция при одинаковых значениях аргументов
равна дисперсии с. п., т. е.
Kxfct) = Dx{t).
□	Действительно.
Kx(t;t)=cov{X(t),X(t)) = м((Х(*) -rn(t)) • (X(t) -m(t))) =
= M{X(t)~m(t))2 = Dx(t). m
Это свойство позволяет считать математическое ожидание и корре¬
ляционную функцию главными характеристиками с. п.; необходимость
в дисперсии отпадает.

184 * Раздел первый. Элементарная теория вероятностей и случайных процессов
2.	Корреляционная функция не меняется при перестановке аргумен¬
тов местами, т. е.
Kx(t 15*2) = Kx(t2]h).
Это свойство непосредственно вытекает из определения корреля¬
ционной функции.
3.	Если к с. п. прибавить неслучайную функцию, то корреляционная
функция не изменится, т. е. если У(£) = X(t) + /(£), то
Ky(ti;t2) — K\(ti;t2)-
□	Так как my(t) = nix(t) + /(£), то
Y(t) - mY(t) = X(t) + /(£) - mx{t) - /(t),
т.e. Y(t) — my(t) = X(t) — mx(t). Отсюда следует, что Ky(ti;t,2) -
= Kx{ti]t2).	■
4.	Модуль корреляционной функции не превосходит произведения
среднеквадратических отклонений, т. е.
|^А'(*1?*2)| ^ yjDx{t\) • Д\-(*2) — Vx(t\) • ОЛ'(Ы*
Свойство 4 непосредственно вытекает из соответствующего свой¬
ства корреляционного момента двух с. в. (см. п. 3.7) с учетом пер¬
вого свойства корреляционной функции с. п. X(t).
5.	При умножении с. п. X(t) на неслучайный множитель /(£) ее кор¬
реляционная функция умножится на произведение f(t\) * /(£2), т-е
если У(£) = X(t) • /(£), то
Ky(ti]t2) = /(ti) • /(£2) • #дг(*1;*2)-
Наряду с корреляционной функцией с. п. рассматривается также
нормированная корреляционная функция (или нормированная авто-
ковариационная функция) rx{ti;t2), определяемая равенством
/j. ± \ Kxfah)
rx(tx-,t2) =
ax(ti) -0x(h)'
С учетом свойства 1 можно записать так:
rx(h;t2) =
VKx{h',ti) ■ у/Кх(Ь;Ь)

Глава 6. Основы теории случайных процессов ■ 185
По своему смыслу rx (ti; t2) аналогична коэффициенту корреляции для
с. в., но не является постоянной величиной, зависит от аргументов
и t2.
Свойства нормированной корреляционной функции аналогичны
свойствам коэффициента корреляции (см. п. 3.7):
1- |rx(ti;fc)| < 1;
2.	rx(t]t) = 1;
3.	rxih'M) = r\(<2;<i)-
Пример 6.3. Используя условие примера 6.1, найти корреляционную
и нормированную корреляционную функций с. п. Y(t).
О Действительно,
KY{ti;t2) = М((Х ■ e~tl - 3 • e~tl) ■ (X ■ e~t2 -3-е~*2)) =
= М(Х2 ■ e~tl ■ ert2 - 6Х ■ e~li ■ e~l2 + 9е~11 • e~t2) =
= M(e~tl ■ e~t2 ■ (X - 3)2) = e~h • e_t2 • DX = e_tl • e~<2 • l2 = e_(tl+t2),
т.е. Ky(t\-,t2) = e~^tl+t2\
Тогда ryih^h) = \_ti	=	1,	т.е.	ry{ti;*2) = 1-
При решении примера использовались определения корреляцион¬
ной и нормированной корреляционной функций, а также результат ре¬
шения примера 6.2.	•
Взаимная корреляционная функция случайного процесса
Для определения степени зависимости сечений двух случайных
процессов используют корреляционную функцию связи или взаимную
корреляционную функцию.
Рч|	Взаимной	корреляционной	функцией двух с. п. X(t) и Y(t) называ¬
ется неслучайная функция Rxy{h; ^2) двух независимых аргументов t\
и £2, которая при каждой паре значений t\ и t2 равна корреляционному
моменту двух сечений X(t\) и Y(t2):
RxY(U;t2) = м((Х(*1)-тл-(<1))-(У(Ы-^к(*2))) =	Y(t2)).

186 в Раздел первый. Элементарная теория вероятностей и случайных процессов
Два с. п. X(t) и Y(t) называются некоррелированными, если их
взаимная корреляционная функция тождественно равна нулю, т.е.
Rxy(t\\t2) = о для любых и t2. Если RxYit^tz) Ф 0. то с. п. X(t) и
Y(t) называются коррелированным,и (или связанными).
Свойства взаимной корреляционной функции непосредственно вы¬
текают из ее определения и свойств корреляционного момента (см.
н. 3.7):
1.	При одновременной перестановке индексов и аргументов взаимная
корреляционная функция не меняется, т. е.
Rxvih'h) = Ryx{t 2]ti).
2.	Модуль взаимной корреляционной функции двух с. и. не превыша¬
ет произведения их средних квадратических отклонений, т.е.
|Ялу(*1;*2)| < 0\y(*i) -<7у(*2)-
3.	Корреляционная функция не изменится, если к с. п. X(t) и Y(t)
прибавить неслучайные функции /(£) и <p(t,) соответственно, т.е.
RxiYiiii'ih) = R\y{ti',t2)i
где X^t) - X(t) + /(t), Yi(t) - Y(t) + tp{t).
4.	Неслучайные множители можно вынести за знак корреляции, т.е.
если A"i(t) — f(t) • X(t)y Yi(t) = <p(t) * Y(£), to
RxiYiitilfa) = f{t l) • у>(*2)Дхт(*1; t2).
5.	Если X(t) = Xi (/,) + X2(1), *ro
Kx{h‘,h) = Kxi (tj; *2) + K;x2{ti;t2) + #,YbY2l*i^2) + Rx2Xi (h]h)-
6.	Если с. и. X\ (t) и Лг2(<) некоррелированы. то корреляционная функ¬
ция их суммы равна сумме их корреляционных функций:
Kx{U\t2) - KxiiU'J^) + Kx2(ti]t2).
Для оценки степени зависимости сечений двух с. и. используют
также нормированную взаимную корреляционную функцию rxy(ti\t2),
определяемую равенством:
г y(i 't)~	Rxy(h^t2)	_	Rxv(ti]h)	
ГЛ> Ь 2 ~ y/Dx(ti)-Dy(t2) ~ s/Kx(h;ti) • s/KY(tr,h)

Глава 6. Основы теории случайных процессов "187
Функция rxy(t\;t2) обладает теми же свойствами, что и функция
Rxy{t 1^2)5 но свойство 2 заменяется на следующее: \rxy{ti] fa)\ ^ 1.
т. е. модуль нормированной взаимной корреляционной функции не пре¬
вышает единицы.
[яГ| Пример 6.4. Найти взаимную корреляционную функцию двух с. п.
X(t) — t • V и Y(t) — {t-1- 2)V4 где V — с. в., причем DV = 3.
О Так какту(^) = M(t-V) = t-MV = t-my. amy(t) = М((£+2)-Т^) =
= (£ + 2) • mv'. to
Я\т(ti; £2) = M (tiV — t\my) ((^2 + 2)V — (<2 + 2)mv) =
= M{h(v - mv-)) ((<2 + 2)(V - mv)) = ti • (<2 + 2) • М(Г - mv )2 =
— t\ • (^2 2)DV =	•	(£2	H-	2),
т.e. RxY(ti;t2) = 3*i • (*2 + 2).	•
Упражнения
1-	Случайный процесс определяется формулой X(t) = V • sin t, где
t ^ 0, F ~ Л[2;4] — случайная величина, имеющая равномерное
распределение. Найти: а) сечение с. п. X{t) в момент времени t =
б)	реализацию с. п. при одном испытании, в котором с. в. V приняла
значение 2.
2.	Для случайного процесса из упражнения 1 найти: а) математи¬
ческое ожидание с. п.; б) дисперсию и среднее квадратическое от¬
клонение; в) корреляционную (автоковариационную) функцию; г)
нормированную корреляционную функцию с. п. X(t).
6.4.	Стационарный случайный процесс в узком
и широком смысле
Важным классом с. п. являются стационарные случайные процес¬
сы, т. е. процессы, не изменяющие свои характеристики с течением вы¬
мени. Они имеют вид непрерывных случайных колебаний вокруг неко¬
торого среднего значения. Таковыми являются: давление газа в газо¬
проводе, колебания самолета при «автополете», колебания напряжения
в электрической сети и т. д.

188 а Раздел первый. Элементарная теория вероятностей и случайных процессов
о
Случайный процесс X(t) называется стаг^ионарным в широком
смысле, если его математическое ожидание mx{t) постоянно, а корре¬
ляционная функция Kx{t\]t2) зависит только от разности аргументов,
т. е.
mx(t) = ш = const, Kx{h',t2) = Kx{t2 - h).
Из этого определения следует, что корреляционная функция ста¬
ционарного процесса есть функция одного аргумента:
Kx(ti;t2) = Кх{т), где r = t2 -<i-
Это обстоятельство зачастую упрощает операции над стационарными
случайными процессами.
Случайный процесс называют стационарным; в узком смысле, если
все его характеристики зависят не от значений аргументов, а лишь
от их взаимного расположения. То есть для функций распределения
сечений процесса должно выполняться равенство:
F>ti-t-h,t2-t'h1...4tn+h(xl 7 Х2, ' * * Хп) -^1^2	(^17 Х2ч • • • «
при любых Л > 0, п ^ 1, t\, t2,..., tn G Т.
Отметим, что из стационарности с. п. в узком смысле следует ста¬
ционарность его в широком смысле; обратное утверждение неверно.
В дальнейшем будем рассматривать только стационарные с. п. в
широком смысле.
Приведем основные свойства корреляционной функции стационар¬
ного случайного процесса (с. с. п.).
1.	Дисперсия с. с. п. постоянна и равна значению корреляционной
функции в нуле, т. е.
D\ (t) = Кх (0) = const
(т.е. в начале координат Dx{t) = Kx(t;t) = Kx{t — t) = Kx{0)).
2.	Корреляционная функция с. с. п. является четной, т. е.
Кх(т) = Кх(-т).
3.	Модуль корреляционной функции с. с. п. не превышает ее значения
при т = 0, т. е.
\Кх(т)\ ^ КХ{0).
Нормированная корреляционная функция с. с. п. есть неслучайная
функция аргумента т:
, ч Кх{т) Кх(т)	.	,
гх{т) = ——— =	j—’ ПРИ этом 1гА'(т)| ^ 1.
Кх{ 0) о1х

Глава 6. Основы теории случайных процессов ■ 189
Большинство с. с. п. обладают важным для практики эргодическим
свойством, сущность которого состоит в том, что по одной, достаточно
длинной, отдельной реализации можно судить о всех свойствах процес¬
са также как по любому количеству реализаций, т. е., другими словами,
отдельные характеристики с. с. п. |тхДл'(т)} могут быть определены
как соответствующие средние по времени для одной реализации доста¬
точно большой продолжительности. Связь между классами стационар¬
ных и эргодических с. п. приведена на рисунке 61.
Рис. 61
Достаточным условием эргодического с. п. X(t) относительно мате¬
матического ожидания и корреляционной функции является стремле¬
ние к нулю его корреляционной функции при т —» оо, т. е. lim Кх{т) = 0.
т—у оо
В качестве оценок характеристик эргодических с. с. п. принимают
усредненное по времени значение:
Т
rhx(t) = т f x№dt'
о
Т-т
^Л'(г) = J N*) “ '(*)) ‘ + r) “ ™х(*)) dt.
о
Интегралы, в правых частях равенств, вычисляют на практике при¬
ближенно.
Случайные процессы X(t) и Y(t) называются стационарно связан¬
ными, если их взаимно корреляционная функция Kxy(t 1.5*2) зависит
только от т = t\ — *2- В качестве примера стационарного процесса мож¬
но взять с. п. X(t) = A-cos(u>t + (p) — гармоническое колебание. Можно
показать, что nix{t) = 0, а
Kx{t\;t2) = ^М(А2) • cosco(ti — *2) = о\ * cosurr.

190 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
6.5.	Линейные и нелинейные преобразования
случайных процессов
При проектировании различных систем (систем автоматическою
управления или регулирования и т.д.) и других практических задач
возникает следующая задача: на вход системы S подается «входной
сигнал» с. п. A"(t) с известными характеристиками; система преобра¬
зует этот сигнал, в результате чего на выходе системы S получается
с. и. Y(£), называемый «выходным сигналом»: требуется определить ха¬
рактеристики с. п. Y(t) на выходе системы S (см. рис. 62).
X(t)
S
Y(t)
Рис. 62
Преобразование с. п. X(t) в с. п. Y(t), осуществляемое системой
(прибором) S, символически записывается в виде Y(t) = A{X(t)\. где
А - - преобразование или оператор системы S.
Оператор А может иметь любой вид: оператор сложения или умно
жения, оператор дифференцирования или интегрирования и т. д.
Так, например, если x(t) — sin 2£, оператор А есть оператор интс*-
t
грироваиия А = Jх(т) dr, то
о	t
y(t) — .4{sin2t} = [ sin2F dV — — icos2V = Jr(l — cos 21).
J	Z	0	I
0
Все виды подобных преобразований (операторов) можно разделить на
две различные группы: линейные L и нелинейные /V. В свою очередь
линейные преобразования делятся на линейные однородные Ьо и ли¬
нейные неоднородные L#.
Преобразование (оператор) L$ называется линейным, однородным,
если оно (он) обладает свойствами:
1.	Оператор суммы функций (с. п.) равен сумме операторов от каждой
функции, входящих в сумму, т. е.
Lo{Xi(t) +X2(t)} = L0{Xl(t)} + L0{X2(t)}.
2.	Постоянную величину можно выносить за знак оператора, т. е.
Lo{cX(t)}=cLo{X(t)}.

Глава 6. Основы теории случайных процессов ■ 191
Преобразование Lя называется линейным неоднородным, если оно
состоит из линейного однородного преобразования Lo с прибавлением
заданной неслучайной функции /(£), т.е. Ьц{Х(1)} = L0{X(£)} + /(£).
Все преобразования, не являющиеся линейными, называются не¬
линейными.
Примерами линейных однородных операторов являются оператор
дифференцирования Y(t) =	—-, оператор интегрирования У(£) =
t
= J X(r)dr, оператор умножения на заданную функцию У(£) =
о
= /(t) • ^(0* Примерами линейных неоднородных —
t
Y(t) = ^-+f(t), Y(t) = J X(r)dr+f(t), Y(t) = f(t)-X(t)+<p{t).
0
Примерами нелинейных — У(£) = X2(f), У(<) = —4- cosX(t).
6.6.	Дифференцирование и интегрирование
случайных процессов
Пусть с. п. X(t), характеристики которого mx{t) и	зада¬
ны, подвергается дифференцированию У(£) = —Требуется найти
характеристики m.y(t) и Ky(t\;t2) «выходного сигнала» — с. п. У(£).
Предполагая, что с. п. X(t) является непрерывным, производная от
него существует, а м. о. предела равно пределу математического ожи¬
дания, от равенства
т. lim *<« + *>-*(«>
д«->о	At
перейдем к следующему
/ч	mx(t	+	At)	— mx(t)	/ч	dmx(t)
MY(t) = m*(i) = fon	^	Ц т.е. my(t) =
Итак, математическое ожидание производной с. п. равно производной
от ее м. о. (коротко: m^(t) = mfx(t) или М (X'(t)) = (MX(t))r).

192 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Можно показать, что корреляционная функция производной от
с. п. X (t) равна второй смешанной частной производной от корреля¬
ционной функции с. п. X(t):
к и .4 Ч _ dKx(ti;t,2)
Kv(h,t2)~	.
Пример 6.5. Используя результаты решений примеров 6.1 и 6.3, найти
dX(t)
м. о. и корреляционную функцию с. п. У (£) — ———•
til
О Искомое м.о. есть М(У(<)) = M(X'(t)) = ^M(X(t))^ = (Зе-*)' =
= —3e_t.
Находим корреляционную функцию с. п. У (/):
К и .* \ — dKx(ti',h) _ де~и <2 _ д (r-u t2\' _
Ky(,u,2) - a,-dk - дь~дь ~ дь(е ><. -
= JL (_е-*1-*2) =	= e-ti-te = е -(ti+fa). #
Пусть известны характеристики mx(t) и	^г)	с.	п.	X(t), а ли¬
нейное преобразование с. п. состоит в его интегрировании:
Y(t) = J X(т) dr.
Требуется найти характеристики с. п. Y (t).
Можно доказать, что математическое ожидание интеграла от
с. п. равно интегралу от м. о. этого с. пт. е.
ъ
■rnY(t) = j тх{т)6т.
о
Иными словами, операции нахождения м. о. и интегрирования можно
менять местами:
t t
M^J X (г) dr И MX (т) dr.

Глава 6. Основы теории случайных процессов "193
Корреляционная функция интеграла от с. п. равна двойному интегралу
от корреляционной функции с. п., т. е.
ti t2
KY(ti;t2) = J J Kx(Ti-,T2)dTidT2.
о о
Пример 6.6. Зная м. о. m\ (t) — 3 • е 1 и корреляционную функцию
Kx{ti;t2) = e_il~*2, найти м.о. и корреляционную функцию с. п.
t
Y(t) = J Х(т) dr.
О
t
О Искомое м. о. есть my(t) = J 3 * е~т dr = —3 • е~т — 3 — Зе~1.
о
Далее:
tl £2	t\	t2
■Ky(*i;*2) = J J e~Tl~T2 dr\dr2 = J e~Tl dr\ J e~T2 dr2 =
00	00
ti
= J e_T1 e_T2 ^ dn = (1 — e~*2) • (—e_Tl) |*'= (1 — e-*3)(1—)- •
Упражнения
1	- Какие из приведенные ниже случайных процессов являются ста¬
ционарными (в широком смысле):
а)	X(t) — V • sin*, * ^ О, V ~ iV[2;4] — случайная величина;
б)	X(t) = sin(* + <£>), * ^ 0, (р ~ R[0; 27т] — случайная величина;
в)	X(t) — A-cos(t-\-<po), * ^ 0, (fo = const*, Л — случайная величина;
г)	X (*) = У • cos 3*, У — случайная величина.
2-	Известны характеристики двух некоррелированных случайных
процессов X(t) и У(*): тх(*) = * + 2, Kx(*i;*2) = *1 * *2, ту(*) =
= — * 4- 3, i^y(*i;*2) = 2е-*1-*2. Найти математическое ожидание и
корреляционную функцию с. п. Z(*) = X(t) 4- У(*)*

194 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
3.	Зная математическое ожидание и корреляционную функцию с. п.
X = V * sint ^ О, V ~ Я[2;4], m\(t) = 3 * sinf, Kx(ti]t2) —
= ^ sinti sin £2, найти: а) математическое ожидание и корреляцион-
_	dX(t)
ную функцию с. п. У (t) = ———; о) м. о. и корреляционную функ¬
цию с. п.
t
Y(t) = J Х(т) dr.
О
4.	Задан с.п. X(t) = Y • cost, где t ^ О, Y ~ N(2; 1) — нормально
распределенная с. в. Является ли с. п. X(t) стационарным? Найти
rn,z(t) и Kz(ti;t'2) случайного процесса Z(t) = X(t) — 2 • —
6.7.	Спектральное разложение стационарного
случайного процесса
Как известно, неслучайную функцию x(t), удовлетворяющую опре¬
деленным условиям (условиям Дирихле), можно разложить в проме¬
жутке [—Т;+Т] в ряд Фурье.
Оказывается, что любой с. п. X(t) также можно разложить (т. е.
представить в виде суммы так называемых элементарных случайных
процессов) в ряд вида
оо
X(t) = Ifio(t) + YlVk' VbW'
k=1
где V/~ — случайные величины, (pk(t) — неслучайные функции времени.
Метод разложения с. п. упрощает различные преобразования с. п.
(линейных и нелинейных), в частности, упрощает определение харак¬
теристик выходного процесса стационарной линейной системы по из¬
вестным характеристикам входного сигнала.
Рассмотрим с. п.
X(t) = U cosuot 4- V sineot,	(6.5)
где U и V — некоррелированные случайные величины с м. о., рав¬
ными нулю и одинаковыми дисперсиями, со — действительное число

Глава 6. Основы теории случайных процессов ■ 195
(или коротко: си — константа, тц = ту — 0, D\j = Dy = D, Krv =
= м(й* v) = o).
Покажем, что этот процесс является стационарным.
□	Находим м. о. с. п. X(t):
М (Х(£)) = mx(t) — MU • cos cot + MV • sinu;£ — 0.
Следует, что X(t) = X(t) (напомним, что X(t) = X(t) — mx(t)), т. e.
X (t) — центрированный случайный процесс.
Находим теперь корреляционную функцию:
Kx(ti;t2) = M(i(tx) • i(t2)) = м(Х(ь) ■ X(t2))	=
= М (U • cos+ Vr • sina;fi) • (С/ • cosutf2 + F	•	sinu;^)	=
= M(C/2) cos art i * cosatf2 + M(F2) sinarfi • sinufe + 0 =
= D(U) cosujti • cosa;t2 + sinu^i • sina;t2 = D • cosa;(ii — /2)-
Следовательно, X(t) c. c. n. (cm. n. 6.4).	■
Рассмотрим теперь с. п.
оо
X(t) = nix + ^ Uk * cosu;^ 4- Vh • sinc^t,	(6.6)
к=о
где MUk = 0 = AfVi, DC/* = W* = £>*, M(Uk	• 14)	=	0,	M(Vfc • И) -
= M{Uk • С//) = 0 при к ф l, — константы.
Этот с. п. также является стационарным.
Действительно,
M(X(t)) = М(тх) + М Гу Uk • cos сokt 4- Vk • smcukt) = тх 4- 0 = тх.
к—0	'
Следовательно, X(t) — X(t).
Так как слагаемые в с. п. (6.6) некоррелированны, то с учетом фор¬
мулы для корреляционной функции с. п. (6.5) и свойства 6 (см. с. 186)
корреляционной функции получаем
ос
Кх [t\\t2) = 'Y^JDk совеет, где r = t\ - t2.	(6.7)
A:=0
Итак, с. it. (6.6) является стационарным с. п.

вв вв
196 • Раздел первый. Элементарная теория вероятностей и случайных процессов
Отметим, что равенство (6.7) можно рассматривать как разложе¬
ние корреляционной функции Kx(t\; t2) на промежутке [—Т; +Т] в ряд
Можно доказать, что ^ 0 для любой корреляционной функции
Каноническое разложение (6.6) называется спектральным разло¬
жением стационарного случайного процесса.
Разложение (6.7) называется спектральным разложением корре¬
ляционной функции С. IT. X(t).
Отметим, что спектральное разложение с. с. п. (6.6) можно предста¬
вить в виде суммы гармонических колебаний со случайными амплиту¬
дами Ak, фазами <рь и частотами
Дисперсия с. с. п., представленного в виде (6.6), равна сумме дис¬
персий всех гармоник его спектрального разложения:
Фурье по косинусам: Кх {hi fa) — Кх(т) = ]Г] D^coscj^t, где
к=о
Т
-	т
т	т
(6.8)
с. с. п. X(t).
оо
X(t) = гпх +^Ак- sin(Wfct + (file),
к=о
ОО
к=О
Действительно,
ОО
оо
Dx = [Кх(Ъ t)] = Кх{0) = ^ Dk соs(u;*: • 0) = ^ Dk.
k=0
k=О
Совокупность дисперсий Dk называют спектром с. с. п., а ординаты
Dk — спектральными линиями, соответствующими частотами и;*;.

Глава 6. Основы теории случайных процессов "197
Dk
Do
D->
Dz
U) 1	(jJ‘2 CJ%
Dk
kcJi шк
Рис. 63
Спектр можно изобразить графически (рис. 63).
Сумма всех ординат спектра равна дисперсии с. п. X(t). Спектр
с. п. (6.6) называется линейчатым дискретным с бесконечным числом
равноотстоящих спектральных линий. Расстояние между соседними
линиями по частоте равно со\ = ^ = Аси.
6.8.	Спектральная плотность случайного процесса.
Теорема Винера-Хинчина
Спектральное разложение с. п. на промежутке [—Т;Т] дает при¬
ближенное его описание. Более полное представление о с. п. при его
спектральном разложении может быть получено при увеличении вели¬
чины Т.
При неограниченном увеличении промежутка разложения (Т —> оо)
оо
число слагаемых в сумме Dx =	неограниченно увеличивается
к=0
(Dfc — коэффициент разложения корреляционной функции, см. (6.7)),
каждое слагаемое Dk неограниченно уменьшается, но сумма остается
постоянной. Интервал между соседними частотами uj\ = ~ = Аш будет
стремиться к нулю. т. е. До; —> 0.
Обозначим среднюю плотность дисперсии через Sx{wk)i те-
Аси
SxM = £ = £?■	(6.9)
Спектральной плотностью S\(m) стационарного случайного про¬
цесса X(t) называется предел отношения дисперсии, приходящийся на
интервал частот Да; к длине этого интервала, когда последняя стре¬
мится к нулю:
S\(w)= lim	(б.Ю)
v ' Ды-юАш	v ’

198 в Раздел первый. Элементарная теория вероятностей и случайных процессов
т. е. спектральная плотность с. с. п. есть предел средней плотности дис¬
персии (6.9), когда Аси 0.
Получим формулы, связывающие спектральную плотность Sx(u)
и корреляционную функцию Кх{т) в случае, когда Т -» оо (Аси —> 0).
Выразим дисперсию Df~ из равенства (6.9) и подставим ее в равен-
о
Переходя к пределу при Т х> (До; —> 0), из равенств (6.11) и (6.12)
получаем следующее утверждение.
Теорема 6.1 (Теорема Винера—Хинчина). Корреляционная функция
и спектральная плотность с. с. п. связаны между собой взаимно обратны¬
ми косинус-преобразованиями Фурье:
Дискретный линейчатый спектр разложения переходит, при Т —»
—> оо, в непрерывный спектр, в котором каждой частоте со ^ 0 соответ¬
ствует неотрицательная ордината Sx(w)-
Кривая Sx (с^) изображает плотность распределения дисперсий по
частотам непрерывного спектра (см. рис. 64).
Спектральная плотность Sx(w) стационарного с. п. обладает сле¬
дующими свойствами:
1.	Спектральная плотность — неотрицательная функция, т.е.
ства (6.7) и (6.8). Получаем: Dk = Sx{ojk) ■ Аш = Sx{uk) *
OO
Kxih: t-г) = Kx(t) = ^ Sxfak) * coso^r • Ди>,	(6.11)
k=0
T
0
т. e.
т
(6.12)
OO
(6.13)
о
OO
(6.14)
о
Sx4 >0.

Глава 6. Основы теории случайных процессов "199
Это свойство вытекает из определения (6.10), где D^ ^ 0, Да; > 0.
2.	Интеграл от спектральной плотности в пределах от 0 до ос равен
дисперсии с. с. п., т. е.
оо
/
Sx{w)dw = DX-
Это свойство вытекает из равенства (6.13):
оо	оо
D\ — А"х(0) = J S'x(cj) cos(u; • 0) • dcu = J Sx{w) dw.
о	0
Отметим, что для упрощения математических выкладок удобно ис¬
пользовать спектральное разложение с. с. п. в комплексной форме, счи¬
тая, что частоты изменяются в интервале (—оо,+оо) (частоты ш < 0
физического смысла не имеют).
Спектральной платностью стационарного с. п. в комплексной
форме называется функция
S\'M = |-Sa-(M)-	(6.15)
Формулы Винера-Хинчина в комплексной форме имеют вид
оо
Кх(т) = / Зд'М • еШТ ■ du,	(6.16)
—	ОО
X)
S*x(w) = ± j Кх{т)-е-™-dr.	(6.17)
—	ОО
Они получаются, если в спектральном разложении (6.7) заменить три¬
гонометрические функции, используя формулы Эйлера
COS U7/е Г
-I- е
-гикт
Sin 01^Т
Зшкт _ е-гикт
2i	5

200 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
и перейти к пределу при Т —» оо (Да; -* 0).
Отметим, что спектральная плотность S^(uj) — четная функция
на интервале (—сх,+оо), т. е. S\(—oj) — S\(uj).
На участке (0; +оо) имеем	=	i	-	5х(^)	(см.	рис.	65).
Значения функции 5^ (и;) в 2 раза меньше значений Sy(uj) при тех
же значениях аргумента ш.
Пример 6.7. Найти спектральную плотность стационарного с. п. X(t)y
зная его корреляционную функцию Кх(т) = D • с q’ItL a > 0.
О Используя формулу (6.17), получаем
оо
SHu) = ^ J D • е-а'1т| • е~*шт dr =
—ос
0	ос
= Sr ( / е° Т ' 6 dT + J €~a T ' eriWT dT) =
—оо	0
0	оо
= ё ( / e(a~iW)T dT + / e-(Q+^)r dr^j =
—	оо	0
= И. (	1	.	е(а-гы)т1°	1	.	е -(а+гы)т °°\ _
2тт \a-iuj	I-оо	а	+	га;	о	/
D (л + ш + а — i(j) D	2 а
= я (_i	1—(о -1)4) =
27Г \а — ш а + ш	)
т-е- sxM = -;ттЧт "л"<w> = -пггЫ-	*
7г(а+аг)	7r(az + иг)
Пример 6.8. Найти корреляционную функцию с. п. со спектральной
плотностью
о* / л /£(b М ^
Р

Глава 6. Основы теории случайных процессов ■ 201
О Используя формулу (6.16), получаем
dr) = /
к
LOq
— CJQ
—LJQ
т	2 *
Вид К,\ (т) изображен на рис. 66.
So егшоТ - е^гшог о	о с	sincj0T
—	• Z = Z ■ О 0 ■ CJo '
6.9.	Стационарный белый шум
Одним из конкретных типов стационарного с. п. является так на¬
зываемый стационарный белый шум.
Стационарным белым шумом называется стационарный с. п. X(t),
спектральная плотность которого постоянна:
S'y(cj) = So = const для со Е (—оо; Н-оо).
Найдем корреляционную функцию белого шума, используя фор¬
ою
мулу (6.16) и учитывая, что / elu)t • duj = 27г • 5(f), где 5(f) — дельта-
—сю
функция (см. замечание):
оо	оо
Кх(т)= j So • eiwT dio — So J e™T dw = 50 • 2тг • j(r),

202 " Раздел первый. Элементарная теория вероятностей и случайных процессов
т. е.
Кх(т) = 2ттS0 ■ S(t).
Как видно, корреляционная функция стационарного белого шума про¬
порциональна дельта-функции; коэффициент 27г5'о называется интен¬
сивностью белого шума.
Замечание 1. Дельта-функция Дирака й(а;) есть предел последова-
{1- |ж| <;
2-е’
0,	|х|	>	е.
ЭО
Условно записывают тале: Six) = s ’	^	,	причем [ S(x)dx — 1.
[оо, х = 0;	J
—	ОО
ОО
Дельта-функция представима интегралом Фурье: й(ж) = J eiujx dx.
- оо
Наглядно график функции Дирака похож на график, изображенный
на рисунке 67.
Дельта-функция может быть представлена как плотность распре¬
деления масс, при которой в точке х — 0 сосредоточена единичная
масса, а масса в остальных точках равна нулю.
Замечание 2. Равенство Кх{т) = 5о-27г-<£(т) означает некоррелиро¬
ванность любых двух различных сечений X(ti) и Xfa) (ведь 6(х) = 0
при всех значениях х ф 0). В силу этого осуществить белый шум невоз¬
можно, т. е. белый шум — полезная математическая абстракция. Белый
шум используется в случаях, когда спектральная плотность с. п. при¬
мерно постоянна в определенном диапазоне (интересующем нас) час¬
тот.

Глава 6. Основы теории случайных процессов ■ 203
Упражнения
1.	Задана корреляционная функция некоторого случайного процес-
= (2'
\о,
с. п. X(t) стационарным в широком смысле?
z, т\<Т
са X(t): К\{т) = ^ п’ [	^	^	Выяснить,	является	ли	заданный
2.	Показать, что функция Кх (^) = Зе т может быть корреляционной
функцией стационарного с. п. X(t).
3.	Найти корреляционную функцию стационарного с. п. X(t), зная,
что mx{t) = 1, а спектральная плотность Sb(co) =	-—
7г(1+иг)
4.	Известно, что спектральная плотность стационарного с. п. X(t)
имеет вид Sa'(^) —
7г(1 + СО2)
Найти дисперсию случайного процесса X(t).
6.10.	Понятие марковского случайного процесса
Среди случайных процессов особое место занимают марковские
случайные процессы.
Рассмотрим некоторую физическую систему 5, в которой происхо¬
дит случайный процесс. С течением времени она может под влиянием
случайных факторов переходить из одного состояния в другое состоя¬
ние.
Случайный процесс называется процессом с дискретными состо¬
яниями, если множество его возможных состояний si, 52, 53,...,	...
конечно или счетно (можно заранее перечислить), а переход из одно¬
го состояния в другое осуществляется скачком, переходы возможны
только в определенные моменты времени ti,f2^з>	
Если переходы возможны в любой момент времени, т. е. моменты
переходов из одного состояния в другое случайны, то процесс называ¬
ется процессом с непрерывным временем.
Случайный процесс с дискретными состояниями называется мар¬
ковским, если для любого момента времени to условная вероятность
каждого из состояний системы S в будущем (т.е. при t > to) зависит
только от ее состояния в настоящем (т.е. при t = to) и не зависит от

204 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
того, когда и как система пришла в это состояние (т. е. каковы были
состояния системы S в прошлом, при t < to).
Марковский процесс называют также процессом без последствия:
будущее в нем зависит от прошлого только через настоящее, т.е. ве¬
роятность системы S попасть в состояние sj в момент времени t^
(S(tk) = Sj) зависит лишь от состояния Si, в котором система нахо¬
дилась в предыдущий момент времени tk-1 (S(tk-1) = S{):
где x\, X2 • •. — есть возможные состояния системы {si, s2,..., Si,...
Марковский процесс служит моделью для многих процессов в био¬
логии (распределение эпидемий, рост популяции), в физике (распад
радиоактивного вещества), в теории массового обслуживания. Отме¬
тим, что в СМО множество состояний системы определяется числом
каналов, т.е. линий связи, вычислительные машины, продавцы и т.д.;
переходы между состояниями системы S происходят под воздействием
потока событий (т.е. потока заявок, требований, отказов и т.д.), кото¬
рые являются простейшими, пуассоновскими.
Случайные процессы с дискретными состояниями удобно иллю¬
стрировать с помощью так называемого графа состояний. В нем со¬
стояния si,62,... системы S изображаются прямоугольниками (или
кружками), а возможные непосредственные переходы из состояния в
состояние — стрелками (или ориентированными дугами), соединяю¬
щими состояния.
Пример 6.9. Построить граф состояний следующего с. п.: устройство S
в случайный момент времени может выйти из строя, оно осматривается
в определенные моменты времени, например через каждые 3 часа, и в
случае необходимости — ремонтируется.
о Возможные состояния системы (устройства) S: S\ — устройство
исправно; s2 - - устройство неисправно, требуется ремонт; S3 — устрой¬
ство неисправно, ремонту не подлежит, списано.
Граф системы имеет вид, изображенный на рис. 68.
Процесс представляет собой случайное блуждение системы S по
состояниям, время (3 часа) — шаг процесса.
Реализация с. п. блуждения системы может иметь, в частности, та¬
кой вид:
р (S{tk) = *j 1 S{t 1) = Xi, S(t2) = X2,..., S'(ffc-i) = Si) =F
= P(S(t,k) = Sj I S(t,k^i) = Si),
... , Sj,. .

Глава 6. Основы теории случайных процессов ■ 205
Рис. 68
что означает: при 1-м, 2-м, 3-м осмотрах устройство исправно; при
4-м осмотре — неисправно, ремонтируется; при 5-м, 6-м — исправ¬
но; при 7-м — устройство признано негодным, списано. Процесс закон¬
чился.	•
Для описания с. п. с дискретными состояниями пользуются веро¬
ятностями состояний системы 5, то есть значениями Pi(t),P2(t),...
... ,pn{t), где Pi(t) = P{S(t) = Si} — вероятность того, что в момент
времени t система находится в состоянии .s$; S(t) — случайное состоя¬
ние системы 5 в момент t.
Очевидно, что для любого момента t сумма вероятностей всех со¬
стояний равна единице (как сумма вероятностей полной группы несов¬
местных событий):
71
$>(*) = L
г—1
6.11.	Дискретный марковский процесс. Цепь
Маркова
Пусть в некоторой системе 5 происходит с. п. с дискретными состо¬
яниями si, S2, • • •, sn и дискретным временем, т. е. переход системы из
одного состояния в другое происходит только в определенные моменты
времени		Эти моменты называют шагами процесса (обычно
разности смежных моментов наблюдения ^ ~ U _i равны постоянному
числу — длине шага, принимаемого в качестве единицы времени); Iq —
начало процесса.
Этот с. п. можно рассматривать как последовательность (цепь) со¬
бытий 5(0), 5(1), 5(2),... (5(0) — начальное состояние системы, т.е.
перед 1-м шагом; 5(1) — состояние системы после 1-го шага, 5(2) по¬
сле 2-го шага и т. д.), т. е. событий вида {S(k) = s*}, где г — 1,2,3,..., п;
* = 0,1.2,....

206 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Марковский случайный процесс с дискретными состояниями и дис¬
кретным временем называют марковской цепью (цепь Маркова).
Отметим, что цепь, в которой условные вероятности состояний
в будущем зависят только от состояния на данном, последнем ша¬
ге (и не зависят от предыдущих), называют простой цепью Маркова.
(А. А. Марков (1856 1922) —русский математик).
Будем рассматривать только простые цепи Маркова вычислять ве¬
роятности состояний системы
Pi(k) = P{S(k) ~ ^}, г — 1,2,3,..., п: к — 0.1.2,..
Здесь P{S(k) = 5г} — безусловная вероятность того, что на к-м шаге
система будет находиться в состоянии Si.
Для нахождения безусловной вероятности нужно знать начальное
распределение вероятностей pi(0),p2(0), • • • ,Рп(0), т. е. вероятности со¬
стояний pi(0) в момент времени *о — 0 (начало процесса) и так называ¬
емые переходные вероятности Pij(k) марковской цени на к-м шаге.
Переходной вероятностью Pij(k) называют условную вероятность
перехода системы S на к-м шаге в состояние Sj, если известно, что на
предыдущем (к — 1)-м шаге она была в состоянии s*, т. е.
Pij(k) = P{S(k) = sj | S{k- 1) = а*}, i,j = 1,2,3,...,n
(первый индекс указывает номер предшествующего, а второй — номер
последующего состояния).
Цепь Маркова называется однородной, если Pij{k) ~ ptj, т. е. услов¬
ные вероятности Pij{k) не зависят от номера испытаний.
Далее будем рассматривать только однородные цепи, которые мо¬
гут быть заданы с помощью вектора (pi(0),pi(0), -.. ,Pn(0)) — вероят¬
ности состояний в момент	времени to	= 0	и матрицы
Ри	PV2	•••	Р1»Л
■р _	Р21	Р‘12	• • •	Р2п
\Рп 1	Рп2	• ■ •	Рпп/
называемой матрицей перехода системы.
Элементы матрицы V обладают следующими свойствами:
а)	Pij ^ 0.
П
б)	£ р^ — 1 {г = 1,2,3, ...,п), т. е. сумма вероятностей каждой
з=1
строки матрицы перехода равна единице (как ве!Юятности событий —

Глава 6. Основы теории случайных процессов ■ 207
перехода из одного состояния в любое возможное состояние Sj —
образующих полную группу).
Справедлива формула V(n) = Vn, т. е. матрица переходов за п ша¬
гов есть п-я степень матрицы переходов за один шаг.
/0,2
[\1 Пример 6.10. Задана матрица перехода V = I 0,3
^	\0,5
матрицу вероятностей переходов за два шага.
о	По формуле 7^(2) = V2 находим
/0,2	0,3	0,5\	/0,2	0.3	0,5\	/0,38	0,27	0,35\
Р(2) = [ 0,3	0,2	0,5	0,3	0,2	0,5	-	1 0,37	0,28	0,35	. •
\0,5	0,3	0,2/	\0,5	0,3	0,2/	\0,29	0,27	0,44/
Заметим, что цепь Маркова представляет собой дальнейшее обоб¬
щение схемы Бернулли уже на случай зависимых испытаний; незави¬
симые испытания являются частным случаем марковской цепи. Под
«событием» понимается состояние системы, а под «испытанием» — из¬
менение ее состояния.
Если «испытания» (опыты) независимы, то появление определен¬
ного события в любом опыте не зависит от результатов ранее произве¬
денных испытаний.
0,3 0,5\
0,2 0,5 I. Найти
0,3 0,2/
6.12.	Понятие о непрерывном марковском процессе.
Уравнения Колмогорова
Пусть в некоторой системе S происходит марковский с. п. с дис¬
кретными СОСТОЯНИЯМИ Si, 52, • . * , $п•
PCI	Если	переходы системы из состояния в состояние происходят не в
фиксированные моменты to, ii,	а в случайные моменты времени
(что чаще встречается на практике), то такой процесс называют мар¬
ковским процессом с дискретными состояниями и непрерывным вре¬
менем.
Марковские с. п. указанного типа используются, в частности, для
исследования реальных систем массового обслуживания (СМО); про¬
цессы в них протекают в непрерывном времени. Под состоянием систе¬
мы понимается число требований (заявок) на обслуживание, находя¬
щихся в системе.

208 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
Будем считать, что переходы системы из состояния Si в состояние
Sj осуществляются под воздействием пуассоновского потока событий
(см. п. 1.21) с интенсивностью Хц — const.
Граф состояний системы с проставленными у стрелок интенсивно¬
стями называют размеченным (см. рис. 69).
Рис. 69
Переход системы из состояния 5i в 52 происходит в момент, когда
наступает первое событие потока.
Вероятность события, состоящего в том, что в момент времени t си¬
стема S будет находиться в состоянии 5], как уже знаем, обозначается
через pi(t):
п
Pi(t) = P{S{t) = Sj}, причем ^TPi(t) = 1.
i— 1
Для нахождения этих вероятностей, т.е. pi(t) — вероятностей состоя¬
ний системы 5i, 52,..., sn* нужно решить систему дифференциальных
уравнений — уравнений Колмогорова, имеющих вид
Xij * Pj(t) — Pi{t) • ^2 ^ij• (i = 1,2,3,..., n)
i	j=i
n
С начальными условиями Pl(0),P2(0), Pn{0); ^г(О) ^ 0,	Рг{0) = 1 и
г= 1
п
условием нормировки	—	1*
i— 1
При составлении системы уравнений Колмогорова удобно пользо¬
ваться размеченным графом состояний системы.
Правило составления уравнений Колмогорова таково: в левой ча¬
сти каждого из уравнений стоит производная вероятности 5* состояния,
dpi(t) . ч
т.е. ———; в правой части — сумма произведений вероятностей Pi(t)
всех состояний (когда стрелка ведет в данное состояние) на интенсив¬
ности соответствующих потоков минус суммарная интенсивность всех
потоков (когда стрелка ведет из данного состояния), умноженная на
вероятность данного Si состояния.

Глава 6. Основы теории случайных процессов ■ 209
Рис. 70
Например, для системы S, размеченный граф состояний которой
показан на рис. 70, система дифференциальных уравнений будет:
rp[(t) =	A2i •	P2(t)	- (Ai2 + Лгз)	•
p'2(t) =	Ai2 •	Pl(t)	- (Л21 + A23)	•P2[t),
Рз(*) =	^13	+ *^23 'P2(t)-
Нормировочное условие pi(t)	+ Рз(*) = 1-
При интегрировании такой системы следует учесть состояние си¬
стемы в начальный момент, т. е. при t — 0. Так, если в этот момент она
была в состоянии то полагают р*(0) = 1, р«(0) = 0 при г ф к.
Замечание. Случайный процесс, устанавливающийся в системе при
t —> ос (т.н. предельный стационарный режим), характеризуют так
называемые предельные вероятностлi состояний, т. е. вероятности Pi(t)
при t —> ОО.
Предельные вероятности существуют, если число состояний конеч¬
но, «состояний без выхода» (из них невозможен переход ни в какое
другое состояние) нет, потоки событий стационарны (А^ = const).
Предельная вероятность состояния показывает среднее относи¬
тельное время пребывания системы в этом состоянии.
Для нахождения предельных вероятностей в уравнениях Колмо-
dpi(t.)
горова полагают все производные
dt
равными 0 и решают систему
п
алгебраических уравнений (с учетом нормировки Pi(t) — !)•
i—i
Пример 6.11. Найти предельные вероятности для системы S, граф
которой изображен на рис. 71.
1
2
si
S2
«з
Г“ 4
3
Рис. 71

210 ■ Раздел первый. Элементарная теория вероятностей и случайных процессов
0	Составляем дифференциальные уравнения Колмогорова:
Vi = 4рг ~ Ри
; р'2 = Р\ + 3Рз - (2 + 4) • р-2.
Уз = 2 • Р2 - з • Рз-
Тогда система алгебраических уравнений, описывающих стационарный
режим системы S, принимает вид
'4Р2 - Pi = 0,
Р\ + 3/>з - 6р2 = 0,
2Р2 — Зрз = 0,
.Pi + Р2 +РЗ = I-
12	3	2
Решая эту систему, находим pi = —.,р2 — рз = уу, т. е. система S в
среднем 70,6 % будет находиться в состоянии s i; 17,6 % — в состоянии
s2; И,8 % — в состоянии 53.	•
Упражнения
1	- Цепь Маркова управляется матрицей перехода
V	=
Определить матрицу вероятностей переходов за три шага.
2.	Цепь Маркова с двумя состояниями s \ и s2 задана матрицей веро¬
ятностей переходов
V	=
В качестве начального состояния процесса некоторое устройство
выбирает s\ с вероятностью ~ и s2 — с вероятностью Постро¬
ить граф, соответствующий матрице. Найти вероятность того, что
после первого шага процесс перейдет в состояние s\.
3- Матрица вероятностей переходов за один шаг цепи Маркова имеет
вид
/0,2 0,3
V =	0,4	0,2
\0,5 0,2
Найти предельные вероятности.

Основы
математическом
статистики
Раздел второй

Глава 7
Выборки и их характеристики
7.1.	Предмет математической статистики
Математическая статистика, — раздел математики в котором
изучаются методы сбора, систематизации и обработки результатов на¬
блюдений массовых случайных явлений для выявления существующих
закономерностей.
Математическая статистика тесно связана с теорией вероятностей.
Обе эти математические дисциплины изучают массовые случайные
явления. Связующим звеном между ними являются предельные теоре¬
мы теории вероятностей. При этом теория вероятностей выводит из ма¬
тематической модели свойства реального процесса, а математическая
статистика устанавливает свойства математической модели, исходя из
данных наблюдений (говорят «из статистических данных»).
Предметом математической статистики является изучение случай¬
ных величин (или случайных событий, процессов) по результатам на¬
блюдений. Полученные в результате наблюдения (опыта, эксперимен¬
та) данные сначала надо каким-либо образом обработать: упорядочить,
представить в удобном для обозрения и анализа виде. Это первая зада¬
ча. Затем, это уже вторая задача, оценить, хотя бы приблизительно,
интересующие нас характеристики наблюдаемой случайной величи¬
ны. Например, дать оценку неизвестной вероятности события, оценку
неизвестной функции распределения, оценку математического ожида¬
ния, оценку дисперсии случайной величины, оценку параметров рас¬
пределения, вид которого неизвестен, и т. д.
Следующей, назовем ее условно третьей, задачей является провер¬
ка, статистических гипотез, т.е. решение вопроса согласования ре¬
зультатов оценивания с опытными данными. Например, выдвигается
гипотеза, что: а) наблюдаемая с. в. подчиняется нормальному закону;
б)	м. о. наблюдаемой с. в. равно нулю; в) случайное событие обладает
данной вероятностью и т. д.
Одной из важнейших задач математической статистики является
разработка методов, позволяющих по результатам обследования вы¬

Глава 7. Выборки и их характеристики ■ 213
борки (т. е. части исследуемой совокупности объектов) делать обосно¬
ванные выводы о распределении признака (с. в. X) изучаемых объектов
по всей совокупности.
Для обработки статистических данных созданы специальные про¬
граммные пакеты (STADIA, СтатЭксперт, Эвриста, SYSTAT, STAT-
GRAPHICS и др.), которые выполняют трудоемкую работу по расче¬
ту различных статистик, построению таблиц и графиков. Простейшие
статистические функции имеются в программируемых калькуляторах
и популярных офисных программах (EXCEL).
Результаты исследования статистических данных методами мате¬
матической статистики используются для принятия решения (в зада¬
чах планирования, управления, прогнозирования и организации произ¬
водства, при контроле качества продукции, при выборе оптимального
времени настройки или замены действующей аппаратуры и т.д.), т.е.
для научных и практических выводов.
Говорят, что «математическая статистика — это теория прииятия
решений в условиях неопределенности».
Математическая статистика возникла в XVIII веке в работах
Я. Бернулли, П. Лапласа, К. Пирсона. В ее современном развитии опре¬
деляющую роль сыграли труды Г. Крамера, Р. Фишера, Ю. Неймана
и др. Большой вклад в математическую статистику внесли русские уче¬
ные П. Л. Чебышев, А. М. Ляпунов, А. Н. Колмогоров, Б. В. Гнеденко
и другие.
7.2.	Генеральная и выборочная совокупности
Пусть требуется изучить данную совокупность объектов относи¬
тельно некоторого признака. Например, рассматривая работу диспет¬
чера (продавца, парикмахера	), можно исследовать: его загружен¬
ность, тип клиентов, скорость обслуживания, моменты поступления
заявок и т. д. Каждый такой признак (и их комбинации) образует слу¬
чайную величину, наблюдения над которой мы и производим.
Рч|	Совокупность	всех подлежащих изучению объектов или возможных
результатов всех мыслимых наблюдений, производимых в неизменных
условиях над одним объектом, называется генеральной совокупностью.
Более строго: генеральная совокупность — это с. в. Х(и)^ заданная
на пространстве элементарных событий ft с выделенным в нем классом
S	подмножеств событий, для которых указаны их вероятности.

214 ■ Раздел второй. Основы математической статистики
Зачастую проводить сплошное обследование, когда изучаются все
объекты (например — перепись населения), трудно или дорого, эконо¬
мически нецелесообразно (например — не вскрывать же каждую кон¬
сервную банку для проверки качества продукции), а иногда невозмож¬
но. В этих случаях наилучшим способом обследования является вы¬
борочное наблюдение: выбирают из генеральной совокупности часть ее
объектов («выборку») и подвергают их изучению.
Выборочной совокупностью (выборкой) называется совокупность
объектов, отобранных случайным образом из генеральной совокупно¬
сти.
Более строго: выборка — это последовательность Xi,X2,... ,Xn
независимых одинаково распределенных с. в., распределение каждой
из которых совпадает с распределением генеральной случайной вели¬
чины.
Число объектов (наблюдений) в совокупности, генеральной или вы¬
борочной, называется ее объемом', обозначается соответственно через N
и п.
Конкретные значения выборки, полученные в результате наблюде¬
ний (испытаний), называют реализацией выборки и обозначают строч¬
ными буквами #1, #2, • • •, хп.
Метод статистического исследования, состоящий в том. что на осно¬
ве изучения выборочной совокупности делается заключение о всей ге¬
неральной совокупности, называется выборочным.
Для получения хороших оценок характеристик генеральной сово¬
купности необходимо, чтобы выборка была репрезентативной (или
представительной), т.е. достаточно полно представлять изучаемые
признаки генеральной совокупности. Условием обеспечения репрезен¬
тативности выборки является, согласно закону больших чисел, соблю¬
дение случайности отбора, т. е. все объекты генеральной совокупности
должны иметь равные вероятности попасть в выборку.
Различают выборки с возвращением (повторные) и без возвраще¬
ния (бесповторные). В первом случае отобранный объект возвращается
в генеральную совокупность перед извлечением следующего; во вто¬
ром — не возвращается. На практике чаще используется бесповторная
выборка.
Заметим, если объем выборки значительно меньше объема гене¬
ральной совокупности, различие между повторной и бесповторной вы¬
борками очень мало, его можно не учитывать.
В зависимости от конкретных условий для обеспечения репрезента¬
тивности применяют различные способы отбора: простой, при котором
из генеральной совокупности извлекают по одному объекту; гпипиче-

Глава 7. Выборки и их характеристики • 215
ский, при котором генеральную совокупность делят на «типические»
части и отбор осуществляется из каждой части (например, мнение о
референдуме спросить у случайно отобранных людей, разделенных по
признаку пола, возраста,...механический, при котором отбор произ¬
водится через определенный интервал (например, мнение спросить у
каждого шестидесятого...); серийный, при котором объекты из гене¬
ральной совокупности отбираются «сериями», которые должны иссле¬
доваться при помощи сплошного обследования.
На практике пользуются сочетанием вышеупомянутых способов от¬
бора.
Пример 7.1. Десять абитуриентов проходят тестирование по матема¬
тике. Каждый из них может набрать от 0 до 5 баллов включительно.
Пусть Xk — количество баллов, набранных к-м (к = 1,2,..., 10) аби¬
туриентом.
Тогда значения 0, 1, 2, 3, 4, 5 — все возможные количества бал¬
лов, набранных одним абитуриентом, — образуют генеральную сово¬
купность.
Выборка Xi, Х2, A3,..., Х\$ - - результат тестирования 10 абитури¬
ентов.
Реализациями выборки могут быть следующие наборы чисел: {5,
3,	0, 1, 4, 2. 5. 4, 1, 5} или {4, 4, 5, 3, 3, 1, 5, 5, 2, 5} или {3, 4, 5. 0. 1. 2,
3,	4, 5, 4} и т. д.
7.3.	Статистическое распределение выборки.
Эмпирическая функция распределения
Пусть изучается некоторая с. в. X. С этой целью над с. в. X про¬
изводится ряд независимых опытов (наблюдений). В каждом из этих
опытов величина X принимает то или иное значение.
Пусть она приняла 7i\ раз значение х\, раз — значение х2, ...,
rifc раз — значение хд.. При этом п\ -Ьп2 +.. - + Пк — п — объем выборки.
Значения xi,x2, • • •, называются вариантами с. в. X.
Вся совокупность значений с. в. X представляет собой первичный
статистический материал, который подлежит дальнейшей обработке,
прежде всего — упорядочению.
К|	Операция расположения значений случайной величины (призна¬
ка) по неубыванию называется ранжированием статистических дан¬
ных. Полученная таким образом последовательность Х(2),.. •

216 ■ Раздел второй. Основы математической статистики
значений с. в. X (где х*^) ^ х(2) ^	^	Х(п) и х^ = ^min	..., Ж(п) =
= max Xi) называется вариационным рядом.
l^i^n
Числа Пг, показывающие, сколько раз встречаются варианты ж* в
ряде наблюдений, называются частотами, а отношение их к объему
выборки — частостями или относительными частотами (р*), т. е.
* Пг
Pi = 7Г>
(7.1)
где п
= £
п.-
Перечень вариантов и соответствующих им частот или частостей
называется статистическим распределением выборки или статисти¬
ческим рядом.
Записывается статистическое распределение в виде таблицы. Пер¬
вая строка содержит варианты, а вторая — их частоты щ (или часто¬
сти р*).
Пример 7.2. В результате тестирования (см. пример 7.1) группа аби¬
туриентов набрала баллы: 5. 3. О, 1, 4, 2, 5, 4, 1, 5. Записать полученную
выборку в виде: а) вариационного ряда; б) статистического ряда.
О а) Проранжировав статистические данные (т. е. исходный ряд), по¬
лучим вариационный ряд (.'£(i),.'£(2)>	х(щ):
(О, 1, 1, 2. 3, 4, 4, 5, 5, 5).
б)	Подсчитав частоту и частость вариантов xj = 0, x<i — 1, х% — 2,
Х4 = 3, X5 — 4, хв — 5, получим статистическое распределение выборки
(так называемый дискретный статистический ряд)
Xi
0
1
2
3
4
5
ТЦ
1
2
1
1
2
3
или
Xi
0
1
2
3
4
5
pt
i
10
2
10
1
10
1
10
2
10
3
10
Статистическое распределение выборки является оценкой неиз¬
вестного распределения. В соответствии с теоремой Бернулли (п. 5.3)
относительные частоты р* сходятся при п —> оо к соответствующим

Глава 7. Выборки и их характеристики ■ 217
вероятностям р?;, т.е. р„* 	> р{. Поэтому при больших значениях п
72—УОО
статистическое распределение мало отличается от истинного распре¬
деления.
В случае, когда число значений признака (с. в. X) велико или при¬
знак является непрерывным (т. е. когда с. в. X может принять любое
значение в некотором интервале), составляют интервальный стати¬
стический ряд. В первую строку таблицы статистического распределе¬
ния вписывают частичные промежутки \xo,xi), [х\,х2), • ■ ■ Л^А: -1,^),
которые берут обычно одинаковыми по длине: h = х\ — хо = х2 — х\ —
—	Для определения величины интервала (h) можно использовать
формулу Стерджеса:
1		 3?max ^min
1 + log2П ’
где жтах — жт1п — разность между наибольшим и наименьшим значени¬
ями признака, m = 1 + log2п — число интервалов (log2n ^ 3,322 lgn).
За начало первого интервала рекомендуется брать величину #Нач —
= ^min — Во второй строчке статистического ряда вписывают коли¬
чество наблюдений щ (г = 1 ,/с), попавших в каждый интервал.
Пример 7.3. Измерили рост (с точностью до см) 30 наудачу отобран¬
ных студентов. Результаты измерений таковы:
178,	160,	154, 183, 155, 153,	167, 186,	163, 155,
157, 175,	170, 166, 159, 173,	182, 167,	171, 169,
179,	165,	156, 179, 158, 171,	175. 173,	164, 172.
Построить интервальный статистический ряд.
О Для удобства проранжируем полученные данные:
153, 154, 155, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 167, 169,
170, 171, 171, 172, 173, 173, 175, 175. 178, 179, 179, 182, 183, 186.
Отметим, что X — рост студента — непрерывная с. в. При более
точном измерении роста значения с. в. X обычно не повторяются (веро¬
ятность наличия на Земле двух человек, рост которых равен, скажем
л/3 = 1,732050808... метров, равна нулю!).
Как видим Xmin = 153, .xmax = 186; по формуле Стерджеса, при
л = 30, находим длину частичного интервала
г	_ 186 — 153 ^	33	^	33	~ 5 59
1	+ log2 30 ~ 1	+ 3,322 lg 30 ~ 5.907	~	’	'
Л
Примем h = 6. Тогда хнзч = 153 — ^ — 150. Исходные данные
разбиваем на 6 (т = 1 H-log230 = 5,907 ^ 6) интервалов: [150,156),
[156,162), [162,168), [168,174), [174, 180), [180, 186).

218 я Раздел второй. Основы математической статистики
Подсчитав число студентов (гг7;), попавших в каждый из получен¬
ных промежутков, получим интервальный статистический ряд:
Рост
[150 156)
[156-162)
[162 168)
[168-174)
[174 180)
[180-186)
Частота
4
5
6
7
5
3
Частость
0,13
0,17
0.20
0,23
0,17
0,10
Одним из способов обработки вариационного ряда является постро¬
ение эмпирической функции распределения.
Эмпирической (статистической) функцией распределения называ¬
ется функция F*(x), определяющая для каждого значения х частость
события {X < х}:
FZ{x)=P*{X<x\.	(7.2)
Для нахождения значений эмпирической функции удобно F*(x) за¬
писать в виде
р;м) =
где п — объем выборки, пх — число наблюдений, меньших х (х Е Ш).
Очевидно, что F*(x) удовлетворяет тем же условиям, что и истйн-
ная функция распределения F(x) (см. п. 2.3).
При увеличении числа п наблюдений (опытов) относительная ча¬
стота события {X < .х} приближается к вероятности этого события
(теорема Бернулли, п. 5.3). Эмпирическая функция распределения
F*(x) является оценкой вероятности события {Л" < #}, т.е. оценкой
теоретической функции распределения F(x) с. в. X. Имеет место
Теорема 7.1. Пусть F(x) — теоретическая функция распределения
X, a F*(x) — эмпирическая. Тогда для любого е > О
с. в.
lim P{\FZ(x)-F(x)\<e}^l.
п—>оо
Пример 7.4. Построить функцию F*(x). используя условие и резуль¬
таты примера 7.2.
О Здесь п = 10. Имеем ^(.х) =	= 0 при х ^ 0 (наблюдений мень¬
ше 0 нет); ^*0(х) = при 0 < х < 1 (здесь пх = 1) и т. д. Окончательно

Глава 7. Выборки и их характеристики ■ 219
получаем
'0,
при
X
<
0.
0,1,
при
0
<
X
1.
0,3,
при
1
<
X
<
2,
0,4,
при
2
<
X
<
з,
0.5,
при
3
<
X
<
4,
0,7,
при
4
<
X
<
5,
.1.
при
5
<
X.
График эмпирической функции распределения приведен на рис. 72.	•
7.4.	Графическое изображение статистического
распределения
Статистическое распределение изображается графически (для на¬
глядности) в виде так называемых полигона и гистограммы. Полигон,
как правило, служит для изображения дискретного (т. е. варианты от¬
личаются на постоянную величину) статистического ряда.
Рч|	Полигоном частот называют ломаную, отрезки которой соединя¬
ют точки с координатами (жх, ггх), (ж2, ^2)? • • • ? (#ь пк)\ полигоном ча¬
стостей — с координатами	(ж2,/>2),	(xk,Pk)-
Варианты (ж*) откладываются на оси абсцисс, а частоты и, соот¬
ветственно, частости — на оси ординат.

220 ■ Раздел второй. Основы математической статистики
Пример 7.5. Для примера 7.2 (п. 7.3) полигон частостей имеет вид,
изображенный на рис. 73.
Заметим, что р\ + р\ + ... + р% — 1.
Как видно, полигон частостей является статистическим аналогом
многоугольника распределения (см. п. 2.2).
Для неп}>ерывно распределенного признака (т. е. варианты могут
отличаться один от другого на сколь угодно малую величину) можно
построить полигон частот, взяв середины интервалов в качестве значе¬
ний жх,ж2, • • • , Более употребительна так называемая гистограмма.
Гистограммой частот (частостей) называют ступенчатую фигу¬
ру, состоящую из прямоугольников, основаниями которых служат ча-
стичные интервалы длины п. а высоты равны отношению 	плот-
ft
ность частоты или 	плотности частости).
h п • а
Очевидно, площадь гистограммы частот равна объему выборки, а
площадь гистограммы частостей равна единице.
Пример 7.6. Используя условие и результаты примера 7.3 из п. 7.3
построить гистограмму частостей.
о В данном случае длина интервала равна h = 6. Находим высоты
hi прямоугольников: h\ =	’,	« 0,022, h2 = - с ~ 0.028, hz = —- ~
0,033, hA =
0,23
6
0,038, Л5
0,17
6
= 0,028, Ле
= 0,017.
О
6 - g
Гистограмма частостей изображена на рис. 74.
Гистограмма частот является статистическим аналогом дифферен¬
циала функции распределения (плотности) f(x) с. в. X. Сумма площа-

Глава 7. Выборки и их характеристики ■ 221
дей прямоугольников равна единице
(h-^ + ...+h-^=pi + ...+pi = 1),
что соответствует условию
оо
J f(x) dx = 1
—ОО
для плотности вероятностей f(x) (см. п. 2.4). На рис. 74 показана и
плотность вероятностей /(ж).
Если соединить середины верхних оснований прямоугольников от¬
резками прямой, то получим полигон того же распределения.	•
7.5.	Числовые характеристики статистического
распределения
Для выборки можно определить ряд числовых характеристик, ана¬
логичным тем, что в теории ь^ ^юятностей определялись для случайных
величин (см. п. 2.5).
Пусть статистическое распределение выборки объема п имеет вид:
Xi
Xi
X2
^3
xk
Щ
П\
П2
™3
Пк
(7.3)
Выборочным средним хв называется среднее арифметическое всех
значений выборки:
xt
~ п ^2Xi *7н'
(7.4)
i-1

222 ■ Раздел второй. Основы математической статистики
Выборочное среднее можно записать и так:
к
i—1
т
где р* = — — частость. Для обозначения выборочного среднего ис¬
пользуют следующие символы: ж, М*(Х), т*.
Отметим, что в случае интервального статистического ряда в ра¬
венстве (7.4) в качестве берут середины его интервалов, а щ со¬
ответствующие им частоты.
Выборочной дисперсией DB называется среднее арифметическое
квадратов отклонений значений выборки от выбо^ючной средней жв,
т. е.
к
Du = h ~ х*)2'щ (7-6)
i— 1
или, что то же самое.
Оъ = Y^{xi-xB)2 -р*.	(7.7)
г—1
Можно показать, что DB может быть подсчитана также по формуле:
к
г=1
DB = x2 - (ж)2,	(7.8)
здесь х = хв.
Выборочное среднее квадратическое отклонение выборки опреде¬
ляется формулой
Ств = y/DB.	(7.9)
Особенность выборочного с. к. о. (сгв) состоит в том, что оно изме¬
ряется в тех же единицах, что и изучаемый признак.
При решении практических задач используется и величина
к
S2 =	■ ^2(xi - хв)2 ■ 7ii,	(7.10)
г=1
т. е.
S2 =	(7.11)

83 83
Глава 7. Выборки и их характеристики ■ 223
которая называется исправленной выборочной дисперсией (см. далее,
п. 8.1).
Величина
S=y/S?	(7.12)
называется исправленным выборочным средним квадратическим от¬
клонением.
Для непрерывно распределенного признака формулы для выбо¬
рочных средних будут такими же, но за значения х\,х2,... на¬
до брать не концы промежутков [#o,#i), [#ь#2)>	 а их середины
Хо +Х\ Х\+ Х2
2	’ 2
В качестве описательных характеристик вариационного ряда х^,
Ж(2)ч • • • ,#(п) (или полученного из него статистического распределения
выборки (7.3)) используется медиана, мода, размах вариации (выбор¬
ки) и т. д.
Размахом вариации называется число R = Х(п) — ж(х)> гДе х{\) —
= min Ж*;, Х(п\ = max XИЛИ R = #max“#min- Жтах — наибольший,
1<СА;^п
^min — наименьший вариант ряда.
Модой М* вариационного ряда называется вариант, имеющий наи¬
большую частоту.
Медианой М* вариационного ряда называется значение признака
(с. в. X), приходящееся на середину ряда.
Еслип = 2к (т.е. ряд .с(1),Ж(2), - • • ,Ж(*),Ж(А;+1), • • • ,Х{2к.) имеет четное
х(к) + х(к+1)
число членов), то М* —	^	5 если п = 2к + 1, то М* = Ж(а;+х).
Пример 7.7. По условию примера 7.2 из п. 7.3 найти характеристики
выборки — результаты тестирования 10 абитуриентов.
О	Используя формулы (7.4) (7.12) и определения из п. 7.5, находим:
хв =	• (0 • 1 + 1 * 2 + ... + 5 • 3) = 3,
DB = i((0-3)2-l + (1-3)2-2 + ... + (5-3)2-3) = 3,2,
ав = v^2 и 1.79.
S'2 = ^ • 3,2 » 3,56,
s = v/ibe « 1,87,
R = 5-0 = 5,
М* = 5,
м* =	=	3,5.	•

224 а Раздел второй. Основы математической статистики
Упражнения
1	- Найти и построить эмпирическую функцию распределения для вы¬
борки, представленной статистическим рядом.
Xi
1
3
6
щ
10
8
12
2.	На телефонной станции производились наблюдения за числом не¬
правильных соединений в минуту. Результаты наблюдений в тече¬
ние часа представлены в виде статистического распределения.
Xi
0
1
2
3
4
5
6
Щ
8
17
16
10
6
2
1
Найти выборочные среднее и дисперсию. Сравнить распределение
(	е~а	•	а
частостей с распределением Пуассона I pn,m ~	1-
3-	Изучается с. в. X — число выпавших очков при бросании игральной
кости. Кость подбросили 60 раз. Получены следующие результаты:
3,	2, 5,	6, 6, 1,	4,	6, 4,	6, 3, 6, 4,	2,	1, 5,	3,	1, 6, 4,
5,	4, 2,	2, 4, 2,	6,	3, 1,	5, 6, 1, 6,	6,	4, 2,	5,	4, 3, 6,
4,	1, 5,	6, 3, 2,	4,	4, 5,	2, 5, 6, 2,	3,	5, 4,	1,	2, 5, 3.
1. Что	в данном	опыте-наблюдении представляет генеральную со¬
вокупность? 2. Перечислите элементы этой совокупности. 3. Что
представляет собой выборка? 4. Приведите 1-2 реализации выбор¬
ки. 5. Оформите ее в виде: а) вариационного ряда; б) статистическо¬
го ряда. 6. Найдите эмпирическую функцию распределения выбор¬
ки. 7. Постройте интервальный статистический ряд. 8. Постройте
полигон частот и гистограмму частостей. 9. Найдите: а) выбороч¬
ную среднюю; б) выборочную дисперсию: в) исправленную выбо¬
рочную дисперсию и исправленное среднее квадратическое откло¬
нение; г) размах вариации, моду и медиану.

Глава 8
Элементы теории оценок и проверки гипотез
8.1.	Оценка неизвестных параметров
Понятие оценки параметров
Пусть изучается случайная величина X с законом распределения,
зависящим от одного или нескольких параметров. Например, это пара¬
метры а и а для нормального закона распределения.
Требуется по выборке Xi,X2,	Хп, полученной в результате п
наблюдений (опытов), оценить неизвестный параметр в.
Напомним, что Х\, Х2, - •., Хп — случайные величины: Х\ — ре¬
зультат первого наблюдения, Х2 — второго и т.д., причем с.в. X*,
г = 1,2,..., п, имеют такое же распределение, что и с. в. X; конкретная
выборка х\,х2,... ,хп — это значения (реализация) независимых с. в.
Статистической оценкой вп (далее просто оценкой в) параме¬
тра в теоретического распределения называют его приближенное зна¬
чение, зависящее от данных выбора.
Очевидно, что оценка в есть значение некоторой функции резуль¬
татов наблюдений над случайной величиной, т. е.
Функцию результатов наблюдений (т. е. функцию выборки) назы¬
вают статистикой.
Можно сказать, что оценка, в параметра в есть статистика, которая
в определенном смысле близка к истинному значению в.
Так, F*(x) есть оценка Fx(x)1 гистограмма — плотности f{x).
метр а в распределении Пуассона
или пара-
XbX2,...,Xn.
0 = 0(ХьХ2,...,Хп).
(8.1)

226 ■ Раздел второй. Основы математической статистики
Оценка в является случайной величиной, так как является функ¬
цией независимых с. в. Х\,	, Хп\ если произвести другую выборку,
то функция примет, вообще говоря, другое значение.
Если число опытов (наблюдений) невелико, то замена неизвестного
параметра в его оценкой 0, например математического ожидания сред¬
ним арифметическим, приводит к ошибке. Это ошибка в среднем тем
больше, чем меньше число опытов.
К оценке любого параметра предъявляется ряд требований, кото¬
рым она должна удовлетворять, чтобы быть «близкой» к истинному
значению параметра, т. е. быть в каком-то смысле «доброкачественной»
оценкой.
Свойства статистических оценок
О
О
О
Качество оценки определяют, проверяя, обладает ли она свойства¬
ми несмещенности, состоятельности, эффективности.
Оценка в параметра в называется несмещенной, если Мв = в.
Если Мв ф в, то оценка в называется смещенной.
Чтобы оценка в не давала систематической ошибки (ошибки одного
знака) в сторону завышения (Мв > в) или занижения (Мв < 0), на¬
до потребовать, чтобы «математическое ожидание оценки было равно
оцениваемому параметру».
Если Мвп —> 0, то оценка вп называется асимптотически несме¬
щенной.
Требование несмещенности особенно важно при малом числе на¬
блюдений (опытов).
Оценка вп параметра в называется состоятельной, если она схо¬
дится по вероятности к оцениваемому параметру:
вп-^0,
п—Уоо
т. е. для любого е > 0 выполнено
Иш р{\вп-в\ < Л = 1.
п—»оо I	)
Это означает, что с увеличением объема выборки мы все ближе
приближаемся к истинному значению параметра 0, т. е. практически
достоверно вп. ~ в.
Свойство состоятельности обязательно для любого правила оцени¬
вания (несостоятельные оценки не используются).

Глава 8. Элементы теории оценок и проверки гипотез ■ 227
Состоятельность оценки вп часто может быть установлена с помо¬
щью следующей теоремы.
Теорема 8.1. Если оценка вп параметра в является несмещенной и
D6n —» 0 при п —у оо, то вп — состоятельная оценка.
Q Запишем неравенство Чебышева для с. в. вп для любого е > 0:
D6n
Р{\вп - в\ < е) > 1 -
-2
Так как по условию lim D6n = 0, то lim Р(\вп — в\ < е) ^ 1. Но
п—юс	п-+ оо v
вероятности любого события не превышает 1 и, следовательно,
lim Р(\0п - 0\ < е) = 1,
П 	1.ЛЛ	х	'
71—>00
т. е. 6п — состоятельная оценка параметра в.	■
Несмещенная оценка вп параметра в называется эффективной,
если она имеет наименьшую дисперсию среди всех возможных несме¬
щенных оценок параметра в, т. е. опенка вп эффективна, если ее дис¬
персия минимальна.
Эффективную оценку в ряде случаев можно найти, используя не¬
равенство Рао-Крамера:
D°n > nTf’
где I = 1{в) — информация Фишера, определяемая в дискретном слу¬
чае формулой
2
1 = м[^1пр(Х,в)\ = ]Г
г= 1
Рв(Хг,0)
_р(хг,в)
где р(х, в) = р{Х = ж}, а в непрерывном — формулой
оо
=мШы«х-М2=I [ж
о)
0)J
2
• f(x,0)dx,
—	ОО
где f(xy6) — плотность распределения н. с. в. X.
Эффективность оценки определяется отношением

228 в Раздел второй. Основы математической статистики
где — эффективная оценка ^D6^	т^ем ближе effвп к 1,
тем эффективнее оценка вЕсли efF вп —> 1 при п	оо, то оценка
называется асимптотически эффективной.
Отметим, что на практике не всегда удается удовлетворить всем
перечисленным выше требованиям (несмещенность, состоятельность,
эффективность), и поэтому приходится довольствоваться оценками, не
обладающими сразу всеми тремя свойствами. Все же три свойства, как
правило, выделяют оценку однозначно.
Точечные оценки математического ожидания и дисперсии
Пусть изучается с. в. X с математическим ожиданием а — MX и
дисперсией DX; оба параметра неизвестны.
Статистика, используемая в качестве приближенного значения не¬
известного параметра генеральной совокупности, называется ее точеч¬
ной оценкой. То есть точечная оценка характеристики генеральной со¬
вокупности — это число, определяемое по выборке.
Пусть rci,X2,. • • ,хп — выборка, полученная в результате проведе¬
ния п независимых наблюдений за с. в. X. Чтобы подчеркнуть случай¬
ный характер величин жх,ж2,	хп, перепишем их в виде А’ь-Хг,...
... , ХПу т. е. под Xi будем понимать значение с. в. X в г-м опыте.
Случайные величины Х\,Х2, • • ■ ,Хп можно рассматривать как п не¬
зависимых «экземпляров» величины X. Поэтому МХ\ — MX2 — ...
... = МХп = MX = a, DX\ = DX2 = ... = DXn = DX.
Теорема 8.2. Пусть Х\,Х2, • • -, Хп — выборка из генеральной совокуп¬
ности и MXi = MX = a, DXi = DX (г = 1,п). Тогда выборочное среднее
п
Хв — ^ ^2 Xi — несмещенная и состоятельная оценка математическош
г=1
ожидания MX.
□	Найдем м. о. оценки Хв:
м*‘=м(я х>) =>(£*<)=я Ё мл'<=я '»■<*=»•
4 i= 1	7	^г=1	7	г=1
Отсюда по определению получаем, что Л”в — несмещенная оценка MX.
Далее, согласно теореме Чебышева (п. 5.2), для любого е > 0 имеет

Глава 8. Элементы теории оценок и проверки гипотез ■ 229
место равенство
<4 = i.
Ч г= 1	г= 1	}
которое, согласно условию теоремы, можно переписать так:
lim Р{\ХВ-МХ\ <е}^1
71-+00	1	J
или, что то же самое, lim Р {\в — в\ < е\ — Согласно определению
		п	>ос	L	J
получаем, что Хв — состоятельная оценка MX.	■
Можно показать, что при нормальном распределении с. в. X эта
оценка, т.е. Хв, будет и эффективной. На практике во всех случа¬
ях в качестве оценки математического ожидания используется среднее
арифметическое, т.е. Хв.
В статистике оценку математического ожидания принято обозна¬
чать через X или Хв, а не X.
Покажем, что
MDB = r±^DX.	(8.2)
Действительно,
wd. = «(sE№-7)2) ="(нЕ-#-(вЁ*)2) =
= йм(Ёх?) - ЛМ(Ё = Я' м<*? + Х1 + ■ ■ • +
г=1	' П	г=1	'
-	•	M{Xi + х2 +... + хп)2 = Umx? + M*f + ... + ШГ*)-
п,
±-м(х21+Х%+...+Х*+2(Х1Х2 + Х1Хз + Х2Хз + ... + Хп хХп)) =
Т),	4	4	~	' /
п
с2
п
- 4т • (МХг ■ МХ2 + МХ1МХ3 + МХ2МХ3 + ... + MXn-iMXn) =
гг
=	■	(MX2	+	MX2	+	...	+	MX2)-
п	4	v	'
ть
-	~(МХ ■ MX + MX ■ MX + ... + MX ■ MX) =
n
• n • MX2 - \ •n(n ~ ^ -(MX)2 =	-
ГГ	ГГ	£	iL

230 " Раздел второй. Основы математической статистики
= \^{МХ2 - (MX)2) =	•	DX.
Из равенства (8.2) следует, что MDB ф DX, т. е. выборочная дис¬
персия является смещенной оценкой дисперсии DX. Поэтому выбороч¬
ную дисперсию исправляют, умножив ее на п ^, получая формулу
S2 = ^Ь[А> (см. (7.11)).
Теорема 8.3. Пусть Х2,..., Хп — выборка из генеральной совокуп¬
ности и MXi — MX — «, DXi = DX (г = 1,п). Тогда исправленная
п
выборочная дисперсия S2 = n ^ ^	—	X)2	—	п	П	i	•	DB	—	несмещен-
г=1
ная состоятельная оценка дисперсии DX.
Q Примем без доказательства состоятельность оценки S2. Докажем
ее несмещенность.
Имеем
MS'
= М (-^-тА.) = -*4- • МВв = -2-- ■ 1^—^DX = DX,
\п — 1	/	гг — 1	п — 1 п
т. е. MS'2 = DX. Отсюда по определению получаем, что S'2 — несме¬
щенная оценка DX.	■
Отметим, что при больших значениях п разница между DB и 52
очень мала и они практически равны, поэтому оценку S'2 используют
для оценки дисперсии при малых выборках, обычно при п 30.
Имеют место следующие теоремы.
Теорема 8.4. Относительная частота ^ появления события А в п не¬
зависимых испытаниях является несмещенной состоятельной и эффек¬
тивной оценкой неизвестной вероятности р — Р(А) этого события (р —
вероятность наступления события А в каждом испытании).
Отметим, что состоятельность оценки в = ^ непосредственно вы¬
текает из теоремы Бернулли (см. п. 5.3).
Теорема 8.5. Эмпирическая функция распределения выборки F*(x)
является несмещенной состоятельной оценкой функции распределения
F(x) случайной величины X.

Глава 8. Элементы теории оценок и проверки гипотез "231
Пример 8.1. Монету подбрасывают п раз. Вероятность выпадения гер¬
ба при каждом подбрасывания равна р. В ходе опыта монета выпала
гербом па раз. Показать несмещенность оценки 6 = вероятности
6	= р выпадения герба в каждом опыте.
О Число успехов (па) имеет распределение Бернулли. Тогда М(па) =
—	пр, D(ua) = npq = пр( 1 — р). Следовательно, Мв = М	—
= — ■ М(п4) = — - п • р — р = 6, т. е. оценка в =	— несмещенная. •
8.2.	Методы нахождения точечных оценок
Рассмотрим наиболее распространенные методы получения точеч¬
ных оценок параметров распределения: метод моментов и метод макси¬
мального правдоподобия (кратко: ММП) и метод наименьших квадра¬
тов (кратко МНК).
Метод моментов
Метод моментов для нахождения точечных оценок неизвестных
параметров заданного распределения состоит в приравнивании теоре¬
тических моментов распределения соответствующим эмпирическим
моментам, найденных по выборке.
Так если распределение зависит от одного параметра 6 (например,
задан вид плотности распределения /(ж, б)), то для нахождения его
оценки надо решить относительно 6 одно уравнение:
MX = ХВ
оо
(MX = J х • /(а*, 6) dx = (р(в) есть функция от 6).
—оо
Если распределение зависит от двух параметров (например, вид
плотности распределения /(ж, 61,62)) — надо решить относительно в\
и 62 систему уравнений:
Гmx = xbj
[DX = DB.

232 ■ Раздел второй. Основы математической статистики
И, наконец, если надо оценить п параметров в\,&2, ■ ■ • *6п
решить одну из систем вида:
надо
мх = ±£хи
i= 1
п
MX2 4ЕXI
г=]
ИЛИ
мхк = ^ £ xi-
г= 1
MX = X,
DX = DB,
M(X - MX)k = i Y.(Xi - Хв)*.
l
Z=1
Метод моментов является наиболее простым методом оценки пара¬
метров. Он был предложен в 1894 г. Пирсоном. Оцеики метода момен¬
тов обычно состоятельны, однако их эффективность часто значительно
меньше единицы.
Пример 8.2. Найти оценки параметров нормального распределения
с. в. X методом моментов.
О Требуется по выборке	найти точечные оценки неиз¬
вестных параметров а = МХ = в\ и о — \JDX — 62.
По методу моментов приравниваем их, соответственно, к выбороч¬
ному среднему и выборочной дисперсии («i = MX — начальный мо¬
мент I порядка, /Х2 = DX — центральный момент И порядка). Полу¬
чаем
f МХ = хв,
[DX = DB.
т. е.
{а = жв,
а2 - DB.
Итак, искомые оценки параметров нормального распределения:
в\ = Хв И 62 = у/Db-	•
Метод максимального правдоподобия
Пусть Х\.Х2, - - ■ ухп — выборка, полученная в результате проведе¬
ния п независимых наблюдений за с. в. X. И пусть вид закона рас¬
пределения величины X, например, вид плотности /(ж,0), известен, но

Глава 8. Элементы теории оценок и проверки гипотез ■ 233
неизвестен параметр в, которым определяется этот закон. Требуется
по выборке оценить параметр в.
В основе метода максимального правдоподобия (ММП), предло¬
женною Р. Фишером, лежит понятие функции правдоподобия.
Функцией правдоподобия, построенной по выборке х\, х2, • *., хп<, на¬
зывается функция аргумента в вида
Цхих2,...,хп;в) = f(xi,6) • /(*2,6») •... ■ f(xn.в)
или
L(x.e) = Y[f(xi,6),
i= 1
где f(x,6) — плотность распределения с. в. X в случае, если X — не¬
прерывная. Если X — дискретная с. в., то функция правдоподобия име¬
ет вид
п
Ь(х,в) = р(хив) -р(х2,6) - ... -р(хп,в) = Цр(ж*,0),
г=1
где p(xi,9) = Р{Х = xi,9}.
Из определения следует, что чем больше значение функции L(x, б),
тем более вероятно (правдоподобнее) появление (при фиксированном
в)	в результате наблюдений чисел х\,х2, ■ • ■ ,хп.
За точечную оценку параметра 0, согласно ММП, берут такое
его значение 0, при котором функция правдоподобия достигает мак¬
симума.
Эта оценка, называемая оценкой максимального правдоподобия,
является решением уравнения
dL(x^e)
<10
_ = U.
0=0
Так как функции L{x,6) и In L(x,6) достигают максимума при од¬
ном и том же значении б, то вместо отыскания максимума функции
L(x, в) ищут (что проще) максимум функции In L(x, в).
Таким образом, для нахождения оценки максимального правдопо¬
добия надо:
1.	решить уравнение правдоподобия
d{]nL(x,6))
d6
~ = 0;
6=6

234 а Раздел второй. Основы математической статистики
2.	отобрать то решение, которое обращает функцию InL(x,6) в мак¬
симум (удобно использовать вторую производную: если
сР InЬ(х,в)
дВ2
е=е
<0,
то в = в — точка максимума).
Если оценке подлежат несколько параметре в\, 02 5 • • •, 0п распре¬
деления, то оценки 0i,..., вп определяются решением системы уравне¬
ний правдоподобия:
(д(1п L)
двх
Э(1п L)
двп
= 0,
= 0.
Пример 8.3. Найти оценку параметра а распределения Пуассона ме¬
тодом максимального правдоподобия.
п с*—
о	В данном случае Р{Х = т} =	——. Поэтому
р(хьв) = Р{Х = Хг,в} =
Xj.
при Xj € N. Составляем функцию правдоподобия (для дискретной
с. в. X):
Ux е) = в^-е-в	Ex.	j	
1’ ] xi\	х2\	xn\	Xl\-...-xnV
Тогда
71
In Ь(хув) = —п • 0 + ^2 хг * 1п0 — ln(o;i! • х21 *... • хп\)
i=1
и
dlnL(x*,0)
йв в
i— 1
Уравнение правдоподобия имеет вид:

Глава 8. Элементы теории оценок и проверки гипотез ■ 235
Отсюда находим
п
А так как
cPln L(x,6)	_	i ^
Г)	~	^	U,
М2	0=0	<92^
г=1
то оценка в = .тв является оценкой максимального правдоподобия.
Метод наименьших квадратов
Метод нахождения оценки в неизвестного параметра 0, основанный
на минимизации суммы квадратов отклонений выборочных данных от
определяемой (искомой) оценки в, называется методом наименьших
квадратов (коротко: МНК).
Другими словами, в МНК требуется найти такое значение 0, кото¬
рое минимизировало бы сумму
г=1
Отметим, что МНК является наиболее простым методом нахождения
оценок параметра в.
Пример 8.4. Найти оценку параметра а распределения Пуассона ме¬
тодом наименьших квадратов.
Итак, в — а — хъ.
п
F(6) =	—	в)2	-*	min.
п
п
из уравнения F'(0) = 0 находим критическую точку: —2 ^^(А$ — 0) = О,
г—1
п
п
Т1
п
т. е.	Xi — в = 0, т. е. Х{ = пв, 0кр = ^	А^.	А	так	как

236 ■ Раздел второй. Основы математической статистики
п
при любом значении 0, то вкр = ~ Х{ — точка минимума функ-
г—1
ции F(0). Таким образом, оценкой параметра а в распределении Пуас-
птяр—а
сона Р(т:а) = 	Ц—, га = 0,1,2	 согласно МНК, является
га!
г—1
Можно доказать, что:
М(б)=б = а, £>(0) = |.	•
Упражнения
1.	Найти оценку параметра распределения Пуассона методом момен-
тов.
2-	Пользуясь ММП, оценить вероятность появления герба, если при
10 бросаниях монеты герб появился 6 раз.
3.	Найти оценку неизвестной вероятности успеха в схеме Бернулли
методом моментов и ММП.
4-	Дано: с. в. X ~ Д[а, Ь]. По выборке х],	• • *, хп оценить величины
а и b методом моментов.
5-	Найти оценки параметров нормального распределения с. в. X ме¬
тодом максимального правдоподобия.
8.3.	Понятие интервального оценивания параметров
Точечные оценки неизвестного параметра в хороши в качестве пер¬
воначальных результатов обработки наблюдений. Их недостаток в том,
что неизвестно, с какой точностью они дают оцениваемый параметр.

Глава 8. Элементы теории оценок и проверки гипотез ■ 237
Для выборок небольшого объема вопрос о точности оценок очень су¬
щественен, так как между в и в может быть большое расхождение в
этом случае. Кроме того, при решении практических задач часто тре¬
буется определить и надежность этих оценок. Тогда и возникает зада¬
ча о приближении параметра в не одним числом, а целым интервалом
(<М2).
Оценка неизвестного параметра называется интервальной, если
она определяется двумя числами — концами интервала.
Задачу интервального оценивания можно сформулировать так: по
данным выборки построить числовой интервал (#1,02)? относительно
кото{юго с заранее выбранной вероятностью 7 можно сказать, что вну¬
три этого интервала находится точное значение оцениваемого парамет¬
ра (см. рис. 75).
О
к	е2	»
Рис. 75
Интервал (#ь#2)> накрывающий с вероятностью 7 истинное зна¬
чение параметра в. называется доверительным интервалом, а вероят¬
ность 7 — надежностью оценки или доверительной вероятностью.
Очень часто (но не всегда) доверительный интервал выбирается
симметричным относительно несмещенной точечной оценки 0, т. е. вы¬
бирается интервал вида (в — е,в 4- е) такой, что
р{ве(6-е,в + £)} = р{|0-0| <е} = 7.
Чигло е > 0 характеризует точность оценки: чем меньше разность
\в — 0|, тем точнее оценка.
Величина 7 выбирается заранее, ее выбор зависит от конкретно
решаемой задачи. Так, степень доверия авиапассажира к надежности
самолета, очевидно, должна быть выше степени доверия покупателя к
надежности телевизора, лампочки, игрушки... Надежность 7 принято
выбирать равной 0,9; 0,95; 0,99 или 0,999. Тогда практически достоверно
нахождение параметра в в доверительном интервале (в — е, в + е).

238 а Раздел второй. Основы математической статистики
8.4.	Доверительные интервалы для параметров
нормального распределения
Построим доверительные интервалы для параметров нормального
распределения, т. е. когда выборка производится из генеральной сово¬
купности, имеющей нормальное распределение с параметрами а и ст.
Доверительный интервал для математического ожидания при
известной дисперсии
Пусть с. в. X rsj N(a, а); а — известна, доверительная вероятность
(надежность) 7 — задана.
Пусть £*i, #2? • ♦ -, %п — выборка, полученная в результате проведе¬
ния п независимых наблюдений за с. в. X. Чтобы подчеркнуть случай¬
ный характер величин xj, ж2, • • • > перепишем их в виде Xi,X2,...
... ,ХП, т. е. под Xi будем понимать значение с. в. X в г-м опыте.
Случайные величины Х\, Х^ .. -, Хп — независимы, закон распреде¬
ления любой из них совпадает с законом расщэеделения с. в. X (т. е.
Xi ~ iV(a, ст)). А это значит, что МХ\ — МХ4 — ... = МХп — MX = а,
DXx = DX2 = ... = DXn = DX.
Выборочное среднее
7=1
также будет распределено по нормальному закону (примем без дока¬
зательства). Параметры распределения X таковы: М(Х) = a. D(X) =
СГ2
= —. Действительно.
М(Х) = м(±£хЛ = 1-£м* = 1 -^MX = i.nMX = a,
'	*=1	'	г=1	г= 1
4	г=1	7	г=1	г=1
Таким образом, X ~ iV ( a, ).
V \/п/
Следовательно, пользуясь формулой
р{|А:-в|<0 = 2Фо(^)=2ф(1)-1
n-DX = £.

Глава 8. Элементы теории оценок и проверки гипотез ■ 239
(формула (2.47)), можно записать
7 - Р{|Х - а\ <е} = 2Ф0	=	2Ф0(<),
*	£ ' ^ IX
где г = ———. Из последнего равенства находим
е =	(8-3)
yjn
поэтому 7 = Р < \Х — о| <	>	=	2Фо(<)	или
I	V«J
p\x-t--^<a<X + t-^= 1=2Ф0(*)=7-	(М
I	\/n	x/nj
В соответствии с определением доверительного интервала получа¬
ем, что доверительный интервал для a, = MX есть
(x.-t-^x + t--^),
V у/п	y/nj
где t определяется из равенства (8.4), т. е. из уравнения
(8.5)
Фо«) - \	(8-6)
1	+ 7
(или Ф(£) = —2—); ПРИ заданном 7 по таблице функции Лапласа на¬
ходим аргумент t.
Заметим, что из равенства (8.3) следует: с возрастанием объема
выборки п число е убывает и, значит, точность оценки увеличивается;
увеличение надежности 7 влечет уменьшение точности оценки.
Пример 8.5. Произведено 5 независимых наблюдений над с. в. X ^
^ N(a, 20). Результаты наблюдений таковы: х,\ = —25, х2 = 34.	=
= —20, х’4 = 10, £5 = 21. Найти оценку для а = MX, а также построить
для него 95%-й доверительный интервал.
О	Находим сначала хв: х = р • (—25 + 34 — 20 + 10 + 21) = 4, т. е.
		^7
х = 4. Учитывая, что 7 = 0,95 и Фо(£) — ^ получаем Фо(£) — 0,475.
По таблице (см. Приложение) выясняем, что t = £7 = 1,96. Тогда е =
1	96 .20
= ———— % 17,5 (формула (8.3)). Доверительный интервал для а =
л/5
= MX (согласно (8.5)) таков: (4 — 17,5;4 + 17,5), т. е. (—13,5:21,5).	•

240 ■ Раздел второй. Основы математической статистики
Доверительный интервал для математического ожидания при
неизвестной дисперсии
Пусть с. в. X ~ N(a, а), а — неизвестна, 7 — задана. Найдем такое
число £, чтобы выполнялось соотношение Р{Х —е<а<Х + е} = 7
или
Р{\Х-а\<е}=1.	(8.7)
Введем случайную величину
S	’
,/п
где S — исправленное Сиднее квадратическое отклонение с. в. X, вы¬
численное по выборке:
S =
\
п
г-1
Доказывается, что с. в. Т имеет распределение Стьюдента (см.
п. 4.3) сп-1 степенью свободы. Плотность этого распределения имеет
вид:
Щ)
fT(t,n - 1) =
yjir(п - 1) • Г
ОО
где Г(р) = J ир~1 ■ e~udu — гамма-функция; /у(£,п — 1) — четная
о
функция.
Перейдем в левой части равенства (8.7) от с. в. X к с. в. Т:
JL	s
у/п	у/п J
или Р ||Т| <	—	7	или	Р{\Т\	<	М	=	Т>	гДе
£ ■ \/п
(8.8)

Глава 8. Элементы теории оценок и проверки гипотез ■ 241
Величина t7 находится из условия
'7
'7
Р{\т\ < £у} = J fritn - l)clt = 2 • J frit, n - l)dt = 7,
о
т. e. из равенства
'7
2 • J hit, n-1) dt = 7.
о
Пользуясь таблицей квантилей распределения Стьюдента (см. прило¬
жение 4 на с. 287), находим значение £7 в зависимости от доверительной
вероятности 7 и числа степеней свободы п — 1 (£7 — квантиль уров¬
ня 1 — 7).
Определив значение £7, из равенства (8.8), находим значение е:
Следовательно, равенство (8.7) принимает вид
покрывает а = MX с вероятностью 7, т.е. является доверительным
интервалом для неизвестного математического ожидания с. в. X.
Пример 8.6. По условию примера 8.5, считая, что с. в. X ~ N(a,o),
построить для неизвестного MX = а доверительный интервал. Считать
7 = 0,95.
Q Оценку х для MX уже знаем: х = 4. Находим значение S:
s2 = I ((-25 - 4)2 • 1 + (34 - 4)2 + (-20 - 4)2 + (10 - 4)2 + (21 - 4)2) =
= 660,5; S % 25,7. По таблице для 7 = 0,95 и п—1=4 находим Ц = 2,78.
Р hY - i, • — < х — } = 7-
{
}
А это значит, что интервал
Следовательно, е = 2,78 • 2^4
« 31,9. Доверительный интервал таков:
(-27,9; 35,9).

242 а Раздел второй. Основы математической статистики
Доверительный интервал для среднего квадратического
отклонения нормального распределения
Пусть с. в. X ~ N(a,o), а — неизвестно, 7 — задано. Можно по¬
казать, что если MX = а известно, то доверительный интервал для
среднего квадратического отклонения а имеет вид:
/ v/n ♦ S0 л/га • 50\
\ Х2 1 Xi j’
1	п
где п — объем выборки, Sq — п ^	~	а)2’	a
22	22
Xi = xi + 7 ; Х2 = х 1 — 7
2	2	;n
являются квантилями х2_РаспРеДеления с п степенями свободы (см.
п. 4.3), определяемые по таблице квантилей Ха п распределения Хп (см-
приложение 3 на с. 286).
Если а — MX неизвестно, то доверительный интервал для неиз¬
вестного о имеет вид:
(
\/п — 1 ■ S \/п — IS
Х2 ’ Xi
где п — объем выборки, S =
\
и
—у * ^^(Xi — X)2 — исправленное
п
г=1
среднее квадратическое отклонение, квантили
2 2	2	2
Xi=Xi+7	1 Х2 ~ X1 — 7
2	1	2	1
й	2	I	1	1+7	1-7
определяются по таблице ха к ПРИ к = п — 1 и а = —^— и а = —^—
соответственно.
Пример 8.7. Для оценки параметра нормально распределенной слу¬
чайной величины была сделана выборка объема в 30 единиц и вычи¬
слено S = 1,5. Найти доверительный интервал, покрывающий о с ве¬
роятностью 7 = 0,90.
о	Имеем п = 30, 7 = 0,9. По таблице Ха к находим
Х? = Х21 + о,9	=х2(0,95;29)	=	17,7,
—2——;30—1

Глава 8. Элементы теории оценок и проверки гипотез ■ 243
xi = х! - 0,9	=	Х2(0,05;	29)	=	42,6.
Доверительный интервал имеет вид:
(у/т
V \
у/30 - 1 • 1,5 \/30 — 1 • 1,5 \
у/Ш ’ VT7J )
или 1.238 < а < 1,920.
Скажем несколько слов о доверительном интервале для оценки ве¬
роятности успеха при большом числе испытаний Бернулли.
Доверительный интервал, который с надежностью 7 покрывает
оцениваемый параметр р при больших значениях п (порядка сотен),
имеет вид (рьрг), где
где р* = —	относительная частота события А; t определяется из
равенства 2Фо(£) = 7.
Для оценки приближенного равенства р zz р* можно использовать
1-	Глубина моря измеряется прибором, систематическая ошибка ко¬
торого равна нулю, а случайные ошибки распределены нормально
с а = 15 м. Сколько надо сделать независимых измерений, чтобы
определить глубину моря с ошибкой не более 5 м при надежности
2.	По условию примера 7.3 найти точечную оценку и доверительный
интервал для среднего роста студентов, считать 7 = 0,95.
3- Производятся независимые испытания с одинаковой, но с неизвест¬
ной вероятностью р появления события А в каждом испытании.
Найти доверительный интервал для оценки р с надежностью 0,95,
если в 400 испытаниях события А появилось 80 раз.
(8.10)
Упражнения
7 = 0,9?

244 ■ Раздел второй. Основы математической статистики
8.5.	Проверка статистических гипотез
Задачи статистической проверки гипотез
Одна из часто встречающихся на практике задач, связанных с при¬
менением статистических методов, состоит в решении вопроса о том,
должно ли на основании данной выборки быть принято или, напро¬
тив, отвергнуто некоторое предположение (гипотеза) относительно ге¬
неральной совокупности (случайной величины).
Например, новое лекарство испытано на определенном числе лю¬
дей. Можно ли сделать по данным результатам лечения обоснованный
вывод о том, что новое лекарство более эффективно, чем применявшие¬
ся ранее методы лечения? Аналогичный вопрос логично задать, говоря
о	новом правиле поступления в вуз, о новом методе обучения, о пользе
быстрой ходьбы, о преимуществах новой модели автомобиля или тех¬
нологического процесса и т.д.
Процедура сопоставления высказанного предположения (гипотезы)
с выборочными данными называется проверкой гипотез.
Задачи статистической проверки гипотез ставятся в следующем
виде: относительно некоторой генеральной совокупности высказыва¬
ется та или иная гипотеза Н. Из этой генеральной совокупности из¬
влекается выборка. Требуется указать правило, при помощи которого
можно было бы по выборке решить вопрос о том, следует ли отклонить
гипотезу Н или принять ее.
Следует отметить что статистическими методами гипотезу можно
только опровергнуть или не опровергнуть, но не доказать. Например,
для проверки утверждения (гипотеза Н) автора, что «в рукописи нет
ошибок», рецензент прочел (изучил) несколько страниц рукописи.
Если он обнаружил хотя бы одну ошибку, то гипотеза Н отверга¬
ется, в противном случае - не отвергается, говорят, что «результат
проверки с гипотезой согласуется».
Выдвинутая гипотеза может быть правильной или неправильной,
поэтому возникает необходимость ее проверки.
Статистическая гипотеза. Статистический критерий
■=>
Под статистической гипотезой (или просто гипотезой) понима¬
ют всякое высказывание (предположение) о генеральной совокупности,
проверяемое по выборке.

Глава 8. Элементы теории оценок и проверки гипотез ■ 245
Статистические гипотезы делятся на гипотезы о параметрах рас¬
пределения известного вида (это так называемые параметрические ги¬
потезы) и гипотезы о виде неизвестного распределения (нспараметри-
ческие гипотезы).
Одну из гипотез выделяют в качестве основной (или нулевой) и
обозначают Но, а другую, являющуюся логическим отрицанием Но,
т. е. противоположную Но — в качестве конкурирующей (или альтер¬
нативной) гипотезы и обозначают Н\.
Гипотезу, однозначно фиксирующую распределение наблюдений,
называют простой (в ней идет речь об одном значении параметра), в
противном случае	сложной.
Например, гипотеза Но, состоящая в том что математическое ожи¬
дание с.в. X равно «0) т.е. MX = ао, является простой. В качестве
альтернативной гипотезы можно рассматривать одну из следующих ги¬
потез: Ну. MX > ао (сложная гипотеза), Ну. MX < ао (сложная), Н\:
MX ф ао (сложная) или Ну. MX = а\ (простая гипотеза).
Имея две гипотезы Но и Н\, надо на основе выборки Х^... ,Хп
принять либо основную гипотезу Но, либо конкурирующую Н\.
Правило, по которому принимается решение принять или откло¬
нить гипотезу Но (соответственно, отклонить или принять Н\), назы¬
вается статистическим критерием (или просто критерием) проверки
гипотезы Но.
Проверку гипотез осуществляют на основании результатов выбор¬
ки Х\,Х2,. -. )Хп, из которых формируют функцию выборки Тп =
= Т(Х 1,^2,..., хп)щ называемой статистикой критерия.
Основной принцип проверки гипотез состоит в следующем. Мно¬
жество возможных значений статистики критерия Тп разбивается на
два непересекающихся подмножества: критическую область S, т. е.
область отклонения гипотезы Но и область S принятия этой гипоте¬
зы. Если фактически наблюдаемое значение статистики критерия (т. е.
значение критерия, вычисленное по выборке: Тнабл = Т(хх,х2, • • • ,xn))
попадает в критическую область S, то основная гипотеза Но отклоняет¬
ся и принимается альтернативная гипотеза Ну если же Тнабл попадает
в 5, то принимается Но, а Н\ отклоняется.
При проверке гипотезы может быть принято неправильное реше¬
ние, т. е. могут быть допущены ошибки двух родов:
Ошибка первого рода состоит в том, что отвергается нулевая гипо¬
теза Но, когда на самом деле она верна.
Ошибка второго рода состоит в том, что отвергается альтернатив¬
ная гипотеза Hi, когда она на самом деле верна.

246 в Раздел второй. Основы математической статистики
Рассматриваемые случаи наглядно иллюстрирует следующая таб¬
лица.
верна
неверна
Отвергается
ошибка 1-го рода
правильное решение
Принимается
правильное решение
ошибка 2-го рода
Вероятность ошибки 1-го рода (обозначается через а) называется
уровнем значимости критерия.
Очевидно, а = P(Hi\Hq). Чем меньше а, тем меньше вероятность
отклонить верную гипотезу. Допустимую ошибку 1-го рода обычно за¬
дают заранее.
В одних случаях считается возможным пренебречь событиями, ве¬
роятность которых меньше 0,05 (а = 0,05 означает, что в среднем в 5
случаях из 100 испытаний верная гипотеза будет отвергнута), в других
случаях, когда речь идет, например, о разрушении сооружений, гибе¬
ли судна и т. п., нельзя пренебречь обстоятельствами, которые могут
появиться с вероятностью, равной 0,001.
Обычно для а используются стандартные значения: а = 0,05;
а = 0,01; 0,005; 0,001.
Вероятность ошибки 2-го рода обозначается через /3, т.е. /3 =
= P(H0|#i).
Величину 1 — /3, т. е. вероятность недопущения ошибки 2-го рода
(отвергнуть неверную гипотезу принять верную Hi), называется
мощностью критерия.
Очевидно, 1- /3 = P(Hi\Hi) = Р((хi,x2,	хп) 6 5|ЯХ).
Чем больше мощность критерия, тем вероятность ошибки 2-го рода
меньше, что, конечно, желательно (как и уменьшение а).
Последствия ошибок 1-го, 2-го рода могут быть совершенно раз¬
личными: в одних случаях надо минимизировать а, в другом — /3.
Так, применительно к радиолокации говорят, что а — вероятность
пропуска сигнала, /3 — вероятность ложной тревоги; применительно
к производству, к торговле можно сказать, что а — риск поставщика
(т.е. забраковка по выборке всей партии изделий, удовлетворяющих
стандарту), /3 — риск потребителя (т. е. прием по выборке всей партии
изделий, не удовлетворяющей стандарту); применительно к судебной
системе, ошибка 1-го рода приводит к оправданию виновного, ошибка
2-го рода — осуждению невиновного.
Отметим, что одновременное уменьшение ошибок 1-го и 2-го рода
возможно лишь при увеличении объема выборок. Поэтому обычно при
заданном уровне значимости а отыскивается критерий с наибольшей
мощностью.

Глава 8. Элементы теории оценок и проверки гипотез ■ 247
Методика проверки гипотез сводится к следующему:
1.	Располагая выборкой Х\, Х2,. •., Хп, формируют нулевую гипотезу
Но и альтернативную Н\.
2.	В каждом конкретном случае подбирают статистику критерия Тп =
—	T(Xi, Х*2,	Хп), обычно из перечисленных ниже: U — нормаль¬
ное распределение, %2 — распределение хи-квадрат (Пирсона), t —
распределение Стьюдента, F — распределение Фишера Снедекора.
3.	По статистике критерия Тп и уровню значимости а определяют
критическую область S (и S). Для ее отыскания достаточно найти
критическую точку £кр, т. е. границу (или квантиль), отделяющую
область S от S.
Границы областей определяются, соответственно, из соотношений:
Р(Тп > tKр) = а, для правосторонней критической области S
(рис. 76); Р(Тп < tKр) = а, для левосторонней критической обла-
сти S (рис. 77); Р{Тп < t£p) = Р(Тп > tjjp) = т|, для двусторонней
критической области S (рис. 78).
Рис. 76
Для каждого критерия имеются соответствующие таблицы, по ко¬
торым и находят критическую точку, удовлетворяющую приведен¬
ным выше соотношениям.
4.	Для полученной реализации выборки х = (жх,ж2, •.., хп) подсчи¬
тывают значение критерия, т. е. ТнабЛ = Т(хi, #2, • • •,хп) = t.
5.	Если t Е S (например, t > tKp для правосторонней области S'), то
нулевую гипотезу Щ отвергают; если же t 6 S (t < £кр), то нет
оснований, чтобы отвергнуть гипотезу Щ.

248 а Раздел второй. Основы математической статистики
Рис. 77
Рис. 78
8.6.	Проверка гипотез о законе распределения
Во многих случаях закон распределения изучаемой случайно вели¬
чины неизвестен, но есть основания предположить, что он имеет вполне
определенный вид: нормальный, биномиальный или какой-либо дру¬
гой.
Пусть необходимо проверить гипотезу Щ о том, что с. в. X под¬
чиняется определенному закону распределения, заданному функцией
распределения fo(x), т. е. Щ: F\(x) = Fo(a;). Под альтернативной гипо¬
тезой Н1 будем понимать в данном случае то, что просто не выполнена
основная (т.е. Н\: Fx(x) / Fq(x)).

Глава 8. Элементы теории оценок и проверки гипотез ■ 249
Для проверки гипотезы о распределении случайной величины X
проведем выборку, которую оформим в виде статистического ряда:
(8.11)
га
где ni — п — объем выборки.
i—1
Требуется сделать заключение: согласуются ли результаты наблю¬
дений с высказанным предположением. Для этого используем специ¬
ально подобранную величину — критерий согласия.
Критерием согласия называют статистический критерий проверки
гипотезы о предполагаемом законе неизвестного распределения. (Он
используется для проверки согласия предполагаемого вида распреде¬
ления с опытными данными на основании выборки.)
Существуют различные критерии согласия: Пирсона. Колмогорова,
Фишера, Смирнова и др.
Критерий согласия Пирсона — наиболее часто употребляемый кри¬
терий для проверки простой гипотезы о законе распределения.
Критерий х2 Пирсона
Для проверки гипотезы Н$ поступают следующим образом.
Разбивают всю область значений с. в. X на т интервалов Al,
Д2,...,Дт и подсчитывают вероятности р(г = 1,2, . ..,т) попа¬
дания с. в. X (т. е. наблюдения) в интервал Д7;, используя формулу
Р{а ^ X ^ /3} = Fq(P) — Fq(ix). Тогда теоретическое число значений
с. в. X, попавших в интервал Д*, можно рассчитать по формуле п - р^
Таким образом, имеем статистический ряд распределения с. в. X (8.11)
и теоретический ряд распределения:
(8.12)
Если эмпирические частоты (щ) сильно отличаются от теоретиче¬
ских (npi = гг'), то проверяемую гипотезу Но следует отвергнуть; в
противном случае — нринять.
Каким критерием, характеризующим степень расхождения между
эмпирическими и теоретическими частотами, следует воспользовать¬
ся? В качестве меры расхождения между щ и npi для г = 1,2,... ,т
Ai
А.2
n'i = пр]
Tl'2 = Пр2
п'т = пРт
XI
х2
хт
т
щ
™2

250 * Раздел второй. Основы математической статистики
К. Пирсон (1857-1936; англ. математик, статик, биолог, философ) пред¬
ложил величину («критерий Пирсона»):
тп /	vo	чп	2
2	_ (пг ~ nPi) _ ni
^ ~	Tipi	—	L	npi n*	(8.13)
i— 1	i— 1
Согласно теореме Пирсона, при п —у ос статистика (8.13) имеет х2-
распределение с к = тп — г — 1 степенями свободы, где гп — число групп
(интервалов) выборки, г — число параметров предполагаемого распре¬
деления. В частности, если предполагаемое распределение нормально,
то оценивают два параметра (а и сг), поэтому число степеней свободы
к — та — 3.
Правило применения критерия х2 сводится к следующему:
1.	По формуле (8.13) вычисляют Х«абл — выборочное значение стати¬
стики критерия.
2.	Выбрав уровень значимости а критерия, по таблице х2-РаспРеДе_
ления находим критическую точку (квантиль) х2
3.	Если Хнабл ^ Ха,до то гипотеза #о не противоречит опытным дан¬
ным; если Хнабл > то гипотеза #о отвергается.
Необходимым условием применения критерия Пирсона является
наличие в каждом из интервалов не менее 5 наблюдений (т. е. щ > 5).
Если в отдельных интервалах их меньше, то число интервалов надо
уменьшить путем объединения (укрупнения) соседних интервалов.
Пример 8.8. Измерены 100 обработанных деталей; отклонения от за¬
данного размера приведены в таблице:
[Xi, Жг_|-1)
[-3,-2)
[-2,-1)
[-1,0)
[ОД)
[1,2)
[2-3)
[3,4)
[4,5)
Щ
3
10
15
24
25
13
7
3
Проверить при уровне значимости а = 0,01 гипотезу Hq о том. что
отклонения от проектного размера подчиняются нормальному закону
распределения.
О	Число наблюдений в крайних интервалах меньше 5, поэтому объ¬
единим их с соседними. Получим следующий ряд распределения (п =
= 100):
[жг', Жг-f 1)
[-3,-1)
[-1,0)
[0,1)
[1,2)
[2,3)
[3,5)
Щ
13
15
24
25
13
10
Случайную величину — отклонение — обозначим через X. Для вычи¬
сления вероятностей pi необходимо вычислить параметры, определя¬
ющие нормальный закон распределения (а и а). Их оценки вычислим

Глава 8. Элементы теории оценок и проверки гипотез ■ 251
по выборке: х =	•	(—2	•	13	+ (—0,5) • 15 + ... + 4 • 10) = 0,885 ~ 0,9,
Д, = щ (4 • 13 + 0,25 • 15 +..,+16 • 10) - (0,885)2 « 2,809, а « 1,676 «1,7.
Находим pi (г = 1,6). Так как с. в. X ~ N(a% а) определена на
(—сю, оо), то крайние интервалы в ряде распределения заменяем, соот¬
ветственно, на (—оо,—1) и (3, +оо). Тогда р\ = Р{—оо < X < — 1} =
= Фо ^—-у	—	Фо(—оо) = i — Фо(1,12) = 0,1314. Аналогично по¬
лучаем: р2 — 0,1667, рз = 0,2258, р4 = 0,2183, р$ ■= 0Д503, ре =
= р{ 3 ^ X < оо\ = ф0(оо) - Фо	=	0,5 - Фо(1,24) = 0,1075.
Полученные результаты приведем в следующей таблице:
[ж»,ж*+1)
(-00,-1)
[-1,0)
[0,1)
[1,2)
[2,3)
[3, оо)
Щ
13
15
24
25
13
10
п' - npi
13,14
16.67
22,58
21,83
15,03
10,75
Вычисляем х‘набл:
6	2
2 _	_
Хнабл — 2L, npi П ~
г~\
= (щг+ W+ -•+ 5ш) - 100= ШМЪ - т'
т-е- xLwi ~1»045-
Находим число степеней свободы; по выборке рассчитаны два па¬
раметра, значит, г — 2. Количество интервалов 6, т. е. т — 6. Следо¬
вательно, А: = 6 — 2 — L = 3. Зная, что а = 0,01 и к = 3, по таблице
Х2-распределения находим Ха,к =	Итак>	Хнабл	<	Xa,fc> следова¬
тельно, нет оснований отвергнуть проверяемую гипотезу.	•
Критерий Колмогорова
Критерий Колмогорова для простой гипотезы является наиболее
простым критерием проверки гипотезы о виде закона распределения.
Он связывает эмпирическую функцию распределения F* (х) с функци¬
ей распределения F(x) непрерывной случайной величины X.

252 а Раздел второй. Основы математической статистики
Пусть Ж], жо,	хп — конкретная выборка из распределения с не¬
известной непрерывной функцией распределения F{x) и F*(x) — эмпи¬
рическая функция распределения. Выдвигается простая гипотеза Нц:
F(x) = Fo(rc) (альтернативная Н\: F(x) ф ^о(ж), ж Е М).
Сущность критерия Колмогорова состоит в том, что вводят в рас¬
смотрение функцию
Dn = max \F*(x) — Fo(a:)|,	(8.14)
—оо<ж<оо
называемой статистикой Колмогорова, представляющей собой мак¬
симальное отклонение эмпирической функции распределения F*(x) от
гипотетической (т. е. соответствующей теоретической) функции распре¬
деления Fq(x).
КолмогО!Х)в доказал, что при п —> оо закон распределения слу¬
чайной величины у/п • Dn независимо от вида распределения с. в. X
стремится к закону распределения Колмогорова:
Р{у/п• Dn < ж} -> К(х),
где К (ж) — функция распределения Колмогорова, для которой соста¬
влена таблица, ее можно использовать для расчетов уже при п ^ 20:
O'
0,1
0,05
0,02
0,01
0,001
хо
1,224
1.358
1,520
1,627
1,950
Найдем Do такое, что P(Dn > Do) = о.
Рассмотрим уравнение К(х) = 1— и. С помощью функции Колмого¬
рова найдем корень жо этого уравнения. Тогда по теореме Колмогорова,
Х(\
Р{у/п - Dn < х0} = 1 - а, Р{у/п ■ Dn > х*о} = а, откуда D0 = —.
у/п
Если Dn < Do, то гипотезу Но нет оснований отвергать; в против¬
ном случае — ее отвергают.
Пример 8.9. Монету бросали 4040 раз (Бюффон). Получили п\ = 2048
выпадений герба и п2 = 1992 выпадений решки. Проверить, используя
а)	критерий Колмогорова: б) критерий Пирсона, согласуются ли эти
данные с гипотезой Но о симметричности монеты (а = 0.05).
О	Случайная величина X принимает два значения: х\ — — 1 (решка)
и ж2 = 1 (герб). Гипотеза Но- Р{Х = —1} — Р{Х = 1} = i.
а)	По таблице распределения Колмогорова находим корень урав-
Жп
нения К(х) = 1 — а при а = 0,05. Следует Жо = 1,358. Тогда Do = -7= —
у/п
= 1-358- « 0,021.
v/4040

Глава 8. Элементы теории оценок и проверки гипотез ■ 253
Для нахождения по выборке Dn строим функции Fq(^) и F*(x) и
вычисляем величину Dn = max|Fr*(rc) — Fo(a;)|.
Xi
решка
Х\ — —1
0,5
герб
x2 = 1
0,5
Xi
решка
Х\ = —1
герб
Х‘2 = 1
Щ
1992
2048
Pi
~ 0,493
~ 0,507
0,	при х < —1,
F0{x) =	0,5, при — 1 < х ^ 1,
1,	при 1 < X.
0,	при х ^ —1,
F*(x) = < 0,493, при — 1 <	х ^ 1,
1,	при 1 < X.
Максимальное отклонение ^о(ж) от F*(x) равно U,007, т.е. Dn =
= 0,007. Поскольку Dn < Do, то нет оснований отвергать гипотезу
Но) опытные данные согласуются с гипотезой Но о симметричности
монеты.
б) Вычисляем статистику х2-
п?
Хиабл -Е npi п 1
1992
+
2048
г= 1
• 4040 b ■ 4040
4040 = 0,776.
По таблице х2_РаспРеДеления находим критическую точку Хак ~
= Хо,05;1 = 3,8- Так как Хнабл < Хо,05;1* то опытные данные согласуются
с гипотезой о симметричности монеты.	#
Упражнения
1	- Распределение признака X (случайной величины X) в выборке за¬
дано следующей таблице*,.
Хг—1 - Xi
0-0,1
0,1 - 0,2
0,2 - 0,3
0,3 - 0,4
0,4 - 0.5
Щ
105
95
100
100
102
Xi 1 Xi
0,5 - 0,6
0,6 - 0,7
0,7 -0,8
0,8 - 0,9
0,9 - 1,0
т
98
104
96
105
95
При уровне значимости а = 0,01 проверить гипотезу Но, состоящую
в том, что с. в. X имеет равномерное распределение на отрезке [0,1]
(вероятности определяются формулами pi = hi (г = 1,2
где hi — длина i-ro отрезка [xi-i\xi\ h% 1^).

254 11 Раздел второй. Основы математической статистики
2.	Результаты наблюдений над с. в. X (рост мужчины) представлены
в виде статистического ряда:
X(рост)
[150 - 155)
[155 - 160)
[160 - 165)
[165 - 170)
щ(частота)
6
22
36
46
X(рост)
[170 - 175)
[175 - 180)
[180 - 185)
[185 - 190)
щ(частота)
56
24
8
2
Проверить при уровне значимости а — 0,05 гипотезу Но о том, что
с. в. X подчиняется нормальному закону распределения, используя
критерий согласия Пирсона.
3.	По данным упражнения 2 проверить гипотезу о нормальном рас¬
пределении с. в. X, используя критерий Колмогорова.

Ответы к упражнениям
Раздел первый
Глава 1
1.3
1.	1) В = В • п = В(А + А) = А • В + А - В; 2) (А + С) ■ (В + С) =
= АВ+АС+ВС+С = АВ+АС+С = А-В+С; 3) Пусть w € А + В =*■ w $
<£ А + В w (£ A,w В w £ A, w^zB^w^iA'B, т. е. А + В С А • В.
Аналогично убеждаемся, что АВСА + В=>А + В = А- В.
2.	a) ABC] б) АБС; в) А+Б+С; г) A B C: д) ABC; е) АВ-С+АВС+А-ВС;
ж) АБС 5= Л + В + С.
3- А\А2(А^ + А4 + ^5)^6-
1.7
1.	Из 90 двузначных чисел 9 имеют одинаковые цифры, т. е. п = 90, т =
81
= 90 — 9 = 81'. Следовательно, Р = qq = ^-9-
2.	р = 0,01, так как т = 1, п = 10 • 10 = 100.
1.8
1.	5-4-3 = 60; 5-4-3 + 5- 4- 3- 2 + 5- 4- 3- 2-1 = 300.
2.	12 + 15 + 7 = 34.
3.	10-9-8-7 = 5040.
4.	(10 • 9 • 8)2 = 7202 = 518400 или А?0 • Af0.
5.	94 = 6561.
6.	А\2 = 12 -11 • 10 • 9 = 11880.
7.	^i0 = 720.
8.	А\ • А\ ■ А\ = 120.
9.	Рг> • Рг = 720; Р7 - 720 = 4320.

256 ■ Ответы к упражнениям
10. Р4 = 4! = 24 (один сел где угодно).
11.3-2-8! = 241920 (см. рис. 79).
Рис. 79
12.	112 = 7 • 16; 2520 = С? • С?6.
13.	а) С| = 56; б) С?2 • С| = 6160.
14.	Cf5 - С? • или gjgj^i = 135135.
15.	AJ, = 27 = 128 или 2 • 2 • 2 • 2 • 2 • 2 • 2.
16.	63 = 216. Это 234. 666. 165, ....
17.	А\ = 46 = 4096.
18. С46 = С§ = С| =	= 21-4-84.
19.	C'l = С?0 = CjQ = 45.
20.	= С| = Cl = 15, 15 • Р4 = 360.
21-о)4! = 24:6)2Г^г!г1ГТТ = “0
“■ 2г1Ь = 1260-
23- 9ГЖЗ! = 4004110 =	' Ст • С|.
1.9
1.	п = А1 = 84 = 4096 a) m = 8 • 7 • 6 • 5 = 1680, рх = ss 0.41: б) т = 8,
р2 =	« 0,00195; в) т = 1, р-А = и 0,00024.
/^2
2.	п = С|6, т = С%-С$,р=	- 5 4 « 0.000064.
С36
3.	п = 7!, т = 6! • 2, р =	= | ~ °’29'
4.	п = 5! = 120, т = 2- 2-1-1;р =	=	0,033	
5.	а) р = ^ « 0,573; б) р =	»	0,36.
^60 ^60
6.	а) р =	^	«	0,08;	б)	р	=	^	и	0,408.
7-	Группа из 5 команд может быть выбрана	Cf0	способами	(вторая	группа
образуется автоматически), т. е. п = (7f0.	В	первую	группу	попадет ли¬
бо один лидер, либо два. Стало быть, т =	•	С?	+	С%	•	Поэтому
С1	/^4 I /^2	/^3
о * о7 Н- • о7

Ответы к упражнениям ■ 257
8.
С1	г^\	г‘1
^ _	4	°32	+ °4 * °32
С326
г2
'-'36
0,21. Воспользовались свойством: Р(Л + В) =
= Р(А) + Р(В), АВ = 0. Здесь А = {одна дама}, В = {две дамы}.
1.10
1.	Сторона треугольника равна Ry/З. Значит, р =
5д
So
Ж2лД = 3^3
4тгД2 4тг
2.
3.
« 0,41.
р=^=0,32.
Обозначим длину I отрезка через х, II — через у, тогда III отрезок имеет
длину I — х — у. Q = {(х,у) : 0 < х + у < /}, т. е. 0 < х + у < I — все
возможные комбинации длин частей отрезка. Чтобы из них можно было
составить треугольник, необходимо выполнение условий: х + у>1 — х — у,
х + 1-х-у> у, у + 1 — х - у > х, т. е. х < у < х + у > Эти
неравенства определяют область, заштрихованную на рис. 80.
I.	L . L
Имеем: Р = 22 2 = 0,25.
\1-1
1,
1.
2.
15
Да. Q = {1,2,3,4,5,6}, А = {2,4,6}, В = {4,5,6}. Ясно, что Р(А) = | = ±,
Р(В) = ±, а Р(АВ) = | = I т.е. Р(Л£) = ± ф Р(А) • Р(В) = ± • \ = ±
a) Aj — первая буква Т, А2 — вторая буква 11,	А;, — пятая буква
И. Вероятность события А — получится слово ТИСКИ, равна Р(А) =
= Р(А\А2А.зА^А^) =
3
2
9
211
876
1
10 9 8 7 6 - 2520 " °-°004; б) °’ так как ВТОр°й
буквы К в слове СТАТИСТИКА нет; в) р =	'	g	‘	^	=	~	0,008;
г )р =
1
3.
3 2 2 2 1 1	1
10 9876543 2
qi = 0,2 — вероятность отказа г-го элемента, р* = 0,8 — вероятность его
исправной работы. Цепь последовательно соединенных элементов 1 2-3

258 ■ Ответы к упражнениям
будет работать, если исправны все три элемента: р\23 = Pi *Р2 *Рз = 0,83 =
= 0,512. Цепь параллельно соединенных элементов 5 6 откажет, только в
случае отказа обоих элементов: q^ = q^q^ = 0,22 = 0,04, а вероятность ее
исправной работы равна р56 = 1 - <?5б = 0,96; Р456 = Ра ■ Р56 — 0,8 • 0,96 =
= 0,768; р12345б = 1 - <?123 * <7456 = 1 - (1 - 0,5 1 2) * (1 - 0,768) « 0,887;
Ротк = 1 — Р123456 * Р7 ~ 1 — 0,887 • 0,8 « 0,291.
1-16
1	- А — первый шар белый, В — второй шар черный. Тогда АВ + АВ — оба
шара разных цветов. Р(АВ + АВ) = й'о + Н’§=:Ш = 7^и 0,39.
У о У о (2* 1о
2.	а) А\ — попадание первого орудия, А2 — второго, Л3 — третьего. Значит,
ABC + ABC + ABC = D — попадание только одного из них. P(D) =
= 0,7*0,3 0,3*3 = 0,189; б) S = А\А2Л^ — три промаха. P(S) = 0,3*0,3*0,3 =
= 0,027. Значит, P(S) = Р{Аг + А2 + Аг) = 1 - Р(§) = 1 - 0,027 = 0,973.
3.	Вероятность выхода из строя всех п приборов равна
(1 - 0,7) • (1 - 0,7) *... * (1 - 0,7) = 0,3П.
Следовательно, вероятность безотказной работы равна 1 — 0,3™. По усло¬
вию 1 — 0,3П ^ 0,95. Отсюда 0,3П < 0,05, п In 0,3 ^ In 0,05,
п ^ In 0.05 : In 0,3 « 2,488,
т. е. n ^ 3.
1.18
1	- А — вышла из строя одна микросхема, Но — отказали обе, Н± — отка¬
зала первая микросхема, Н2 — отказала вторая, Н% — обе не отказали.
Тогда Р(Но) = 0,07 • 0,1 = 0,007, Р(НХ) = 0,063, Р(Н2) = 0,093, Р(Щ) =
з
= 0,837 (Контроль: £ Р(Щ) = 1). Р(А\Н0) = 0, Р(А\Нг) = 1, Р{А\Н2) -
г=0
= 1, Р{А\Н3) = 0. Значит, Р(А) = 0,156 и Р(Нг\А) « 0,404.
on о
2.	А\ — студент П сдаст экзамен, если зайдет первым, Р(А\) ~ 4Q = 4*
А2 — студент П сдаст экзамен, если зайдет вторым, Р{А2) = ? Введем
гипотезы: Н\ — первый студент вытащил билет, который знает студент
П, Н2 — первый студент вытащил билет, который не знает студент П.
Р<Я1> = 1 = 1’ р<Яг> = я = И!+ г = •)'Далее: р(Лг|Я1> = 1'
Р(Л2|Н2) =	• Значит, Р(А2) = §§§ + |§5 = §Все равно.

Ответь; к упражнениям ■ 259
3.	Пусть А -т— изделие пройдет контроль, Н\ — взятое изделие стандарт¬
но, Н-2 — не стандартно. Р(Н\) — 0,9; Р{Н2) — 0,1; Р{А\Н\) — 0,96;
Р(А\Н2) = 0,06. Следовательно, а) Р{А) = 0,9 • 0,96 + 0.1 • 0.06 = 0.87:
0 9 • 0 96
6) PiHM) =	~	0.993.
1.20
1.	а) Р10(4) = С*0 ■ (|)4 • (|)6 « 0,21; б) Р10(0) = (1)*° и 0.00098; в) р =
= Рю(1) + Рю(2) + ... + Рю(Ю) = 1 - Рю(0) « 0,999.
2.	р = , = 1. Р4(2) = С? • (I)2 • (I)2 = §; Р6(3) = С? ■ (А)’ • (I)3 = А.
Следует, что /4(2) > Рб(3), т.е. 2 из 4.
3.	а) Р3(3) = С| • 0,513 • 0,49° « 0,133, б) Р3( 1) = С3Х • 0,51 - 0,492 » 0,368.
4.	Спички брались 2 • 10 — 6 = 14 раз, из них 10 раз из коробки, которая
оказалась пустой. Имеем 10 «успехов» в 14 «испытаниях», т.е. Р14МО) =
= ^° • Ш10 • (I) ‘ « 0,061.
1.21
1.	р = 1 : 365	0,0027, п = 84, а « 0,23. По формуле Пуассона I’s4 (2) ~
« 0,232 2^7945 « 0,021. (е~0,23 « 0,7945).
2-	п = 200, р = 0,02. Значит, а = [пр] = 4. Искомая вероятность:
Р2оо(0) + Р2оо(1) =	« 0,09.
3-	Здесь п = 100, m = 60, р = q = Используем локальную теорему Муав-
ра Лапласа: ж = ^0 Ю0 0>^ _ 2. Тогда Рюо(60) = ^(2) = 0,2-0,054 «
F	000	-	0,5	•	0,5	V	'	5^v	;
и 0,0108.
4.	Psooi'm, 800) = 0,95. Используем интегральную теорему Муавра-Лапласа:
т — 388	0
х\ — —^—> x‘i = 29- Значит,
ФоЫ = 0.5;	0.5	-	Ф0	(Ш	14388)	=	0,
95.
Откуда Фо (^^14 т) = 0,45,	т	=	1,65.	Значит,	т	=	365.

260 ■ Ответы к упражнениям
Глава 2
2.2
1.	Возможные значения с. в. X есть (J, 1, 2, 3, 4. Их ве^ятности равны со¬
ответственно: pi = Р{Х = 0} = С% ■	р2	= Р{Х = 1} =
= С*' (2) (2) = Тб’Рз = Тб’Р4 = 16’Ръ = р^х =	= Тб'
2.	Пусть А\ — первый студент сдает экзамен, А2 — второй сдает экзамен.
С. в. X принимает три значения: 0,1,2. а) р\ = Р{Х = 0} = P(AiA2) —
= 0,4 • 0,1 = 0,04; р2 = Р{Х = 1} = Р(АгЛ2 + АгА2) = 0,6 - ОД + 0,4 • 0,9 =
= 0,42; рз = Р{Х = 2} = Р(АхА-г) = 0,6 • 0,9 = 0,54 £> = L); б) Pl =
= Р{Х = 0} = P(AiAlA2A2)= 0,4-0,4 0,1 0,1 = 0,0016; р2 = Р{Х = 1} =
= Р{А\А2Л2 + А\А\А2 + А] А2А\А2 + Л\А\А2А2) — ... = 0,1668;
Рз — Р{Х = 2} — Р(А\А2 *+- А\Л2А2 + А\А\А2 + А\Л2А\А2) =
= 0,6 - 0,9 + 0.6 • 0,1 • 0,9 + 0,4 • 0,6 • 0,9 + 0,4 • 0,1 • 0,6 ■ 0,9 = 0,8316
(Eft = 1).
г=1
2.3
1.	А\ — попадание I стрелка, А2 — И. Тогда Р{Х = жх} = Р{Х — 0} =
= Р(АхА2) = 0,08, Р{Х = х2) = Р{Х = 1} = Р(АХА2 + АхА2) = 0.44,
Р{Х = 2} = 0,48.
{0, х ^ 0;
0,08, 0<ж<1;
0,52,	1 < гс < 2;
1,	2 < х.
2.	F(x) удовлетворяет свойствам функции распределения;
Р{0 < ж < 1} = F(l) - F(0) = 1 - e_1 = 1 - \ и 0.632.
3.	Р{Х 6 (0,1)} = F( 1) - F(0) = Значит, P4(3) = Cf • (±)3 • (i)1 = 0,25.
2.4
1.	F(2) = 1. Значит,
2(x + 1)
{2(i*-' i *-/
g" ~ > ^e(-l;2],
0,	x	(—1; 2].

Ответы к упражнениям ■ 261
2- 5д = ^	=	1.	Значит,	ОМ	=	h	=	4.	Уравнение	MJV:	-—j = у—т. е.
Z Z	U	4	i	—	Q
у = —8ж + 4. Стало быть,
/(^) = <
если х ^ 0, то
—8х + 4, j; Е ^0;	,
о,	X $ (о; ±] ;
F(x)=/
Odt = 0;
если 0 < х < то
и	X
F(x) = J Odt + J (—81 + 4) dt = —4a;2 + 4ж;
если a; > то
F(s) = / 0efo + J(-8t + 4)dt + J 0dt = 1,
таким образом
I,
x > 2’
p{x e (\a)} = J (-8s+ 4 )dx + j 0 dx = i
ЭО
3. а) нет, так как f(x) < 0 при х € (—оо,0); б) да, /(ж) ^ О, J f(x)dx = 1:
—ОО
оо	0	2	оо
в) J f(x) dx = J 0dx + J ax2 dx + J Odx = 1, т. e. = 1, a = Если
—оо	—oo	0	2
O
a = то /(.т) — плотность распределения; a ф - — нет.
о	о

262 ■ Ответы к упражнениям
2.5
1.	Учитывать результаты примера примера 1.31 (п. 1.20) в первом случае
имеем: MX = 0 • 0,001 + 1 ■ 0,027 + 2 • 0,243 + 3 • 0,729 = 2,7; во втором
случае: MX = 0 • 0,006 + 1 • 0,092 + 2 • 0,398 + 3 • 0,504 = 2.4.
ОС	0	7Г	ОО	7Г
2.	MX = | jж/(ж)сЬ:| = J x-Qdx+J i.Tsinxdx+Jx-Qdx = ^ Jxsinxdx =
—	OO	—OO	0	7Г	0
7Г
_ T
3.	Согласно свойствам м. о. и дисперсии, имеем MZ = М(5Х — ЗУ 4 2) =
= МЪХ - МЗУ + М2 = ЪМХ - 3MY + 2 = 5 • 2 - 3 • (-3) + 2 = 21 и
DZ = D{5Х - ЗУ + 2) = D5X + £>(-ЗУ) + D2 = 2bDX + 9DY + 0 =
= 25-2 + 9-9 = 131.
ОО	0	7Г	ОО
4.
DX-[f*n*)*-WXr] =	+	+
—	ОО	—ОС	0	7Г
7Г
—	^	j	ж2	sinxdx —	x2cos.t|^+2 ^ж8тж|^ + со8.т|^)) —
7Г2 	 7Г2 о 7Г2	7Г2
4	= ^--2-2L_ = ^--2« 0,467; а* = гЩб7 « 0,68.
5.	Если х —> Л, то F(.t) —» 0, lim 0,25,-г2 = 0, т. е. 0,25А2 = 0. А = 0;
х—>А+0
lim 0,25ж2 = 1, т. е. 0,25В2 = 1, В = 2. Поэтому
х^В
{0,	при х	^ 0,
0,25,т2,	при 0	< х ^ 2,
1,	при 2	< ж.
Тогда
Поэтому
2
MX = jх • 0,5xdx = у *= 4/3, DX = Jх2 ■ 0,5xdx - (|)2 = |.
о	о
Значит, ах =
о
= хт ■ па = а;
6- м( lf>.) =	=	lya	=	1(а	+	а+	+а)	=	1	•
г=1	г=1	2=1	п
^£*0 = = = ^ = '
4	г=1	х	чг=1	'	г=1	г=1

Ответы к упражнениям ■ 263
2.7
1. Р{-3 < X < 5} = Фо (^) - Фо	= фо(1) + Фо(3) = 0,8413;
Р{Х < 4} = Р{-оо < X < 4} = Ф0 (^) - Фо (z2°2~) = Фо(0,5) +
+ Ф0(оо) = 0,19146 -I- 0,5 = 0,69146; Р{\Х - 3| < 6} = Р{\Х - 3| < 3 • 2} =
= 2Ф0(3) = 0,9973.
2.	По условию а = 0, а = 1 a) Р{Х € (1,3)} = Ф0 (^у^) “ фо (^у^) =
= Ф0(3) - Ф0(1) = 0,49865 - 0,34134 = 0,1573; б) 2Ф0 (у) = 0,8926, отсюда
Фо(0 = 0,4463. По таблицам находим, I = 1,62, и интервал имеет вид
(-1,62; 1,62); в) М0Х = 0; Ме = 0.
3.	Коэффициент асимметрии А нормального распределения равен 0 (.4 = 0),
так как кривая Гаусса симметрична относительно прямой х = а, проходя¬
щей через центр распределения а. Найдем (аналитически) коэффициент
эксцесса, т. е. Е = ^ — 3. Сначала найдем
а4
X2
и = X3
du — Зх2 dx
2<r2 dx =
х1
dv — хе
2п-2 dx
v = —а2е 2cr2
[	4	1
= / х 	7=
J ау/Ъп
—оо
п /	оо	г	хА	\
— —-—\—xza2e 2сг2	+3сг2 I х2е 2a2 dx) =
<7л/27Г \	-сю	J	)
—оо
2 00 2
= _J_(0 + 3c,2(_xe-^„2|“^,T2 yYS^)) =
=^И0+^/л^Ш))
= 	^=(Зст2(73л/2^/7г) = Зет4.
<7\/27Г
Стало быть: Е = ^	3 = 3 — 3 = 0.

264 ■ Ответы к упражнениям
Глава 3
3.3
1.
СХ.	ОО	LXJ	CXJ
1) J j	f(x,y)dxdy = 1.	Поэтому	A j J еГх~у dxdy	=	A J е~х dxx
—оо —оо	0	0	0
оо	оо	оо
х J е~у dy	=	A j е~х	е_2/|0	dx	=	A J е~х dx	=	А	= 1.	2) F(x,y)	=
0	0	о
Ж У	х	у	х
—	j	j	e~u	v	dudv = J e~u du j e~v dv =	j еГи {^~e	v	dii = (1 — e~y)x
0	0	0 0	0
X
x	j	e~u du =	(1 — p	x)(l — e" при ж	^ 0, у ^ 0,	т. e.
(1 - e~x)(l - f	^), при re	> 0,y ^ 0,
FA',v =
С
в противном случае.
х оо	гг	оо	X
3 )Fx(x)= У /(ж, у) ctej = J (^j e~ue~v dvjdu = J L ■ e~u d-u =
-OO —OO
0 0
— —e
= 1 — e x при x ^ 0, т. e.
Fx =
1 — e x, при x > 0,
0,
Аналогично
iV =
j1 - е-У,
\o,
при x < 0.
при 2/^0,
при у < 0.
(1 — е х)\ при х > 0,	J е х, при х ^ 0,
0,	при х < 0,	^0,	при х < 0.
е_2/, при у > 0,
0,	при у < 0.
4)	fx(x) = /i(sc) = F'(a:) =
Аналогично, /у (у) =
5)	Р{Х > 0,Г < 1} = I e~‘dx I <T»dy = -(е_| - 1) j е~‘dx =
= (1 - г) <0
о	о
ОО	1
= i- b 0,63.
о	е

Ответы к упражнениям ■ 265
4	4—х
l.l) j J f(x,y) dxdy = 1, т. е. J dx J С dy = 1, С J(4 — х) dx = 1, С =
0	0	о
Следовательно, /(.r, „) = J Г "ри 0, у > 0, * + » « 4;
[О, в противном случае.
оо	t—X
2) fx(x) = J J /(ж,!/)<*!/] = J ~dy = ij/ o = ж <E (0,4), /y(j/) =
—oo	0
=	0	<	у	< 4: 3) P{0 < X < 1,1 < Y < 3} = JJ f(x,y)dxdy =
Di
1	3
= i J dx J dy = 0,25 (см. рис. 81).
о	i
3-	1) С. в. X принимает значения 0, 1, 2. Очевидно, р\ = Р{Х = 0} = 0,6 0,6 =
= 0,36, р2 = 0,4 • 0,6 + 0,6 • 0,4 = 0.48. рз = 0,4 ■ 0,4 = 0,16. Стало быть:
X
0
1
2
V
0,36
0,48
0,16
з
(£> = 1). Аналогично находим, что
У
0
1
2
Р
0,16
0,48
0,36
есть ряд распределения с. в. Y. 2) Возможные значения системы (X, Y):
(0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2). Совместная таблица

266 ■ Ответы к упражнениям
распределения имеет вид:
X\Y
0
1
2
0
0,0576
0,1728
0,1296
1
0,0768
0,2304
0,1728
2
0.0256
0,0768
0,0576
так как рп — Р{Х = О, У = 0} = 0,4 • 0,4 • 0,6 ■ 0,6 = 0,16 • 0,36 = 0,0576,
Р12 = Р{Х = О, У = 1} = 0,36 • 0,48 = 0,1728, pi3 = Р{Х = О, У = 2} =
= 0,36 • 0,36 = 0,1296, р21 = Р{Х = 1 ,У = 0} = 0,48 • 0,16 = 0,0768,
Р22 = Р{Х = 1,у = 1} = 0,48 • 0,48 = 0,2304, р23 - Р{Х = 1,У = 2} =
= 0,48 • 0,36 = 0,1728, р31 = Р{Х = 2, У = 0} = 0,16 • 0,16 = 0,0256,
Р32 = Р{х = 2, У = 1} = 0,16 • 0,48 = 0,0768, р33 = Р{Х = 2,У = 2} =
= 0,16-0,36 = 0,0576. 3) По таблице распределения, пользуясь равенством
F(x, у) =	Piji	находим	значения	функции распределения F(x,y):
Xi<X у j <у
X\Y
У < o
о < у < 1
1 <у<2
У >2
x^0
0
0
0
0
0 < x ^ 1
0
0,0576
0,2304
0,3600
1 < x ^ 2
0
0,1344
0,5376
0,8400
x >2
0
0,1600
0,6400
1,0000
3.7
1.	Имеем
X
1
2
3
P
0,33
0,33
0,34
и
У
1
2
3
4
p
0,24
0,28
0,27
0,21
2.
Тогда тх = 1 - 0,33 4- 2 ■ 0.33 + 3 • 0,34 = 2,01, rny = 1 ■ 0,24 + 2 • 0,28 + 3 • 0,27 +
-I- 4 • 0,21 = 2,45. Так как
Р{Х = 1, Г = 1} = 0,07 ф Р{Х = 1} • P{Y = 1} = 0,33 - 0,24 = 0,0792,
то с. в. X и У — зависимы. Находим MXY: MXY = 1 * 0,07 + 2 • 0,04 +
+ 3-0,11 + 4-0,11+2-0,08 + 4-0,11+6-0,06 + 8-0,08 + 3-0,09 + 6-0,13 + 9-0,10 +
+ 12 - 0,02 = 4,71. Поэтому cov(X, Y) = 4,71 - 2,01 • 2,45 = -0,2145.
JJ f(x,y)dxdy = 1, поэтому
D
11	1	4	1
a j dx J (1 — xys) dy = a J dx ^у —	a	J	2dx	=	4a	=	1,	a	=
-l -i -l -l

Ответы к упражнениям ■ 267
1 1
^ = i/^ya-v)^ = ...=«,
-1 -1
1 1	1
MY = \JdxJy(l-xy3)dy = \Jdx (у
-1 -1	-1
1 1
DX = \ J(x-0)2dx J (1 ~xy3)dy = ••• = !,
= 0.
-1
значит, ox =
1
v/з'
CO oo
£>Г=[У^ J (y-my)2f(x,y)dxdy
—oo —oo
1
1 1
-1 -1
значит, Gy —
y/Z
1 1
0 0
Kxy = I J J xy( 1 - re?/3) dxdy -() = ...=
-1 -1
_ JL
Следовательно, rxv = —;—= — i.
. _L_	5
v/3	v/3
1 1
5-	с J dx J(x + y) dy = 1. Отсюда с— l. Находим плотность вероятности:
i
fi(x) = J{x +у) dy = х + ^,
о
1
/гЫ = J(x +у) dx= (у + ух) о= у +
о
Так как /i(a;) • f2(y) = (х + |) (у +	/	х	+	у	=	/(ж,	у),	то с. в. X и У
1	1 1
MX = J xdx J(x + у) dy = J х(х+ i j dx =
зависимы.
о	0
r—11 CO

268 ■ Ответы к упражнениям
MY = b
DX = J (х-	dx J(х + у) dy =
11
144
ИЛИ
DX
i	i
= J x2 dx j(x + y) dy -	=
11
144’
DY =
11
144'
3.9
1.	Найдем условное распределение X: p(x[\y\) = P{X = 0|У = —1} =
0,15
0,10
0,25
= рЫух) = Й5 = i-Значит’	=0 • § +1 • § = i = °-6-
РЫУ2) =	р(х2\У2)	= f • Значит, M(X\y2) = 0 ■ ^ + 1 • Щ =
= Щ « 0,63. рЫуз) = §, р(®2|Уз) = Щ- Значит, М(*|у3) = § ~ 0,43.
Кривая регрессии X на у имеет вид, изображенный на рис. 82.
2.	Как известно, для нормально распределенной с. в. (X, Y) ее составляющая
Y также распределена по нормальному закону (формула (3.37)):
(у — ту)2
/2 (у) = —^=е	2<т*
(У/ft у Z7T
Имеем
/ОФ) =
/(^у) =	1
/2(1/)	27Г	1	—	г2
e-5ab5)(x>-arW) : -J-е"? =
\/2тт

Ответы к упражнениям ■ 269
(х - ут)2
1	Р	2(W—
2\2
\/2тг • у/\ — г2
т. е. условная плотность распределения f(x\y) есть плотность нормального
распределения с параметрами m — у?' и а = \/1 — г2. Аналогично находим,
что
\	-хг)2
/(?/|т) =	— 	1	^ 2(\/1 - г2)2
3.11
1.	Так как
№ = <ь-
то
[О,	х<£[а,Ь),
ь
ф) = -pi- / eite dx = r^— • 4(eitb - eita) = —1—(e**6 - eito).
6-0У	b	—	a	it	’	(o	—	6)f	'
2.	Так как pk =	—. A; = 0,1,2,..to
°°	Ju _Л	°°	/„	At\k
<px(t) = eitkq-§r = e~a kT~= e~a ■ e“e<=e ~a(1 e<)-
A.-0	*
Тогда ip*(t) —	a)*(—ег*)-г, значит, MX = [—гс^?7(0)] = —г*(1*аг) =
= a, т. e. MX = a.
-l	о	oo	о
ip(t) = j elix • 0 dx + J eltx(—2x)dx + J eltx • 0 dx = —2 J eltxxdx =
-oo	1	о	-1
= -2 (^eH° -1 • ieH° ) = -2(0 - 4e‘i( + 4(1 - c~u)) =	-
\it |_i it tt.	|-i)	it f2 t
2	2e~u _ 2ite~il - 2 + 2e~u
t2 i2 t2
Глава 4
4.1
F
-3
-1
5
15
P
0,30
0,45
0,20
0,05

270 ■ Ответы к упражнениям
б)
в)
У
0
1
v/2
2
n/5
p
0,10
0,20
0,30
0,25
0,10
0,05
У
v/3
2
0
n/3
2
p
0,3
0,35
0,35
2.
У
0
1
P
0,5
0,5
Многоугольники распределения с. в. X и У изображены, соответственно,
на рис. 83, 84.
Рис. 84
MY = 0 • 0,5 +1 • 0.5 = 0,5; DY = [МУ2 - (МУ)2] = О2 • 0,5 +12 • 0.5 - (0,5)2 =
= 0,5 - 0,25 = 0,25. Отсюда <тУ = у/Ш =	=	0,5.
3.	С. в. X имеет равномерное распределение, значит.
fx(x) =
ft
х € [—2,2],
[-2,2].
Функция у = х + 1 строго возрастает в (—оо, оо), обратная функция
X = у - 1 = ф{у), у е [-1,3]. По формуле д(у) = f(4>(y)) ■ |^'(?/)| нахо¬
дим:
p(j/) = {i'1, у€Ь1,з],
\о. у *[-1,3],
3
т. е. У ~ R[-1,3]. Находим МУ: МУ = J | dy = 1 (или МУ = М(Х +1) =
-1
2	3
= J(ж +1) • ^= 1). DY = MY2 — (MY)2 = J ^dy-l2 = |, <хУ =

Ответы к упражнениям ■ 271
1	— —
4.	По условию fx(x) = —=.е 2 . а) Функция у = Зх имеет обратную
у2тх
х = ^/| = V'(y), ж' = ^'(у) = ~Ь= ■ ~^=- Поэтому
^3 3^/у2
1	1	_	I	?/Z
5(у) = -~= • 13/=-ое 2 V 9, г/ Ф о.
V2tt ЪуЩр-
б) у = |ж| =	^ q’ На (—оо,0) обратная функция для у = |ж| есть
х\ — —у = 'ФЛу)- На [0. ос) обратная функция есть х^ = у = ^2(у)* Стало
быть,
= Х]/Шу)МЫ1 = -А=е_т •1 + “te_T ‘1 = ~я=е~Т'
у/2тг	л/2тг	л/27г
уе [о, оо).
5.	а) По условию f(x) = е~ж, ж ^ 0 и F(rc) = 1 — е-х, ж ^ 0. G(y) =
= P{Y < у} = Р{2Х - 1 < у} = р{х <	= F	=
2/ -i~ 1
= 1 — е~ 2 , у ^ — 1 (так как условие х ^ 0 переходит для у = 2ж — 1
_ у + 1 / 1 \
в условие у ^ —1). Следовательно, #(у) = Gf(y) = —е 2	(—-) =
1 _у+ 1
=	2 при у >	1 (д(у) = 0 при у < -1). б) Если у < 0, то G(y) =
-	Р{У < 0} = 0 и д(у) = G'(y) = 0. При у > 0 имеем G(y) = P{Y < у} =
= Р{Х2 < у} = Р{|Х| < ^} = Pj-v/y < X <	= Р{0 < X < ,/у} =
= Р{Х < v/y}-P{X < 0} = F(,/y)-F(0) = (l-e-v^)-(l-e0) = 1 -е"^,
у > 0. Тогда д(у) = G'(y) = (1 - е ^)'у =	•	е	т.	е.
5(у) = <
2^Г У>0’
0,	у < 0.
Контроль: J g{y)dy = 1, J 0-dx + J ^dy = — J e ^d(—y/y)~
-oo	—oo	0	0
- /“I00
-	— <5 V® = —(0 — 1) = 1. (Иначе: при x € (0, oo) имеем x — */y = ф(у).
lo i
Поэтому g(y) = f(y/y) ■ {y/y)'y = e yft ■ у > 0.)

272 ■ Ответы к упражнениям
4.2
1
Находим Fx+y{z). F(z) = Р{Х + Y < z} = JJ (х + у) dxdy. F(z) = 0 при
Dz
(;X+y<Z)
z ^ 0; при 0 < г ^ 1 (на рис. 85 область Dz заштрихована вертикальными
линиями) имеем
г z—x
FM = /dx /(* + „) *(„ -	<Ц£>-)	*	-
о	о
при 1 < г ^ 2 (область Dz заштрихована горизонтальными линиями)
имеем
2-1 1
1	Z—X
F(z) = J	dx J (х +	у) dy + J	dx	J (х + у) dy =
о	о	г-1	о
= J (а;	+ ^) dx +	J	(xz — х2 +	^	dx =
_ (х1 . х \ \z 1 , (zx2 _	.	(ж	z)3\
^ 2 2у 1о \ 2	3	6	)
—	i(—23 + Ъг2 — 1);
г—1 о
1	1
при г > 2 F(z) = J dx J (х + y)dy — 1. Итак,
о	о
ГО,
-z3
3 ‘
1
3
U,
±(-z3+3z2-1), 1 < z ^ 2,
z >2.
Отсюда
О,	г^О, или 2 > 2,
/(г) = F'(z) = {z2,	0 < г < 1,

Ответы к упражнениям ■ 273
Рис. 85
F(z) = Р {-^г < г} = J (х + у) dxdy. F[z) = 0 при г ^ 0; при 0 < г ^ 1
имеем
dz
(£<*)
va:	'
1	zx
m = f*J
(х + у) dy = з + -g-.
О	о
При 1 < г < оо имеем
Z	XZ
1 ]
F(z) = J dx J(х + y)dy + J dx J (x +y)dy =
о о
Следовательно,
F\z) = f(z) = <
I о
2^J- + J- + i —i	L = i_J	L
Зг2 6*	2г2	2г	Зг	6z2
f 0.	г < U,
1 , 1
± + |.	0<г<1,
—г Н	г, 1 < Z < оо.
3z2	3zJ
ОС	1	ОО
Контроль: J f(z)dz = J	dz+J	dz = I (см. рис. 8(з).
Глава 5
5.5
1.	а) С. в. X — число выпавших гербов. MX = 500 • ~ = 250. Неравенство
200 < X < 300 можно переписать в виде —50 < X—250 < 50 или \Х—250) <

274 ■ Ответы к упражнениям
< 50. А так как DX = [npq] = 500 • ± ^ = 125, то Р{200 < X < 300} =
= Р{\Х - 250| < 50} ^ 1 - Щ- = 1 - Щ = 1 - 0.05 = 0,95; б) с. в.
|_	£ J	5Сг
X — сумма очков, Х{ — число очков на г-ой кости (i = 1,2,... ,6). X =
—	Х\ 4- Х2 + ... + Ххо- MXi — i(l + 2 + 3 + 4 + 5 + 6) = 3,5, DXi =
ю
Значит, MX = J2MXi =: 35’ DX = Ip Поэтому P{\X - 35| < 8} ^
125
175
6-64
П
7=1
= 1 - 0,4557 = 0,5443.
(\	I	^
2.	P< ^ ^2 xi~ b	МХЛ < e > ^ 1 —при e = 0.1 и С = 5 получаем:
^ I г=1	г=1	'	*
1 -
п - (0.1)2
> 0.9. Отсюда п > 5000.
Р ~Р <	^	1	~	"^2*	Из	условия	следует,	что	п	-	500,	£	=	0,1,
1
Р = Я - Получаем Р {|	-	0,5
< 0
0,5 • 0,5
= 0,95.
500 -(0,1)2
4.	Пусть Xi — число набранных очков при г-м выстреле, а Зюо — суммарное
число очков при 100 выстрелах. Тогда МХг = 10 0,5+9-0,3+8 0,1+7 0,1 =
= 9,2,
/и» \
М\'^2ХЧ =9,2-100 = 920;
'г=1	'
DXi = ЮО • 0,5 + 81 • 0,3 + 64 • 0,1 + 49 • 0,1 - (9.2)2 = 0,96,
лоо s
D(YlXi) =0,96-100 = 96.
\=1 '
По формуле (5.14) Р{940 ^ Sioo < оо} rs Ф(оо) — Ф	^	=
= 1 - Ф(2.04) = 1 - 0,9793 = 0,0207.
5. Пусть X — число прижившихся из п посаженных саженцев. По усло¬
вию р = 0.7, q = 0,3. Тогда МХ = 0,7n, DX = 0,3 • 0,7п = 0,21п.
0,21п	0,21п
Р{|Х — 0,7п| ^ 40} ^ 1 —^бб(Г" Имеем 1 —
1600
Глава 6
6.3
1. а) Х\ = -¥=; б) xi = 2sint.
\/2

Ответы к упражнениям ■ 275
2.	a) rnx(t)	= 3sint; б)	Dx{t)	=	^	•	sin2£, exit) =	•	|sint|;	в)	Kx(ti:t2) =
J	v3
1 - .	- , ч	/. ч	fl>	sinfi	•	sint2	>	U,	,	,	^
= Гт(1	.Smt2; r)	rx(tl;b)	=	^	<	0>	(	*	*»,	n	6 Z.
6.6
1.	а) нестационарный; б) стационарный: в),г) нестационарные с. п.
2.	MZ(t) = 5; /fz(t1;t2) = tit2 + 2e-tl-fa.
3.	a) my(t) = 3cost; Ky(ti;t2) = ^costj • cosf2: 6) my(t) — 3(1 — cost);
Ky{ti;t2) = |(1 -costi)(l - cost2).
4.	c. n. X(t) не является стационарным процессом, rnz(t) = 2(cost 4- 2sint);
Kz(t) = (cost! 4-2sinti)(cost2 + 2sint2).
6.9
1.	X(f) — нестационарный с. п.
3.	#хМ = 4е-|г|.
4.	£>x=4.
6.12
1.
Граф, соответствующий матрице Р изображен на рис. 87. Обозначим че¬
рез А событие, состоящие в переходе процесса после первого хода в со¬
стояние 5i; событие Н\ — устройство выбирает состояние si; событие
1	3
Н2 — устройство выбирает состояние s2. Тогда Р(Н\) =	Р(Н2) = ^
2	1
и Р(А\Н\) = j, Р(.А|#2) = 2* По формуле полной вероятности находим
ПА) = £
1 2
+ 3.1 =
3	4	2
13
24'
3
Si
«2
i

276 ■ Ответы к упражнениям
Раздел второй
Глава 7
7.5
1.	п = зо,к = 1е,р* = A,P| = i2 Ffo(x)= ,
0,	при	х ^ 1,
при	1 < х ^	3,
f,	при	3 < х ^	6,
5
1,	при	б < х.
2.	хв = 1,983 « 2; DB = 1,949 « 1,95. Частость р? такова:	^ « 0,133,
р* =	~	0,283, р% = 0,267. р\ = 0.167, р% = 0,100, р% = 0,033, р? = 0,017
7
= 1). Находим вероятности р^ по формуле Пуассона, считая а =
*=1
'	=	хв = 2. pi = — Qf - ~ 0,135, р2 = -	~	0,270,	р3	=	2?
0	135 • 23
w 0,270. р4 = -^|	w 0,180, р5 « 0,090, рб и 0,036. р7 « 0,017 (]Гр; =
г=1
= 1)* Как видим, с. в. X — число неправильных соединений имеет прак¬
тическое пуассоновское распределение.
3.	1.	Все возможные значения с. в. X — числа очков, выпавших на верхней
грани кости при одном подбрасывании ее.
2.	Это числа 1, 2, 3, 4, 5, 6.
3.	Это результат 60-ти подбрасываний игральной кости, описывается с. в.
ХъХъХ1...,ХыХт.
4.	Первая реализация выборки приведена в условии, вторая — может быть
такая: 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2. 2. 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4,
4,	4, 4, 4, 4, 4. 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6. 6. 6, 6, 6,
6,	6. Здесь х(1) = 1,..., Ж(зо) = 4, ж(31) = 4,..., х(б0) = 6.
5.	а) Вторая реализация и представляет собой вариационный ряд.
б)	Статистический ряд таков:
-2
.90	0,135	*22
^	^	п	юг;		 С Z ^ п 07П			,
J'i
1
2
3
4
5
6
Щ
7
10
8
12
10
13
Рг
7
60
10
60
8
60
12
60
10
60
13
60
= 60* 53Р* = !)•
г= 1	г=1

Ответы к упражнениям ■ 277
при	х	^	1,
при	1	<	х ^ 2,
при	2	<	х ^ 3,
6-	*5)(ж) = 4 iH’ при	3	<	х ^ 4>
при	4	<	х ^ 5,
при	5	<	х ^ 6,
при	6	<	х.
0,
7_
60’
17
60’
25
60’
37
60’
47
60’
1,
7.	Число интервалов га = 1 + log2 60 = 1 + 3,322 lg 60 « 6,9. Возьмем т = 6.
г
^max — xmin = 6 — 1 =5; длина интервала h = ^ = 0,833. Возьмем h — 1.
Интервальный статистический ряд имеет вид:
/ 1 , ? )
[0,5; 1,5)
[1,5; 2,5)
[2,5; 3,5)
[3,5; 4,5)
[4.5:5,5)
[5,5; 6,5)
щ
7
10
8
12
10
13
8.	Полигон частот и гистограмма частостей изображены, соответственно,
на рис. 88 и 89.
9.	а) хъ = - ’ 7-^ 2 -106+-~'' ± 6 '-3 » 3,783; б) £>в = ^(12 • 7 + 22 • 10 + ...
... +62 • 13) - (3,783)2 и 2,839; в) S2 » Щ • 2,839 » 2,888, S' = V& « 1.699;
г)	i? = 6 — 1 = 5; Mq = 6; М* = ±(*(30) +*(31)) = | • (4 + 4) = 4.

278 ■ Ответы к упражнениям
Глава 8
8.2
Qm ш р—а
1. Распределение Пуассона Р{Х = га} =	^— содержит один параметр а.
Для оценки его методом моментов запишем уравнение MX = жв, т. е. а =
п
—	жв. Оценка в параметра а = в есть в = хъ = ~	ж*,	т.	е.	0	=	ж.
г=1
2.	X — дискретная с. в. с законом распределения
Жг
0
1
Рг
1 ~р
Р
Так как
/ m d/у	m	/1_61’	если ж*	= О,
р(ж*, 0) = Р{Х = Ж*, в)	=	<
I	0,	если ж;	= 1,
то функция правдоподобия имеет	вид:	L =	Ь(ж,0) = 06	• (1 — 0)4. Тогда
InL = 6 • In# 4- 41п(1 — 0), уравнение правдоподобия
{в ~ Г^ё)1<,=^ = 0'
Отсюда находим, что в — р* = 0,6.
3-	а) Пусть в (или р или р*) — оценка неизвестной вероятности в (или р) по¬
явления некоторого события А (успех) в одном испытании. Согласно мето¬
ду моментов приравниваем теоретическое MX = р выборочному среднему
п
х — ^ ^ ж$, р X.
г~ 1

Ответы к упражнениям ■ 279
Искомая оценка есть 0 = х.
б)	Так как
p(Xi,6) = Р{Х = Хг,в} =
1	— 0, если Xi — О,
0. если Xi = 1,
то функция правдоподобия имеет вид:
L = L(x 1, ж2, • • •,	0) = Р{Х = Х{, 0} =
= p{*i, 0} • р{*2, 0} • • - • • Р{хп, 0} = ^ • (1 - вГ~ПА ■
П
где па — Xi — число успехов (наступления события А) в п независи-
мых испытаниях. InL = па * 1п0 + (п — пд)1п(1 — 0). Решаем уравнение
правдоподобия ^	^ = 0. Отсюда 0 = т.е. оценка 0 =
((1пВД<0).
4-	Требуется оценить две величины в\ и 02, т. е. а и Ь, методом моментов. Так
как для с. в. X ~ Д[а. Ь], то MX = ^ ^ -, DX = -—- ~ . Решаем систему
уравнений
{
MX = жв,
DX = £>в,
т. е.
(X Ъ —
—ГГ— = Хп
оЪ - а)2
12
= сгв>
Ь - 2жв — а, (2жв — 2а)2 = 12сг2, т.е. (жв — а)2 = 3<т2, жв — а = ±\/Зств,
жв = а+\/Зав и. значит, а = xB—\/3aB, b - хв+л/Ъав (вариант хв = а—л/3ав,
т. с. а = хв Ь \/Зсгв, Ь = хв — л/3сгв исключаем, так как а < Ь).
Таким образом, оценки величин а и b таковы: 0\ - а = хв — у/3ав,
02	= Ь = хв+ \/Зав.
Отметим, что, решая систему
MX =	=
г—1
г—1
получим те же оценки для а и Ъ. В этом случае
ь	ь
x2dx a2 +ab + Ъ2
МХ = [	MX2 = [	=
J Ь — a I	J	о	—	а
Система принимает вид:
а 4- Ь _
2	~х’’
о? -Ь о,Ъ -|- Ь2 	2

280 ■ Ответы к упражнениям
Отсюда следует, а — х — \/3 • ух2 — (х)2 = х — \/3 • \/^ — х — \^3ав,
Ь = :г + \/3<7в.
5.	В данном случае
{х — а)2
fx (х) = —t=e- 2а2	=	/у	(ж,	а,	а).
а у/2тт
Так как
(Хг-вг)2
= * ■
то функция правдоподобия имеет вид:
£ (хг - 0\)2
i=l
Ь = Цх,в) =	
Щ- (2тг)2
Тогда
71
\nL = -п1пв2 - |1п2тг -	-0х)2.
2	г=1
Система уравнений максимального правдоподобия имеет вид:
1^—5+i£“'-e‘,,=a
Отсюда
п	п
^jXi- пв1 =0, т. е. 0г = i ^ ж* = ж,
г=1	г=1
_	1	7}	_
0| = 72	—	^)2 (^2 — оценка неизвестного параметра сг = #2)- Можно
г=1	^	^
убедиться, что {61,62) — точка максимума функции \пЬ(х,в). Найдем
Д = АС — В2, где
д _ д2 In L
в =
861862

Ответы к упражнениям "281
с = д2Ы L
т2
. - = '±-1-У(хг-ех)2 =	=
(**) в2 е\ tC	в2	024	в2
2	• ~ ~
Следовательно, Д = — > О, А < 0, поэтому точка (#1,02) есть точка
максимума функции L(x, 0).
8.4
i	• гг	'У
1- Воспользуемся 'формулой (8.3): е = —^ и (8.6) Фо(£) = тт* Имеем е — 5,
v/n	^
ст = 15, 7 = 0,9, Фо(£) = 0.45. По таблице находим, что t = 1,65. Тогда
1,65-15 ^	1,652	• 152
———. Отсюда п = 		
фь	5^
менее 25 измерений.
5 = ————. Отсюда п — ———	= 24,5025, т. е. необходимо сделать не
6
~ =Х=	•	(153	•	4	+159 • 5 +165 • 6 +171 • 7 +177 • 5 +183 • 3) =
Стьюдента находим Ц — 2,05. Следовательно, е = ——у==-— « 3,47. Дове-
г~ 1
= 167,6; S = 9,28; при 7 = 0.95 и п — 1 = 29 по таблице распределения
2,05 • 9,28
л/30
рительный интервал таков: (164,13; 171,07).
По условию п — 400, п \ = 80, 7 = 0,95. Относительная частота события А
71 л	'V
есть р* = — = 0,2. Из соотношения Фо(£) = ^ — 0,475 находим /, пользу¬
ясь таблицей значений функции Лапласа: t = 1,96. Находим р\ и р? (фор-
мула (8.10)): pi = 0.2- 1,96-\	w	0,161,	^	=	0,2+	1,96-	/°’2 ' °’8
400	’	’	^	’	’	V	400
0,239. Итак, доверительный интервал есть (0,161; 0,239).
8.6
1.	п = 1000, X ~ [0,1], т. е. а = 0, b = 1, р\ = 0,1 = р2 = ■ ■. = рю; Хнабл =
= ^ + fS+--- + w + fS“1000 = М; ^= 21J-Так как
^набл Xo,oi;9’ то гипотеза не отвергается.
2.	Находим жв. и сгв: жв = (152,5-6 + 157.5-22 +162,5-36 + .. . + 187,5-2) • щ =
i^w ак q 1 го г 150	155	* г 155 + 160	r-v
= 168,45. Здесь 152,5 — 	^	> 157,5 = 	^	 и т*л* —
= 2^q (152,52 • 6 + 157,52 - 22 + ... + 187,52 • 2) - (168.45)2 = 53,348. Тогда
о	— 7,3. Так как = 2 — мало, то последние два интервала объединяем.
Составляем таблицу:
(—оо, 155)
[155,160)
[160,165)
[165,170)
[170,175)
[175,180)
[180, +ос)
6
22
36
46
56
24
10

282 • Ответы к упражнениям
Для расчета вероятностей pi попадания случайной величины X в г-й ин¬
тервал используем функцию Лапласа:
Р1 = Фо (155~з68,45) - Фо(-оо) = 0.5 - ф0(1,84) = 0,5 - 0,4671 = 0,0329,
= ФоС—1,15) -	Ф0(—1,84) = 0,0922,
= Ф0(—0,47) -	Ф0(—1,15) = 0,1941,
_	/170 — 168,45 \	/155 — 168,45 \
м=фч""тз—;-фч—7^з—) =
= Ф0(—0.21) - Ф0(—0.47) = 0.264.
ръ = 0,2327, ре = 0,127, р7 = 0,5 - Ф0(1,58) = 0,0571.
Тогда
Хнабл
7 2
Y' _!!l _
npi
п
г— 1
]
(—
\0,№
102 \ _
,0571J
По таблице xl,k = Хо,05;4 = 9>5- Так как Х„абл < Хо,о5;4’ то гипотеза Я0 не
отвергается.
X
[150.155)
[155,160)
[160,165)
[165,170)
пг
6
22
36
46
зк
Pi
200 = 0,03
0,11
0,18
0,23
X
[170,175)
[175,180)
[180,185)
[185,190)
Щ
56
24
8
2
Pi
0,28
0,12
0,04
0,01
J^pl = 1, хв = 168,45, ст = 7,3.
1=1
Составим таблицу:
X
150
155
160
165
170
К(х)
0
0,03
0,14
0,32
0,55
F0(x)
0,0057
0,0329
0,123
0,3192
0,5832
Б?
1
* Й
II
<
0,0057
0,0029
0,017
0,0008
0,0332

Ответы к упражнениям ■ 283
X
175
180
185
190
км
0,83
0,95
0,99
1
Fo(x)
0,8159
0,9429
0,9884
0,9984
A = \F*~F0\
0,0141
0,0071
0,0016
0,0016
Проверяемая гипотеза состоит в том, что с. в. X имеет нормальное рас-
~д~
пределение, т. е. Fq(x) = 0,5 + Фо(Ж,7у °):
F0(150) = 0,5 + Ф0 ( — у^6—5) = 0,5 - Ф0(2,53) = 0,0057;
/ 1 кг 	 1 яо 4^ \
F0(155) = 0,5 + Ф0 I	7 з j = 0,5 - Ф0(1,84) = 0.0329;
F0(160) = 0,5 + Ф0 (^°--^45) = 0.5 - Ф0(1.16) = 0,123;
F0(165) = 0,5 + Ф0 у^36—5) = 0-5 - Фо(0,47) = 0,3192;
F0(170) = 0,5 + Ф0 ^17°~ 1368,45^ = 0,5 + Фо(0,21) = 0,5832;
F0(175) = 0,5 + Ф0 (17^~-368,~5) = 0,5 + Ф0(0,90) = 0,8159;
F0(180) = 0.5 + Ф0 (18-°~7~з68~5) = °’5 + фо(1,58) = 0,9429;
F0(185) = 0,5 + Ф0 (185--^45) = 0,5 + Ф0(2,27) = 0,9884:
F0(190) = 0,5 + Ф0	g68,45^	=	0,5	+	Ф0(2,95)	=	0.9984.
Максимальное отклонение эмпирической функции распределения от тео¬
ретической есть Dn = |F*(170) — .Р0(170)| = 0,0332. Следовательно,
Dn • у/п = л/200 • 0,0332 ss 0,47. Критическое значение критерия Кол¬
могорова равно Ао,о5 =	— 1,36. Так как Dn\Jn < Аа, то гипотеза Hq
согласуется с опытными данными.

Приложения
Приложение 1. Значение функции </?(#) = —		е 2
\/27Г
Сотые доли х
X
0
1
2
3
4
5
6
7
8
9
0,0
0.,3989
3989
3989
3988
3986
3984
3982
3980
3977
3973
0,1
3970
3965
3961
3956
3951
3945
3939
3932
3925
3918
0,2
3910
3902
3894
3885
3876
3867
3857
3847
3836
3825
0,3
3814
3802
3790
3778
3765
3752
3739
3726
3712
3697
0,4
3683
3668
3653
3637
3721
3605
3588
3572
3555
3538
0,5
3521
3503
3485
3467
3448
3429
3411
3391
3372
3352
0,6
3332
3312
3292
3271
3251
3230
3209
3187
3166
3144
0,7
3123
3101
3079
3056
3034
3011
2989
2966
2943
2920
0,8
2897
2874
2850
2827
2803
2780
2756
2732
2709
2685
0,9
2661
2637
2613
2589
2565
2541
2516
2492
2468
2444
1,0
2420
2396
2371
2347
2323
2299
2275
2251
2227
2203
1,1
2179
2155
2131
2107
2083
2059
2036
2012
1989
1965
1,2
1942
1919
1895
1872
1849
1827
1804
1781
1759
1736
1,3
1714
1692
1669
1647
1626
1604
1582
1561
1540
1518
1,4
1497
1476
1456
1435
1415
1394
1374
1354
1334
1315
1.5
1295
1276
1257
1238
1219
1200
1181
1163
1145
1127
1.6
1109
1092
1074
1057
1040
1023
1006
0989
0973
0957
1,7
0941
0925
0909
0893
0878
0863
0848
0833
0818
0804
1,8
0790
0775
0761
0718
0734
0721
0707
0694
0681
0669
1,9
0656
0644
0632
0620
0608
0596
0584
0573
0562
0551-
Десятые доли х
X
0
1
2
3
4
5
6
7
8
9
2.
0540
0440
0355
0283
0224
0175
0136
0104
0079
0060
з,
0044
0033
0024
0017
0012
0009
0006
0004
0030
0020
1,
0001

Приложения ■ 285
Приложение 2. Значение функции Фо(.т) = L_ I е 2 dt
\/27г J
о
Сотые доли х
X
0
1
2
3
4
5
6
7
8
9
0,0
0,0000
0040
0080
0112
0160
0199
0239
0279
0319
0359
0,1
0398
0438
0478
0517
0557
0596
0636
0675
0714
0754
0,2
0793
0832
0871
0910
0948
0987
1026
1064
1103
1141
0,3
1179
1217
1255
1293
1331
1368
1406
1443
1480
1517
0,4
1554
1591
1628
1664
1700
1736
1772
1808
1844
1879
0,5
1915
1950
1985
2019
2054
2088
2123
2157
2190
2224
0,6
2258
2291
2324
2357
2389
2422
2454
2486
2518
2549
0,7
2580
2612
2642
2673
2704
2734
2764
2794
2823
2852
0,8
2881
2910
2939
2967
2996
3023
3051
3079
3106
3133
0,9
3159
3186
3212
3238
3264
3289
3315
3340
3365
3389
1,0
3413
3438
3461
3485
3508
3531
3553
3577
3599
3621
1,1
3643
3665
3686
3708
3729
3749
3770
3790
3810
3830
1,2
3849
3869
3888
3907
3925
3944
3962
3980
3997
4015
1,3
4032
1049
4066
4082
1099
4115
4131
4147
4162
4177
1,4
4192
4207
4222
4236
4251
4265
4279
4292
4306
4319
1.5
4332
4345
4357
4370
4382
4394
4406
1418
4430
4441
1.6
4452
4463
4474
4485
.4495
4505
4515
4525
4535
4545
1,7
4554
4564
4573
4582
1591
4599
4608
4616
4625
4633
1,8
4641
4649
4656
4664
1671
4678
4686
1693
4700
4706
1,9
4713
4719
4726
4732
4738
4744
4750
4756
4762
4767
Десятые доли х
X
0
1
2
3
4
5
6
7
8
9
2,
4773
4821
4861
4893
4918
4938
4953
4965
4974
4981
3,
4987
4990
4993
4995
4997
4998
4998
4999
4999
50001
1	Точное значение отличается меньше, чем на 5 ■ 10 5.

286 ■ Приложения
Приложение 3. Квантили к распределения \1
(к — число степеней свободы)
Уровень значимости а
к
0,01
0,025
0,05
0,95
0,975
0,99
1
6,6
5.0
3,8
0,0039
0,00098
0,00016
2
9,2
7,4
6,0
0,103
0,051
0,020
3
11.3
9,4
7,8
0,352
0,216
0,115
4
13,3
11,1
9,5
0,711
0,484
0,297
5
15,1
12,8
11,1
1,15
0,831
0,554
6
16,8
14,4
12,6
1,64
1.24
0,872
7
18,5
16,и
14,1
2,17
1,69
1,24
8
20,1
17,5
15.5
2.73
2,18
1,65
9
21,7
19,0
16,9
3,33
2,70
2,09
10
23,2
20,5
18,3
3,94
3,25
2,56
11
24,7
21,9
19,7
4,57
3,82
3,05
12
26.2
23,3
21,0
5,23
4,40
3,57
13
27,7
24,7
22,4
5,89
5,01
4,11
14
29,1
26,1
23,7
6,57
5,63
4.66
15
30,6
27,5
25,0
7,26
6,26
5,23
16
32,0
28,8
26,3
7.96
6.91
5,81
17
33,4
30,2
27,6
8,67
7,56
6,41
18
34,8
31.5
28,9
9,39
8,23
7,01
19
36,2
32.9
30,1
10,1
8,91
7,63
20
37,6
34,2
31,4
10,9
9,59
8,26
21
38,9
35,5
32,7
11,6
10,3
8,26
22
40,3
36,8
33,9
12,3
11,0
9,54
23
41,6
38,1
35,2
13,1
11,7
10.2
24
43,0
39,4
36,4
13,8
12.4
10,9
25
44,3
40,6
37,7
14.6
13,1
11,5
26
45,6
41,9
38.9
15.4
13,8
12,2
27
47,0
43,3
40,1
16,2
14,6
12,9
28
48,3
44.5
41,3
16,9
15,3
13,6
29
49.6
45,7
42,6
17,7
16,0
' 14,3
30
50,9
47,0
43,8
18,5
16,8
15,0

Приложения ■ 287
Приложение 4. Квантили ^-распределения Стьюдента
(к — число степеней свободы)
Уровень значимости а
(двусторонняя критическая область)
к
0,10
0,05
0,02
0,01
0,002
0,001
1
6,31
12,7
31,82
63,7
318,3
637,0
2
2.92
4,30
6,97
9,92
22,33
31.6
3
2,35
3,18
4,54
5,84
10,22
12,9
1
2,13
2.78
3,75
4,60
7,17
8,61
5
2,01
2,57
3.37
4,03
5,89
6,86
6
1,94
2,45
3,14
3,71
5,21
5,96
7
1,89
2,36
3,00
3.50
4,79
5,40
8
1,86
2,31
2,90
3,36
4.50
5,04
9
1,83
2,26
2,82
3,25
4.30
3,78
10
1,81
2,23
2,76
3,17
4,14
4.59
11
1,80
2,20
2,72
3,11
4,03
4.44
12
1.78
2,18
2,68
3,05
3,93
4,32
13
1,77
2.16
2,65
3,01
3,85
4,22
14
1,76
2,14
2,62
2,98
3,79
4,14
15
1,75
2,13
2.60
2,95
3,73
4,07
16
1,75
2,12
2,58
2,92
3,69
4,01
17
1,74
2,11
2,57
2,90
3,65
3,96
18
1,73
2,10
2,55
2,88
3,61
3,92
19
1.73
2,09
2,54
2,86
3,58
3,88
20
1.73
2,09
2,53
2,85
3,55
3,85
21
1,72
2,08
2,52
2,83
3,53
3,82
22
1,72
2.07
2,51
2,82
3,51
3,79
23
1,71
2,07
2.50
2,81
3,49
3,77
24
1,71
2,06
2,49
2,80
3,47
3,74
25
1,71
2,06
2,49
2.79
3,45
3,72
26
1,71
2,05
2,48
2,78
3,44
3,71
27
1,71
2,05
2,47
2,77
3,42
3,69
28
1,70
2,05
2,46
2,76
3,40
3.66
29
1.70
2,05
2,46
2,76
3,40
3,66
30
1,70
2.04
2,46
2,75
3,39
3,65
40
1,68
2;02
2,42
2,70
3,31
3,55
60
